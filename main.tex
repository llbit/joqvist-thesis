\documentclass[10pt, twoside, openright]{book}

% https://tex.stackexchange.com/questions/14008/how-to-put-a-line-break-in-section-heading
% https://tex.stackexchange.com/questions/268975/arcs-in-linguistic-functional-structure-lfg

\usepackage[T1]{fontenc}
%\usepackage{fontspec}
\usepackage{newtxtext}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{sectsty}
\usepackage{listings}
\usepackage{url}
\usepackage{float}
\usepackage{dpfloat}% double pane floats
\usepackage[fit]{truncate}
\usepackage{multirow}
\usepackage{array}
\usepackage{template/lic_bok}
\usepackage{pdfpages}
\usepackage{tikz}
\usepackage{tikzpagenodes}
\usepackage{forest}
%\usepackage[nooneline]{subfigure}
\usepackage{wrapfig}
\usepackage{multicol}
\usepackage{colortbl}
\usepackage{capt-of}% captionof
\usepackage{extendj}

%\usepackage{showframe}% layout debug

\usepackage[b5paper]{geometry}
% Page margins:
\geometry{
  inner=24mm,
  outer=24mm,
  top=20mm,
  bottom=36mm,
  bindingoffset=10mm,
  headheight=10mm
}

%\setsansfont{Nimbus Sans L}

%\setsansfont{Nimbus}[
%  Path=fonts/,
%  BoldFont=*-Bold,
%  Extension=.otf
%]

%\setmonofont{Inconsolata}[
%  Path=fonts/,
%  UprightFont=*-Regular,
%  BoldFont=*-Bold,
%  Extension=.ttf
%]

\usepackage{titling}% \theauthor, \thetitle

% Custom quote environment:
\usepackage{setspace}
\usepackage{etoolbox}
\AtBeginEnvironment{quote}{\singlespacing\small}

\usepackage{mathtools}

\usepackage[flushleft]{threeparttable}

% Corags dependencies:
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb,amsmath,amsthm}

\allowdisplaybreaks % Allow page breaks in align.

% Multiplicities dependencies:
\usepackage{booktabs}
\usepackage[hidelinks]{hyperref}
%\usepackage{xltxtra}
\usepackage{textcomp}% trademark symbol
\usepackage{makecell}% diagonal in table cell
\usepackage{fancybox}
\usepackage{color}
\usepackage{adjustbox}
\usepackage{xcolor}

% Wavy underline for Paper II.
\usepackage{ulem}
\normalem % get normal emphasis back

% Test selection:
%\usepackage[noend]{algpseudocode}

\usetikzlibrary{
arrows.meta,
chains,
%quotes, % Text above arrows.
scopes,
shapes,
backgrounds,
patterns,
shadows,
decorations,
decorations.text,
decorations.pathmorphing % snakes
}

% Inline verbatim in multiplicities:
\def\inline{\lstinline[basicstyle=\sffamily,keywordstyle={}]}

\usepackage[backend=biber,
maxcitenames=2,
mincrossrefs=99,
style=alphabetic,
doi=false,
isbn=false,
url=false,
refsegment=chapter,
defernumbers=true,
sorting=nyt]{biblatex}
% https://tex.stackexchange.com/questions/187643/biblatex-how-can-i-suppress-some-fields-for-multiple-entry-types
%\AtEveryBibitem{%
%\clearfield{editor}%
%}
\addbibresource{refs.bib}
\addbibresource{papers/corags.bib}
\addbibresource{papers/java7.bib}
\addbibresource{papers/multiplicities.bib}
\defbibheading{subbibliography}{\addcontentsline{toc}{section}{References}\section*{References}}

\newsavebox{\lstboxleft}
\newsavebox{\lstboxright}

\forestset{
  default preamble={
    for tree={
      draw,
      ellipse,
      font=\rmfamily\slshape,
    }
  }
}

\tikzset{
  attribute/.style={
    draw,
    rectangle,
    rounded corners,
    fill=white!80!gray,
    font=\footnotesize\itshape
  },
  % Edge label style for trees (forest):
  edgelbl/.style={midway, anchor=south, sloped, auto=false, font=\footnotesize\itshape}
}

\hyphenation{in-stru-men-ta-tion}
\hyphenation{Jast-Add}
\hyphenation{CEval}

% Multiplicities
\hyphenation{sym-bol-ic}
\hyphenation{ob-ject}
\hyphenation{re-ified}
\hyphenation{syn-chro-nized}
\hyphenation{Jast-Add-J}

% Popvet
\hyphenation{a-na-lys}
\hyphenation{a-na-lys-er}
\hyphenation{a-na-lys-era}


\newtheorem{observation}{Observation}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

% JastAdd code listings
\newcommand{\figcodesize}{\fontsize{8pt}{8pt}}
\newcommand{\codesize}{\fontsize{8pt}{8pt}}
\newcommand\figcodestart{\vspace*{-5pt}}
\lstdefinelanguage{jastadd}
{
  morekeywords = {
    % JastAdd
    syn, inh, eq, ast,
    coll, with, contributes, to, when, root, then,
    nta, circular, lazy,
    rewrite,
    % Java
    package, import,
    class, extends, boolean, false, true,
    final,
    while, do, for, each, if, else, switch, case, default,
    this, abstract, void, new, return, null,
    try, catch, throw, finally, public, aspect, implements,
    interface, protected, int, long, char, super,
    % annotations
    @Parallel, @Override,
    % python
    def, True, False,
    % C++
    bool, auto
  },
  otherkeywords = { ::=, *, <, >, :, [, ], ?, =},
  sensitive = true,
  escapeinside={/+}{+/},
}

\lstset{language=jastadd,
  basicstyle=\ttfamily,
  keywordstyle=\bfseries,
  morekeywords={T, nil},
  numbers=none,
  showspaces=false
}

% https://tex.stackexchange.com/questions/11707/how-to-force-output-to-a-left-or-right-page/11709#11709
\newcommand*\cleartoleftpage{%
  \clearpage
  \ifodd\value{page}\hbox{}\newpage\fi
}

\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}

% Java classes in text
\newcommand{\Class}[1]{\textbf{\texttt{\footnotesize{#1}}}}
\newcommand{\Attr}[1]{\textbf{\texttt{\footnotesize{#1}}}}
\newcommand{\Method}[1]{\textsf{\footnotesize{#1}}}
\newcommand{\Keyword}[1]{\textbf{\texttt{\footnotesize{#1}}}}
\newcommand{\Set}[1]{\textsc{#1}}

% References
\newcommand{\Chapter}[1]{Chapter~\ref{#1}}

\hyphenation{JastAdd}
\hyphenation{IntelliJ}
\hyphenation{JModelica}

% Make all sections sanserif.
\allsectionsfont{\sffamily}

% Make fancy header and no footer.
\pagestyle{fancy}
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\fancyhf{}
\fancyhead[LE, RO]{\bfseries\thepage}
\fancyhead[LO]{\itshape \rightmark}
\fancyhead[RE]{\truncate{.95\headwidth}{\itshape \leftmark}}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0pt}%
\setlength{\headheight}{75pt}
\fancypagestyle{plain}{
  \fancyfoot[LE, RO]{\chapterFoot}
  \fancyhead{}
  \renewcommand{\headrulewidth}{0pt}
}

\newcommand{\chapterFoot}{}
\newcommand{\markmargin}{}
\newlength{\spiff}

\newif\ifpopvetOnly

\newif\ifpaperI % Java 7
\newif\ifpaperII % Multiplicities
\newif\ifpaperIII % Test Selection
\newif\ifpaperIV % CoRAGs

\paperItrue
\paperIItrue
\paperIIItrue
\paperIVtrue

%\popvetOnlytrue

\newcommand{\paperIref}{%
Jesper Öqvist and Görel Hedin.
``Extending the JastAdd Extensible Java Compiler to Java~7''.
In \emph{Proceedings of the 10th International Conference on Principles and Practicies of
Programming on the Java Platform: Virtual Machines, Languages, and Tools (PPPJ'13), ACM,
pp. 147--152}. Stuttgart, Germany, 2013.
}
\newcommand{\paperIIref}{%
Friedrich Steimann, Jesper Öqvist, and Görel Hedin.
``Multitudes of Objects: First Implementation and Case Study for Java''.
In \emph{Journal of Object Technology, Vol. 13, no. 5 (November 2014), pp. 1:1--33}.
}
\newcommand{\paperIIIref}{%
Jesper Öqvist, Görel Hedin, and Boris Magnusson.
``Extraction-Based Regression Test Selection''
In \emph{Proceedings of the 13th International Conference on Principles and
Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools (PPPJ'16), ACM,
pp. 5:1--5:10}. Lugano, Switzerland, 2016.
}
\newcommand{\paperIVref}{%
Jesper Öqvist and Görel Hedin.
``Concurrent Circular Reference Attribute Grammars''.
In \emph{Proceedings of the 10th ACM SIGPLAN International Conference on Software Language
Engineering (SLE 2017), pp. 151--162}. Vancouver, BC, Canada, 2017.
}

\title{Contributions to\\
Declarative Implementation of\\
Static Program Analysis}
\author{Jesper {\"O}qvist}
\date{2019}

% Make tex a bit less neurotic:
\sloppy
\raggedbottom

\begin{document}

\ifpopvetOnly
\else
% Title page.
\frontmatter
\begin{titlepage}
{
  \thispagestyle{empty}

  \begin{tikzpicture}[remember picture, overlay, font=\sffamily]
  \node at (current page text area.north east) [
      anchor=north east,
      yshift = -3cm,
      text width = \textwidth,
      inner sep = 0,
      outer sep = 0,
      align = right
    ] (title)
    {\raggedleft\huge\textbf{\thetitle}\\};
  \node[below = 8cm of title,
      text width = \textwidth,
      inner sep = 0,
      outer sep = 0,
      align = right
    ] (author)
    {\raggedleft\huge\textbf{\theauthor}\\};
  \node[below = 1.5cm of author,
      text width = \textwidth,
      inner sep = 0,
      outer sep = 0,
      align = right
    ] (text) {\raggedleft
      \large{}Doctoral Dissertation, \thedate\\[0.4cm]
      \Large{}Department of Computer Science\\[0.1cm]
      Lund University\\
    };
  \node (mid) at ($(author.south east)!0.7!(text.north east)$) {};
  \draw[thick] (mid -| current page.west) -- (mid -| current page.east);
  \node at (mid -| current page text area.west)
    [anchor = west, shift={(-0.5cm, -1cm)}]
    {\scalebox{0.44}{\includegraphics{template/lomacdsvs.eps}}};
  \end{tikzpicture}
}

\newpage
\thispagestyle{empty}
\begin{flushleft}
\vspace*{\stretch{1}}
Version 1.1-SNAPSHOT\linebreak[2]

Department of Computer Science\\
Lund University\\
Box 118\\
SE-221 00  Lund\\
Sweden\linebreak[2]

Email: \url{jesper.oqvist@cs.lth.se} \linebreak[2]

Typeset using \LaTeX.\\

Printed in Sweden by Tryckeriet i E-huset, Lund, \thedate.\linebreak[2]

\textit{\copyright ~\thedate~\theauthor}
\vspace{10mm}
\end{flushleft}
\end{titlepage}

\newpage
\section*{Abstract}
\input{abstract}

\newpage
\section*{Acknowledgements}

This work was in part funded by the Swedish Research Council under grant 621-2012-4727
and by a 2015 Google Faculty Research Award for supporting concurrent analyses in interactive
programming tools.

This dissertation would not exist if not for Prof. Görel Hedin.
Görel convinced me to start my PhD studies and supervised me along the way.
She told me what to do, corrected my mistakes, and listened to my weird ideas.
I owe Görel my greatest thanks in helping me complete all of this work.

I owe much gratitude to my beloved wife, Elise.  She is the chillest wife, in
the parlance of our times.  Elise kept me sane and healthy while writing this
thesis.  I am also very grateful for our many fun travels and outings during my
time as Ph.D. student.

Thank you Prof. Boris Magnusson for co-supervising me and for the test
selection collaboration.

Thank you Emma Söderberg for co-supervising me, for getting me into running
(\texttt{RuntimeException}), for hosting me at Google for my first internship,
and for many fun collaborations related to JastAdd and other projects during my PhD studies.

Thank you Prof. Friedrich Steimann for the multiplicities collaboration. It was fun to visit
your department in Germany and to work intensely to complete most of the implementation in
a week.

Thank you Niklas Fors, Alfred Åkesson, and Christoff B{\"{u}}rger for collaborations
related to JastAdd development and compiler construction.

For entertaining and enlightening discussions during the time-honored tradition
of fika, I would like to thank
Christoph Reichenbach, Flavius Gruian,
Linus Åkesson, Maj Stenmark, Pierre Moreau,
Jonas Skeppstedt, Gustav Cedersjö, Noric Couderc,
and all of my other colleagues at the computer science department in
Lund, past and present.

Thank you Erik Hogeman for the Java~8 project.

Thank you to all the excellent people whom I had the pleasure to work with at Modelon (Jesper, Jon,
Jonathan, Axel) and Google Mountain View (Ciera, Karl, Valentyn, Hans, Eddie).  Thank you Arun
Chauhan for hosting me during my second internship.

The diagrams in this thesis would not look as nice without a few
essential \LaTeX packages.  Thus, I owe my gratitude to the authors of the
\textsc{PGF}/{\rm Ti\emph{k}Z} packages, and especially Sa\v{s}o
\v{Z}ivanovi\'{c} for developing the excellent \textsc{Forest} package which
I used for drawing all trees in the thesis.

{\singlespacing\footnotesize
I was considering making a list of enemies and placing Måns Magnusson on it for distracting me with
programming problems. However, I finished the thesis on time so I think we can continue to be friends.}


\newpage
\section*{Contribution Statement}

The following papers are included in this dissertation:

\begin{description}
  \item[Paper~I]
    \paperIref
  \item[Paper~II]
    \paperIIref
  \item[Paper~III]
    \paperIIIref
  \item[Paper~IV]
    \paperIVref

    This thesis includes an extended version of this paper, with correctness proofs for the
    algorithms.
    This extended version of the paper was published as a technical report (report number 103,
    Lund University, 2017).
\end{description}

\noindent
The table below indicates the responsibilities Jesper Öqvist had in writing each paper:

\vspace{1em}
\begin{center}
\begin{tabular}{llllll}
  \toprule
  \emph{Paper} & \emph{Writing} & \emph{Concepts} &  \emph{Implementation} & \emph{Evaluation} & \emph{Algorithm} \\
  \midrule
  \textbf{I}    & YES     & YES     & YES & YES & N/A \\
  \textbf{II}   & partial & no      & YES & YES & N/A \\
  \textbf{III}  & yes     & partial & YES & YES & yes \\
  \textbf{IV}   & YES     & YES     & YES & YES & YES \\
  \bottomrule
\end{tabular}
\end{center}
\vspace{1em}

\noindent
Capital letters indicate roles where Jesper Öqvist took primary responsibility for the
given role. For Paper~III, Jesper Öqvist wrote the first drafts of the paper, and then took main
responsibility to finish the implementation and evaluation parts. For Paper~II, Friedrich Steimann
wrote the majority of the paper, with Jesper Öqvist taking responsibility for writing about
implementation and evaluation as well as designing some of the figures/diagrams.
For all papers, Jesper Öqvist developed the implementation and empirical evaluation.
For Paper~IV, Jesper Öqvist wrote all proofs, with feedback from Görel Hedin.


\setcounter{tocdepth}{1}
\tableofcontents

%\pagebreak % fucks up the toc (massively underfull vbox)

\mainmatter
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thefigure}{\arabic{figure}}
\renewcommand{\thetable}{\arabic{table}}
\renewcommand{\thechapter}{\Roman{chapter}}
\renewcommand{\theequation}{\arabic{equation}}
\renewcommand{\thelstlisting}{\arabic{lstlisting}}

\part{Introduction}
\chapter{Introduction}
\section{Introduction}

% More space below equations:
\abovedisplayskip=0pt plus 3pt minus 8pt
\abovedisplayshortskip=0pt plus 2pt minus 8pt
\belowdisplayskip=12pt plus 6em minus 9pt
\belowdisplayshortskip=7pt plus 5em minus 4pt

% NOTE: 'static analysis' == 'static program analysis'

% 1) Motivation
%%%%%%%%%%%%%%%

\emph{Static Program Analysis} is the automated analysis of a computer program before it is executed
\cite{moller2018static,Binkley:2007:SCA:1253532.1254713}.
Static program analysis is essential for software development in several ways:
it is used for translating programs to executable machine code, uncovering bugs,
optimizing program performance, and assisting
software developers in constructing and testing programs.
Programmers use static analysis via smart code editing tools in modern integrated development
environments for efficient software editing and for improving code organization.
Tools like declaration lookup and code completion suggestions are used for navigating and
writing code, while code refactoring tools are used to improve code structure.
In addition, static analysis is an important part of automated software quality assurance
and testing
at many software companies, like Google \cite{Sadowski:2018:LBS:3200906.3188720} and
Facebook \cite{DBLP:conf/nfm/CalcagnoDDGHLOP15}.


% Other references: Flemming Nielsen, Principles of Program Analysis, 2015

% 2) Implementation Challenges & Declarative Programming
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

When developing static program analyses we face similar problems as in other types
of software development, including
the perennial challenge of building a flexible software architecture that enables
future additions after initial development.  % without major redesign or huge development effort.
Static analyses should have a flexible
architecture to ease the process of expanding the analysis to support new language features
as programming
languages evolve and as new analyses are invented.
The challenge of developing a flexible architecture can be addressed by \emph{Declarative
Programming}.
%Declarative programming has many benefits in aiding good static analysis architecture and
%combating the above challenges, making it easier to develop maintainable
%extensible static analyses.

Declarative programming is a programming paradigm in which
the programmer specifies the desired result of a computation rather than exactly enumerating
the necessary steps to compute the result. A common view of declarative programming is that the
programmer writes \emph{what} should be computed, rather than \emph{how} it is computed.
Side effect-free code is an important aspect of declarative programming.
Observable side effects are any effect a function call
has on the state of the program which can change the result of other function calls \cite{DBLP:conf/fase/Naumann05}.
When side effects are present, the ordering of function calls is significant for the result
of the program. A key advantage of declarative programming is that the programmer need not carefully
order function calls to avoid unwanted side effects, instead they are free to focus on what
should be computed.

%Declarative programming can counteract inflexibility, fragile modules, and improve separation
%of concerns.
Declarative programming benefits flexible software architectures by making it easy to break
the code up into smaller parts which can be cleanly combined.
This makes it possible to have clearly separated computations for separate tasks in the software,
making it easier to extend the software with new features, since fewer existing parts of the
software will need to be taken into account when making changes or additions.
Furthermore, side effect freedom makes it easier to combine different computations, aiding analysis
composition and extensibility.


% 3) Topic of Dissertation
%%%%%%%%%%%%%%%%%%%%%%%%%%

One way of developing static analyses and compilers declaratively is by using
\emph{Attribute Grammars} (AGs), a declarative formalism proposed by
\textcite{DBLP:journals/mst/Knuth68} for describing the semantics of programming languages.
Although plain AGs \emph{can} be used to declaratively implement static analyses,
the task of implementing a full programming language was onerous due to drawbacks like
``repetition, overwhelming detail, and the interleaving of many activities'' \cite{DBLP:journals/cj/DueckC90}.
Furthermore, static analyses often rely on computations with
information flow along a graph structure, and these are not well suited for classical AGs
which work with information flow along a tree structure.
\emph{Reference Attribute Grammars} (RAGs) \cite{DBLP:journals/informaticaSI/Hedin00}
is an extension to plain AGs with attributes that can have reference values. These reference attributes
enable us to compute graphs on top of a tree, and allow computations to flow along those
graphs,
substantially simplifying the task of implementing static analyses. Reference attributes proved
very useful in practice for implementing real programming languages (in contrast to most previous
attempts with classical AGs which were mostly minimal toy languages).
Reference attributes are often very elegant, compactly defining the relations that are being
computed.
Compilation, and other static analyses, can be implemented with RAGs, leading to elegant declarative
implementations which are easy to compose and extend.

% 4) RAGs History
%%%%%%%%%%%%%%%%%

Past development efforts have
proven the feasibility of building large-scale compilers using RAGs.
For example, the \emph{JastAdd} \cite{DBLP:journals/scp/HedinM03} metacompiler supports compiler development
with RAGs and has been used to develop large-scale compilers for the Java and Modelica languages.
In particular, ExtendJ \cite{jastaddj} is a full Java compiler,
and JModelica.org \cite{DBLP:journals/cce/AkessonAGBT10}
is a compiler for the Modelica modeling language. Both ExtendJ and JModelica.org are developed
using JastAdd and using reference attributes for declarative static analysis.
\emph{Silver} \cite{DBLP:journals/scp/WykBGK10} is another example of a metacompiler for RAGs, which has been used
for implementing compilers for Java \cite{DBLP:conf/ecoop/WykKBS07}, C \cite{DBLP:journals/pacmpl/KaminskiKCW17}, and PROMELA \cite{DBLP:conf/spin/MaliW11}.
RAGs have also been implemented as embedded libraries, including
Kiama (a Scala library) \cite{DBLP:conf/gttse/Sloane09}, RACR (for Racket) \cite{DBLP:conf/sle/Burger15},
and JavaRAG (a Java embedding) \cite{DBLP:conf/aosd/ForsCH15}.
With a few supporting features, like static Aspect-Oriented Programming, and an attribute replacement
mechanism, it is possible to
extend a compiler implemented with RAGs by adding new attributes \cite{DBLP:conf/aosd/AvgustinovET08}.
This can be used to implement new static analyses, tools, and language
extensions upon an existing compiler, for example adding non-null type checking
to a Java compiler \cite{DBLP:journals/jot/EkmanH07}.

% 5) Contributions
%%%%%%%%%%%%%%%%%%

This thesis presents new applications and algorithms for RAGs.
Central to the contributions are practical applications and tools implemented in the
JastAdd metacompiler and the ExtendJ Java compiler.
Specifically, my main contributions are the following:

\begin{itemize}
  \item An extension of the ExtendJ compiler to support Java~7 (Paper~I).
  \item An extension for ExtendJ with new static analyses and code generation for
    a new programming mechanism, \emph{Multiplicities}, implemented as a Java language extension
    (Paper~II).
  \item A new algorithm for regression test selection with implementation based on ExtendJ (Paper
    III).
  \item A new concurrent evaluation algorithm for RAGs with support for circular (fixpoint)
    attributes, implemented in JastAdd and evaluated by parallelizing ExtendJ (Paper~IV).
\end{itemize}

My research shows that RAGs are very effective for building static analyses, programming
language extensions,
and static-analysis based tools.
By using RAGs to develop declarative compiler extensions, I have demonstrated that the declarative
nature of RAGs benefits composability and extensibility in static analysis.
As part of my thesis work I have made substantial improvements to both ExtendJ and JastAdd.
My improvements to ExtendJ have enabled it to support nearly all of Java~8,
as well as fixing numerous bugs which prevented many real-world programs from compiling correctly.
To verify the improvements to ExtendJ, I developed a large regression test suite for the compiler.
The impact of my work is demonstrated by multiple research groups publishing results based
on extensions for ExtendJ to support new language features and analyses.

%By using declarative design, programming and RAGs it is possible to use efficient evaluation algorithms
%like parallel evaluation, memoization and incremental computation can be used safely with RAG-based static analyses.

An important improvement to ExtendJ was also to weed out some side effects that had been introduced
in the specification in order to speed up compilation. I have managed to replace that code with
completely declarative code with only negligible effects on performance. While those side effects were
carefully crafted to not have any effect on sequential evaluation, they did not work correctly for
parallel evaluation. Complete lack of observable side effects is crucial in order for the parallel
evaluation algorithms to work correctly.

\subsection{Methodology}

The work in this dissertation was carried out using a problem-oriented methodology. I looked at
different interesting problems, designed potential solutions, implemented promising solutions to
explore what worked, and finally evaluated the effectiveness of the solution.
Zobel describes a similar methodology \cite[p.~54]{zobel2004writing}.

Solutions were implemented using aspects of agile software development
\cite{martin2002agile}, like iterative development and continuous testing. For
example, I developed an extensive regression test suite for the ExtendJ
compiler with about a thousand tests for bugs and features. One bug fix or feature
was implemented at a time and the tests were run after each change to the
compiler, to ensure there were no regressions.

I evaluated the correctness and performance of each compiler extension, static analysis,
and algorithm presented in this thesis by compiling real-world Java programs.
The subject programs I used were sourced from the Java open
source community, and from curated corpora of Java programs for static analysis
and runtime benchmarking, in particular:

\begin{description}\raggedright
  \item[DaCapo 2009] A corpus of Java programs for Java runtime benchmarking \cite{DaCapo:paper}.
  \item[Qualitas Corpus 20130901] A collection of Java programs for static analysis
     and software studies \cite{QualitasCorpus:APSEC:2010}.
\end{description}

I evaluated the correctness of compiler extensions in Paper~I to III by comparing the compilation
result to the ground-truth result of the OpenJDK compiler, the reference compiler for Java.
For Paper~III, I measured the test results of the
subject programs to check that all tests that failed were identified by the test selection tool
I developed for that paper (in this case, the subject programs were compiled with OpenJDK for test
running).
For performance evaluation, I measured overall compilation time (in Paper~IV, I additionally measured
individual attribute evaluation times).
For performance evaluations, I used elements of the steady-state performance method described
by \textcite{DBLP:conf/oopsla/GeorgesBE07}. The exact method varied between evaluations, but typically
the solution was run on a subject program multiple times while measuring only the running time
without Java start-up time. The steady-state performance method aims to eliminate
variance between runs caused by the Java runtime system, including garbage collection and runtime code
optimizations.

All implementations I developed for the included papers in this dissertation have been published as open
source software, freely available online for other researchers to use and extend.
Additionally, the implementation and empirical evaluation for Paper~IV was accepted by a peer review
process known as
\emph{artifact evaluation} \cite{Krishnamurthi:2015:RSC:2739250.2658987}, and the artifact itself was
published together with the paper.


\subsection{Thesis Outline}

The rest of this thesis is organized as follows.
\secref{sec:background} gives an introduction to static program analysis, in particular:
applications and implementation concerns.
\secref{sec:rags} presents a basic theoretical background to
reference attribute grammars, including
most of the kinds of attributes supported by the JastAdd metacompiler. An introduction to
the JastAdd metacompiler
itself is given in \secref{sec:jastadd}.

\secref{sec:extension-mechanisms} shows how declarative and extensible static analyses can be
developed with RAGs using the JastAdd metacompiler. In this section, I also show a new pattern
for generic tree traversal with RAGs.

\secref{sec:extendj} describes the ExtendJ compiler:
a full declarative and extensible Java compiler which was built using JastAdd.

\secref{sec:contributions} details my contributions in this thesis, including, but not limited
to, the contributions in the included papers.

Concluding remarks are in \secref{sec:conclusions}, and then the included papers follow.

\section{Background}
\label{sec:background}

This section gives an introduction to static program analysis, reference attribute grammars,
and the JastAdd metacompiler. These topics are needed for the presentation of
the technical contributions in this dissertation.
We will start with a high-level overview of static program analysis, then
follows a theoretical background for static analysis with reference attribute grammars.
Finally, we introduce the JastAdd metacompiler.

\subsection{Static Program Analysis}

Programs in general-purpose programming languages, like Java or C, are executed
to perform some computation based on varying inputs.
The computation is \mbox{\emph{dynamic}:} its output may change for each set of inputs.
The goal of static program analysis, or just \emph{static analysis} for short,
is to answer questions about \emph{static} properties
of programs, that is, properties that hold for all possible inputs.

Let us consider a question we can ask about a program:
Can the program get stuck forever, or will it always halt?  This question is about a
static property of programs, so can we answer it with static analysis? Clearly, the question
is easy to answer for \emph{some} programs. For instance, this program is obviously non-halting:

\begin{lstlisting}
while (true) { }
\end{lstlisting}

\noindent
Unfortunately, it turns out to be a computationally intractable problem to answer this question for
all possible programs.
The question is a variant of the 
famous halting problem which was proven undecidable in
the general case by \textcite{turing1937computable}.

Although many interesting static analysis questions are undecidable in the general case,
we can often
develop useful static analyses by limiting the scope of the question or making
the analysis conservative such that it gives a definite answer only for a subset
of all possible programs (some programs halt, some are definitely non-halting for certain inputs,
others we don't know).

Static analysis originates from compiler construction. A compiler is a program
that translates a computer program in source form (usually text) to an executable form (like
machine code).  We can view compilers as a static analysis that answers
the question: What is the machine code translation of this program?
Although this definition means that a compiler is just a type of static
analysis, we normally reason about compilers not as a single analysis, but as
several distinct and interdependent analyses.

With the advent of optimizing compilers, several new types of static analyses were developed
for optimizing program performance. Performance optimizing analyses are often focused around
removing redundant computations from a program. In more recent years, static analysis
has also become the collective term for tools that analyze program correctness, often in
integrated development tools and continuous integration systems.
This dissertation focuses more on the latter form of analyses, as well as the original
compiler-oriented static analyses (that is, the non-optimizing kinds).
The next section gives an overview of these kinds of analyses.


\subsubsection{Methods of Static Analysis}

There are many different kinds of static analysis, with different applications.
Compilers use static analysis to check the source program for errors and for
generating executable code.
Integrated development environments provide tools to the programmer based on static analysis,
like refactoring tools, code navigation tools, and code completion tools.
After the program code has been edited by a programmer, it is often checked for problems like
code duplication \cite{Roy:2009:CEC:1530898.1531101,DBLP:conf/kbse/NarasimhanR15} on continuous
integration servers \cite{duvall2007continuous,Sadowski:2018:LBS:3200906.3188720}.
Analyses that produce metrics about class encapsulation and complexity
can help guide refactoring efforts to improve existing
code in object-oriented languages \cite{DBLP:journals/tse/ChidamberK94, DBLP:journals/tse/BasiliBM96}.

The field of static program analysis is diverse, with many different kinds of static analysis.
The kinds that are discussed in this dissertation can be grouped into the following categories:

\vbox{
\begin{itemize}
  \item Name Analysis,
  \item Type Analysis,
  \item Control Flow Analysis,
  \item Dataflow Analysis.
\end{itemize}
}

\noindent
These are broad categories encompassing multiple different
static analyses. In the following sections we will look at examples of typical analyses
from each category.


\subsubsection{Name Analysis}

The goal of name analysis is to determine the meaning of variable names,
function names, and type names in a program.\footnote{%
Interpreted languages can use environments at runtime to avoid performing full name analysis
before running a program. Thus, interpreted languages can rely on dynamic name analysis
rather than static name analysis.}
Name analysis is required for all other forms of useful program
analysis: if we can not figure out the meaning of variable (or function) names in a program we
cannot figure out anything else useful about it.

Consider the code fragment:
\par\nobreak
\vbox{%
\begin{lstlisting}
int a = -1;
int fn() {
  int a = 2;
  return a+1;
}
\end{lstlisting}
}

\noindent
The expression \verb'a+1' refers to some variable \verb'a'. However, we can not be
certain which one the programmer meant since there are two
declarations for a variable named \verb'a'. The expression value is either
3 or 0, depending on which declaration is used.
A reasonable programming language specification disambiguates situations like these,
and the compiler uses name analysis to implement the right behaviour.
Name analysis is also used by the compiler to assign memory locations to
variables: memory locations are assigned per declaration and each variable use refers
to the memory location its declaration was assigned.

Java uses another type of name analysis, called \emph{syntactic classification} \cite[\S 6.5.1]{jls7},
to determine what name scope the names in a program belong to.
For example, in a qualified expressions like \texttt{a.b},
the \texttt{b} refers to a field in some class, but it could be a static
field or an instance field, depending on whether or not \texttt{a} is a class name in the current
context.
Syntactic classification disambiguates the expression by marking each name with
the correct name kind.


\subsubsection{Type Analysis}

Types are used in programming for categorizing the objects of the program and
controlling which operations are permitted on them. In Java, for example,
the following statement is disallowed because it is ill-typed:

\begin{lstlisting}
int a = "hi";
\end{lstlisting}

Type analysis is a broad category of analyses concerned with enforcing the typing
rules of programming languages, or for providing additional type-based analysis on
top of the rules of the language.
Type analysis normally consists of the following steps:

\begin{description}
  \item[Type Lookup] Determine the type of all typenames in the program.
    Typenames are used, for example, in variable declarations and class instance expressions.
  \item[Type Analysis] Assign result types for all expressions in the program. For some expressions
    the result type may not be specified by the language: they are marked as ill-typed.
  \item[Type Checking] Check that the operands of each expression and statement
    are compatible according
    to the typing rules of the language. Report type errors for all ill-typed expressions.
  \item[Type Inference]
    Reconstruct types for partially typed expressions, saving work for the programmer.
\end{description}

The first step, type lookup, often piggybacks on the name analysis in a compiler.
The second step, type analysis, may produce some type errors. Type errors found during type
analysis can either be reported directly or handled later in type checking. In the latter case
the type analysis will assign some kind of wrong-type to an ill-typed expression, that is, a type
that is only used for marking incorrectly typed expressions.

In languages using parametric polymorphism, many aspects of type analysis become much more involved.
Type lookup for polymorphic typenames requires more work since these typenames
can include other typenames as arguments.
Another implementation issue for nominally typed\footnote{%
Nominally typed means that types are compared by name, rather
than by structure. For an introduction to nominal and structural typing,
see \textcite[p.~251-254]{DBLP:books/daglib/0005958}.}
languages, like Java, with polymorphic types is to
assign identities to different
parameterizations of a single type.

A common feature in languages with parametric polymorphism is \emph{Type Inference}. Type inference is
intended to save the programmer the tedious work of typing out the full type for
all variables and functions when the compiler could instead figure out the
right type. For example, in Java~6 programs, statements
similar to the following one are not uncommon:

\begin{lstlisting}
List<Map<Integer,String>> maps
    = new ArrayList<new HashMap<Integer,String>>();
\end{lstlisting}

\noindent
There is some redundancy in this statement: a human familiar with Java~6 could easily deduce the
meaning without the type on the first line or, alternatively, the type parameter to \verb'ArrayList'.
In Java~7, the programmer may instead utilize type inference and write just

\begin{lstlisting}
List<Map<Integer,String>> maps = new ArrayList<>();
\end{lstlisting}

\noindent
Similarly, the C++ language adopted type inference in C++11. The \verb'auto' keyword from
C++11 can be used to simplify a variable declaration like this

\begin{lstlisting}
std::list<int>::iterator it = list.begin();
\end{lstlisting}

\noindent
into this

\begin{lstlisting}
auto it = list.begin();
\end{lstlisting}

As demonstrated above, type inference clearly saves some typing
for the programmer. This in itself is often a motivation for the use of type inference.
It could also be argued that reduced code size makes the code easier to read,
with secondary effects of making the code easier to understand and modify for third parties.
In the first case, the inferred type is obvious, since the type
is repeated on the same line. In the second case, however, the inferred type depends on whatever
the type of \texttt{list} is, possibly making the meaning less clear at a glance.


\subsubsection{Control Flow Analysis}

Control flow analysis is used for analyzing the possible orderings of expressions and
statements in a program during execution. Each possible ordering is called a control-flow path,
and these paths are represented implicitly by control flow graphs.

Control flow analysis can be used, for example, to determine if a piece of code is unreachable, or
if all paths through a function have a return statement.

An example of control flow analysis in Java compilers is the exception handling analysis.
This analysis ensures that all control flow paths where an exception can
be raised, for certain types of exceptions, encounter a corresponding exception handler.
Thus the exception handling analysis ensures that a program never gets interrupted in an
unexpected way by certain exceptions.\footnote{The exception (excuse the pun) to this rule
is that non-exception throwable objects, and exceptions the programmer explicitly allowed
to be thrown, may still interrupt the program.}

Control flow analysis normally refers to the analysis of control flow within a single procedure.
Inter-procedural control flow analysis is a related, but quite different, topic.
Rapid Type Analysis is an example of an inter-procedural analysis for object-oriented languages
\cite{DBLP:conf/oopsla/BaconS96}.


\subsubsection{Dataflow Analysis}

Dataflow analysis builds upon control flow analysis to find the possible ways
data can flow through a program.
Dataflow analysis gathers all possible control flow paths and conservatively approximates
the state of variables at all points in the program.
For example, dataflow analysis can determine if a variable can ever contain a null reference at a
certain point in the program.

Definite assignment analysis is a kind of dataflow analysis, in which we analyze variable
uses to determine if the variable has definitely been assigned before that use.
Definite assignment analysis can catch uninitialized variable errors, a well-known
weakness of the C programming language.

Consider the following fragment of code:
\par\nobreak
\vbox{%
\begin{lstlisting}
void f() {
  int a;
  int b = a+1;
}
\end{lstlisting}
}

\noindent
Here, variable \verb'a' is used without being
initialized. In the C programming language, this results in an undefined value assigned to
variable \verb'b'.
Most C compilers allow this to compile, often without warning, which has led to a number of
bugs in real-world programs.\footnote{GCC version 7.2.0, a popular C compiler,
compiles the example code without warning. An optional warning can be enabled by the user,
giving a warning for the example code.}
In the worst case these bugs might not be caught in code review
or testing and make it into a software release.  As of May 2018
there were 27 documented vulnerabilities with descriptions explicitly mentioning ``uninitialized
variable'' or ``uninitialized stack variable'' in the Common
Vulnerabilities and Exposures database \cite{cveorg}.


\subsubsection{Program Models}

As static analyses vary in domain and application, so do their implementations.
However, all static analyses have in common a need for some kind of program model.
Program models represent, in a structured form, the features of the program which are
of interest for the analysis at hand.
The program model used for control flow and dataflow analysis, for example,
is the control flow graph.

In this dissertation we will mainly consider static analyses that are based on an \emph{Abstract Syntax
Tree}~(AST) as program model. For control flow and dataflow analysis, the control flow graph can
be extracted from the AST. Most compiler frontends\footnote{%
Frontend refers to the parts of a compiler that are not directly concerned with generating or
optimizing the executable program output.}
are primarily based on analyzing and transforming ASTs.

The state of the art in static program analysis based on ASTs is to use depth-first tree
traversal to compute the necessary information at each node.
In this thesis, we focus instead on computing properties of the AST with reference attribute grammars.
The next section describes the AST in more detail, and the section
after that introduces reference attribute grammars.


\subsection{Abstract Syntax Trees}

A programming language is a formal language\footnote{As opposed to natural languages which are
informal and highly ambiguous.} which follows a (usually unambiguous) context-free grammar that can be described
in a syntactic metalanguage like, e.g., Extended Backus-Naur Form (EBNF).\footnote{Backus-Naur Form
was first used in the Algol 60 report by Backus et. al.
\cite{DBLP:journals/cacm/BackusBGKMPRSVWWW60}.
Extended Backus-Naur Form adds repetition and optional symbols.}
This section presents an informal view of the formal notions of languages and grammars.
See \textcite{DBLP:books/lib/HopcroftU69} for a more formal presentation.

Formal notations like EBNF describe the syntax of a programming language in a
way that can be mechanically processed and reasoned about (using a
metacompiler).  Here is a concrete example of part of an EBNF grammar for a
simple procedural C-like programming language:

\begin{alignat*}{2}
& \textit{Program}    & & \; \Coloneqq \; \textit{Function}\,\ast \\
& \textit{Function}   & & \; \Coloneqq \; \textit{Type} \; \langle\textit{ID}\rangle \; \texttt{\lq(\rq} \; \textit{Parameter}\ast \texttt{\lq)\rq} \; \textit{Block} \\
& \textit{Parameter}  & & \; \Coloneqq \; \textit{Type} \; \langle\textit{ID}\rangle \\
& \textit{Block}      & & \; \Coloneqq \; \texttt{\lq\{\rq} \; \textit{Statement}\ast \; \texttt{\lq\}\rq} \\
& \textit{Statement}  & & \; \Coloneqq \; \textit{IfStmt} \mid \textit{Call} \\
& \textit{IfStmt}     & & \; \Coloneqq \; \texttt{\lq{}if\rq} \; \texttt{\lq(\rq} \; \textit{Expr} \; \texttt{\lq)\rq}
  \; \textit{Block} \; [ \, \texttt{\lq{}else\rq} \; \textit{Block} \, ] \\
& \textit{Call}       & & \; \Coloneqq \; \langle\textit{ID}\rangle \; \texttt{\lq(\rq} \; \textit{Expr}\ast \, \texttt{\lq)\rq} \; \texttt{\lq;\rq} \\
& \ldots \hfill{} & &
\end{alignat*}

\noindent
Each line gives a \emph{production rule} composed of symbols, specifying how to write a declaration or statement in
the language.
Each rule defines a so-called \emph{nonterminal} symbol of the language, which
is composed of other symbols (terminal and nonterminal).
Terminal symbols are keywords (\texttt{\lq{}if\rq}), punctuation symbols (\texttt{\lq\{\rq}), and identifiers ($\langle\textit{ID}\rangle$).
The left-most part of a rule gives the name of the nonterminal, then the component symbols
follow on the right (after $\Coloneqq$).
Programs in the language should exactly match a rule of the grammar;
in the present example, a valid program should match the \emph{Program} rule.

Any valid program can be proven to be part of a formally defined programming language by building
a derivation tree of grammar rules matching the program.
Parsers are programs that match a valid program to the nonterminals of the parsed programming
language.
A successful parse produces a rooted tree of the (non)terminals
of the program.  This tree is called an \emph{Abstract Syntax Tree}~(AST),
where internal nodes are nonterminals and leaf nodes are terminals.
An AST is abstract in the sense that it does not contain the punctuation symbols
and keywords from the concrete grammar of the language and instead follows the
\emph{abstract grammar} for the language, from which all the punctuation/keyword symbols can be
reconstructed (called unparsing).\footnote{%
An alternative form of parsing output is the parse tree, which
contains the literals and tokens of the concrete syntax. Either form constitutes a proof that the
program belongs to the parsed programming language. The AST matches a derivation tree for the proof.}

An AST is a tree representation of the statements and
expressions of a source program.  The root of the AST represents the whole program. The
next level typically contains global declarations like classes, constants, and functions.
\figref{fig:ast1} illustrates a typical AST for a small
program in the language described by the above EBNF grammar.

A context-free grammar described in EBNF is a declarative description of a language. This
in itself has all the advantages of declarative programming and, additionally, a formal language
like EBNF can be mechanically processed by a type of metacompiler known as a \emph{parser generator}
to obtain an efficient parser for the language.\footnote{Some parser generators
can even build parsers for ambiguous or non-deterministic grammars. For example, the Spoofax
language workbench implements SGLR parser generation \cite{Kats2010}, based on generalized LR parsing
which was invented by Lang
\cite{Lang1974}.} By using a parser generator, the language designer need only
design the syntactic rules of their language and write them down in a suitable syntactic metalanguage.
Certain parser generators can even guarantee, for certain classes of languages, that the language
is unambiguous (if it is LR(1), for example).
It is entirely feasible to hand-construct a parser, but this requires a larger amount of
code to be written,
and the resulting code is neither easy to manually prove correct nor readily machine-checkable
for correctness.

\begin{figure}
  \centering
\begin{minipage}{.5\textwidth}
\begin{lstlisting}
int greet(bool mom) {
  if (mom) {
    print("Hi Mom!");
  } else {
    print("Hi!");
  }
}
\end{lstlisting}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\resizebox{\textwidth}{!}{%
\begin{forest}
[Program
  [Function
    [Type]
    [ID]
    [{Parameter$\ast$}]
    [Block
      [IfStmt
        [Expr]
        [Call]
        [Call]
      ]
    ]
  ]
]
\end{forest}
}%resizebox
\end{minipage}
\caption{Simple imperative program.
On the left: the source code of the program.
On the right: a simplified AST matching the program.
}
\label{fig:ast1}
\end{figure}


\subsection{Reference Attribute Grammars}
\label{sec:rags}

\emph{Attribute Grammars} (AGs) were introduced by Knuth \cite{DBLP:journals/mst/Knuth68} as a formalism
for defining the semantics of a programming language with attributes on the nonterminals of
an AST and equations on production rules.
An attribute value can represent static
properties like the type of an expression, or whether or not a variable use has a previous assignment.
We can view attributes as derived properties of the AST: the attribute values are computed as
functions of the AST and their values are used in computing other attribute values.

\emph{Reference Attribute Grammars} (RAGs) are an extension of attribute grammars to object
oriented programming proposed by Hedin \cite{DBLP:journals/informaticaSI/Hedin00}.
In RAGs, attributes can be references pointing to nonterminals.
Reference attributes work well, for example, for computing graphs derived from the AST,
like control flow graphs and class dependence graphs, among others.

RAGs are useful for building extensible and modular language implementations
and static analyses, a task that was by some considered impractical in classical AGs.\footnote{%
\textcite{DBLP:journals/cj/DueckC90} mention some of the problems in using plain AGs for language implementation in practice.
Additionally, \textcite{DBLP:conf/pldi/FarrowS89} reports advantages and disadvantages of using AGs
for a large compiler implementation (40+ thousand lines).}
Modularity has long been an active research topic in attribute grammars.
Previous proposals for improving modularity in AGs include
modular AGs by \textcite{DBLP:journals/cj/DueckC90},
composable AGs by \textcite{DBLP:conf/popl/FarrowMY92},
and generic AGs by \textcite{saraiva1999generic},
among others.\footnote{For an overview of modular and composable AGs, see \textcite{DBLP:journals/acta/KastensW94}.}
RAGs have been particularly successful compared to the previous approaches to modularity
in AGs. The main difference is that RAGs use reference attributes which point to nodes in the AST.
RAGs also incorporate higher-order attributes \cite{DBLP:conf/pldi/VogtSK89}, a
previous extension to AGs which was also useful for modularity.

In an object-oriented setting, nonterminals correspond to abstract classes, and production rules
correspond to concrete subclasses. However, it is often convenient to
create subclasses of concrete classes, and this does not directly correspond to
nonterminals or productions. Thus, in the following, we will only view attributes and
equations as properties of AST classes, or AST nodes when speaking of a particular instantiation.

An important property of attributes is that they are declarative:
attribute equations specify what is computed, not the exact order of attribute
evaluations needed to compute the result.
An \emph{attribute evaluator} is used to dynamically schedule attribute evaluation in order
to compute the required attribute values.

Attribute evaluators are free to \emph{memoize} attributes. Memoization, also referred to
as \emph{caching}, means that the
attribute value is stored after it has been computed so that it can be reused the next time the
attribute value is needed.  Memoization can be used to
minimize attribute computation time \cite{DBLP:conf/programm/Jourdan84}.\footnote{%
Söderberg developed a
method to automatically select which attributes to memoize to improve performance in practice \cite{DBLP:conf/sle/SoderbergH10}.}
In addition to memoization, attribute evaluators may even parallelize attribute
computation, or incrementally update attributes when the underlying AST changes.
My most important contribution in this thesis is concurrent evaluation
algorithms for RAGs (Paper~IV).
Concurrent evaluation can be used for parallelizing an existing attribute-based compiler
without editing attribute equations, provided that the attributes are well-defined.
Parallelization is possible due to the fact that well-defined attributes are
\emph{observationally pure}, meaning
that they always compute the same value given identical inputs (just like pure functions)
\cite{DBLP:conf/fase/Naumann05}.

It is important to note that the presence of reference attributes makes it impossible to compute a
static attribute
evaluation schedule, as is possible with Knuth style AGs. Reference attributes can, in general,
point to any nonterminal in the AST and it is not possible to know their dependencies a priori.
Reference attributes are thus dynamically scheduled and evaluated on demand.

The following sub-sections introduce several different kinds of attributes which
are part of RAGs and common extensions to AGs available in the JastAdd metacompiler.
The JastAdd metacompiler itself is introduced in \secref{sec:jastadd}.


\subsubsection{Synthesized Attributes}

The simplest kind of attribute is the synthesized attribute. A synthesized attribute
can be seen as propagating information upwards through the AST (towards the root). Synthesized attributes are
defined by equations on AST nodes. An attribute equation can use other attributes or
children of the node that the equation belongs to.

A simple synthesized attribute is declared using the following notation:

\begin{alignat*}{3}
& \textbf{\textit{syn}} \; \textbf{\textit{int}} & \; A.&x && \\
& \textbf{\textit{eq}} & \; A.&x &&= 3
\end{alignat*}

\noindent
This declares a \emph{\textbf{syn}thesized} attribute $x$ on the AST class $A$.
Any AST node of type $A$ will have an instance of the attribute.
The equation for the attribute is given on the second line. An equation must exist for
each concrete subclass of $A$. If an equation is given on $A$ itself, then classes lacking an
equation will just use the equation from $A$.

For a more practical example of synthesized attributes, we will consider an attribute that determines if an
expression has a static constant value. Such expressions are useful to find
because they can be simplified by replacing them with the corresponding constant value, known as \emph{constant value folding}.
The following abstract grammar declares a small expression language for this example:

%syn boolean Expr.isConstant();
%eq Add.isConstant() =
%    getLeft().isConstant() && getRight().isConstant();
%eq IntLiteral.isConstant() = true;

\begin{alignat*}{2}
\texttt{abstract} \; & \textit{Expr} & & \\
\textit{Add} : \; & \textit{Expr} & & \Coloneqq \; \textit{Left:Expr} \; \textit{Right:Expr} \\
\textit{IntLiteral} : \; & \textit{Expr} & & \Coloneqq \; \langle\textit{VALUE}\rangle \\
\textit{VarUse} : \; & \textit{Expr} & & \Coloneqq \; \langle\textit{NAME}\rangle
\end{alignat*}

\noindent
In this grammar, \emph{IntLiteral} represents constant numbers like $1024$
or $-37$,
\emph{VarUse} represents variable uses like $a$, and
\emph{Add} means an addition expression like $a+5$.
Note that \emph{Expr} is declared \verb'abstract': this just means that
\emph{Expr} itself can never occur in any valid expression, and thus we need
not write equations for attributes on \emph{Expr}.\footnote{In some cases
it is convenient to have equations on abstract classes like \emph{Expr}.
This is discussed in more detail in \secref{sec:oo-attr}.}

\newpage
For identifying constant expressions, observe that an addition expression has a constant
value if both terms are constant. Second, a literal value is always constant.
For example, the expression $2+3$ has a constant value of
$5$, as opposed to the expression $a+5$ which can vary depending on the value of
variable $a$. A synthesized attribute based on these ideas could look
like this:

\begin{alignat*}{3}
& \textbf{\textit{syn}} \; \textbf{\textit{boolean}} & \; \textit{Expr}.&\textit{isConstant} && \\
& \textbf{\textit{eq}} & \; \textit{Add}.&\textit{isConstant} &&= \textit{Left}.\textit{isConstant} \land \textit{Right}.\textit{isConstant} \\
& \textbf{\textit{eq}} & \; \textit{IntLiteral}.&\textit{isConstant} &&= \textbf{\textit{true}}
\end{alignat*}

\noindent
This declares the synthesized attribute \emph{isConstant}.
On the first line, 
the attribute is declared on \emph{Expr} and its subtypes with type \emph{\textbf{boolean}},
after which equations are given for \emph{Add} and \emph{IntLiteral}. The
equations are specified with the \emph{\textbf{eq}} keyword and without repeating the attribute type.
Note that the equations directly encode the facts about constant expressions which we observed
previously.

From the above equations it appears as if there only exists constant expressions, but
we have not given an equation for \emph{isConstant} on variable uses yet.
Our goal is to handle expressions with variables, like $5+x$, as illustrated in \figref{fig:syn1}.
To achieve this, we add a new attribute equation so that variables are not considered constant:

\begin{align*}
\textbf{\textit{eq}} \; \textit{VarUse}.&\textit{isConstant} = \textbf{\textit{false}}
\end{align*}

%\begin{lstlisting}
%eq VarUse.isConstant() = false;
%\end{lstlisting}

With just the three attribute equations above we have made a very simple but functional constant
value analysis.  Better yet, we could combine the above equations with a complete language
specification to enable simple constant value folding in a real compiler. Constant value folding also requires
that we compute the constant value, which can be done with these attributes:

%syn int Expr.constantValue();
%eq Add.constantValue() =
%    getLeft().constantValue() + getRight().constantValue();
%eq IntLiteral.constantValue() = Integer.parseInt(getVALUE());
%eq VarUse.constantValue() = 0;

\begin{alignat*}{3}
& \textbf{\textit{syn}} \; \textbf{\textit{int}} & \; \textit{Expr}.&\textit{constValue} && \\
& \textbf{\textit{eq}} & \; \textit{Add}.&\textit{constValue} &&= \textit{Left}.\textit{constValue} + \textit{Right}.\textit{constValue} \\
& \textbf{\textit{eq}} & \; \textit{IntLiteral}.&\textit{constValue} &&= \textit{parseInt}(\textit{VALUE}) \\
& \textbf{\textit{eq}} & \; \textit{VarUse}.&\textit{constValue} &&= 0
\end{alignat*}

\noindent
The last equation here is meaningless and will never be used. However, we must
include it or else the \emph{constValue} attribute is incomplete as it is
conceivable that it could be used in some computation.\footnote{The metacompiler
that compiles this attribute code does not use precise enough static
analysis to allow omitting the equation.}

With the \emph{isConstant} and \emph{constValue} attributes, a compiler can perform
constant folding of subexpressions by replacing each subtree which has \emph{isConstant} $=$ \textbf{\textit{true}}
with an \emph{IntLiteral} containing the value of \emph{constValue}.
For instance, in our example language, the
expression $a+32+64$ can be replaced by $a+96$ without affecting the meaning of the
expression. \figref{fig:const-fold} shows
the corresponding AST before and after constant folding.

{\def\tt#1{{\normalfont\texttt{#1}}}
\begin{figure}[p]
  \centering
  \begin{forest}
    for tree={l=12ex},
    [Add, name=add
      [{IntLiteral \tt{"5"}}, name=left, edge label={node[edgelbl, near end]{Left}}]
    [{VarUse \tt{"x"}}, name=right, edge label={node[edgelbl, near end]{Right}}]
    ]
    \node[attribute, below right = -2mm of add] {isConstant $=$ \textbf{false}};
    \node[attribute, below left = -2mm of left] {isConstant $=$ \textbf{true}};
    \node[attribute, below right = -2mm of right] {isConstant $=$ \textbf{false}};
  \end{forest}
  \caption{Attributed AST for the expression $5+x$. The \emph{isConstant} attribute
    instances are displayed as gray boxes attached to nonterminals (AST nodes).}
  \label{fig:syn1}
\end{figure}

\begin{figure}[p]
  \centering
  \vbox{
  \begin{forest}
    for tree={l=12ex},
    [Add, name=add1
      [{VarUse \tt{"a"}}, name=a, edge label={node[edgelbl]{Left}}]
      [Add, name=add2
        [{IntLiteral \tt{"32"}}, name=i1, edge label={node[edgelbl, near end]{Left}}]
        [{IntLiteral \tt{"64"}}, name=i2, edge label={node[edgelbl, near end]{Right}}]
      ]
    ]
    \node[attribute, below right = -2mm of add2] {constValue $= 96$};
    \node[attribute, below left = -2mm of i1] {constValue $= 32$};
    \node[attribute, below right = -2mm of i2] {constValue $= 64$};
  \end{forest}
  }
  \vspace{3em}
  \vbox{
  \begin{forest}
    for tree={l=12ex},
    [Add, name=add1
      [{VarUse \tt{"a"}}, name=a, edge label={node[edgelbl]{Left}}]
      [{IntLiteral \tt{"96"}}, name=i1, edge label={node[edgelbl]{Right}}]
    ]
  \end{forest}
  }
  \caption{AST for the expression $a+32+64$, before (above) and after (below) constant value folding.}
  \label{fig:const-fold}
\end{figure}
}
\cleartoleftpage

\subsubsection{Inherited Attributes}
\label{sec:inh}

Inherited attributes are used to access information from a parent of an AST node.
This is in contrast to synthesized attributes which only directly use values and
attributes of the AST node they belong to.

The name for inherited attributes comes from the fact that they
inherit their value through the AST structure. Note that this is different from object-oriented
inheritance. In an object-oriented setting, it is also
possible for a subclass to inherit an attribute equation from a superclass, through the
class hierarchy. To avoid
confusion I explicitly refer to the latter case as object-oriented inheritance.

Equations for an inherited attribute are specified on edges of the AST.
For example, the following notation declares an inherited attribute $x$ on class
$B$, with a matching equation on the edge $A \to B$:

%inh int B.x();
%eq A.getB().x() = 3;

\begin{alignat*}{3}
& \textbf{\textit{inh}} \; \textbf{\textit{int}} & \; B.&x && \\
& \textbf{\textit{eq}} & \; A.B.&x &&= 3
\end{alignat*}

\noindent
This attribute is illustrated in \figref{fig:inh-edge}. In this case, $A$ is a parent of $B$,
in some AST following the abstract grammar.
If $B$ could also be a child of some other
class $P$, say, then another equation must be added on edge $P \to B$.

For the simplest form of inherited attributes, an equation must exist for all possible parent edges.
However, this may result in many so-called copy attributes which just pass an attribute value along
down the AST. This can be neatly solved by a generalization of inherited attributes known as
\emph{broadcasting} \cite{Hedin2011}. With broadcasting, an equation is needed only
for at least one ancestor edge for each possible AST instance.
Broadcasting is illustrated in
\figref{fig:inh-broadcast}: node $D$ inherits the value for attribute $y$ through the equation on
the edge $A \to C$. Without broadcasting, an equation would have been needed on edge $C
\to D$. The closest ancestor edge with a matching equation is the one used to evaluate
the attribute value. This means that a closer equation can shadow one that is further up in the tree,
as in \figref{fig:inh-shadow}.

For an example of a practical inherited attribute, consider the problem of
finding the receiver expression of a method call in a language where methods
are invoked on objects via dot expressions:

\begin{lstlisting}
a.m().n()
\end{lstlisting}

\noindent
The receiver expression is just the left-hand side of the dot.
In the above case, \verb'a' is the receiver expression in the call to \verb'm()',
and \verb'a.m()' is the receiver of \verb'n()'.

Here is the relevant part of the abstract grammar for the small method call language:

%abstract Expr;
%Dot : Expr ::= Left:Expr Right:Expr ;
%Call : Expr;

\begin{alignat*}{1}
\texttt{abstract} \; & \textit{Expr} \\
\textit{Dot} : \; & \textit{Expr} \; \Coloneqq \; \textit{Left:Expr} \; \textit{Right:Expr} \\
  \textit{Id} : \; & \textit{Expr} \\
  \textit{Call} : \; & \textit{Expr}
\end{alignat*}

\begin{figure}[p]
  \centering
  \begin{forest}
    [A, s sep=10ex, for children={l=15ex}
      [B, name=B, edge={very thick, draw=red}, edge label={node[edgelbl, color=red]{$A.B.x = 3$}}]
      [C [D] [E]]
    ]
    \node[attribute, fill=white!60!red, below right = -1ex of B] {$x = 3$};
  \end{forest}
  \captionof{figure}{The value of $B.x$ is given by an equation on the edge $A \to B$.}
  \label{fig:inh-edge}
\vspace{2em}
\begin{minipage}[t]{.45\textwidth}
  \centering
  \begin{forest}
    [A, s sep=10ex, for children={l=15ex}
      [B]
      [C, edge={very thick, draw=red}, edge label={node[edgelbl, color=red]{$A.C.y = 6$}}
        [D, name=D, edge={very thick, draw=red}]
        [E, name=E, edge={very thick, draw=red}]
      ]
    ]
    \node[attribute, fill=white!60!red, below left = -1ex of D] {$y = 6$};
    \node[attribute, fill=white!60!red, below right = -1ex of E] {$y = 6$};
  \end{forest}
  \captionof{figure}{The values of $D.y$ and $E.y$ are given through broadcasting from the
    equation on edge $A \to C$.}
  \label{fig:inh-broadcast}
\end{minipage}%
\hfill%
\begin{minipage}[t]{.45\textwidth}
  \centering
  \begin{forest}
    [A, s sep=10ex, for children={l=15ex}
      [B]
      [C, edge={very thick, draw=red}, edge label={node[edgelbl, color=red]{$A.C.y = 6$}}
        [D, name=D, edge={very thick, draw=red}]
        [E, name=E, edge={very thick, draw=black!40!green}, edge label={node[pos=0.8, auto, color=black!40!green]{$C.E.y = 8$}}]
      ]
    ]
    \node[attribute, fill=white!60!red, below left = -1ex of D] {$y = 6$};
    \node[attribute, fill=white!55!green, below right = -1ex of E] {$y = 8$};
  \end{forest}
  \captionof{figure}{For node $E$, the equation on edge $C \to E$ shadows the one
    on edge $A \to C$.}
  \label{fig:inh-shadow}
\end{minipage}
\end{figure}

\begin{figure}[p]
  \centering
  \begin{forest}
    [Dot
      [Id, name=recv1]
      [Dot, edge={very thick, draw=red}, edge label={node[pos=0.9, auto, color=red]{$\textit{Dot.Right.receiver} = \textit{Left}$}}
        [Call, name=access1, edge={very thick, draw=red}]
        [Call, name=access2, edge={very thick, draw=black!40!green}, edge label={node[pos=0.8, auto, color=black!40!green]{$\textit{Dot.Right.receiver} = \textit{Left}$}}]
      ]
    ]
    \node[attribute, fill=white!60!red, below left = -2mm of access1] (lookup1) {receiver};
    \node[attribute, fill=white!55!green, below right = -2mm of access2] (lookup2) {receiver};
    \draw[-latex, very thick, color=red] (lookup1) to [bend left] (recv1);
    \draw[-latex, very thick, color=black!40!green] (lookup2) to [bend left] (access1);
  \end{forest}
  \caption{AST for the expression \texttt{a.m().n()}. Note that the
receiver of the left-hand call is defined by the upper equation, whereas the
receiver of the right-hand call is defined by the lower equation.}
  \label{fig:inh1}
\end{figure}

Assuming that AST nodes contain a parent reference, we could access the receiver without
using attributes by following the parent link of a
\emph{Call} and then inspecting the left-hand child of the parent. However, we then
have to consider the case when calls are chained, leading to calls occurring also as the
left-hand side of a \emph{Dot}. In the case with a call in the left-hand side we would have to go up
one additional level to find the matching receiver expression.
The problem of finding the receiver can more simply be expressed as an inherited attribute:

%inh Expr Call.receiver();
%eq Dot.getRight().receiver() = getLeft().receiver();

\begin{alignat*}{3}
& \textbf{\textit{inh}} \; \textit{Expr} & \; \textit{Call}.&\textit{receiver} && \\
& \textbf{\textit{eq}} & \; \textit{Dot.Right}.&\textit{receiver} &&= \textit{Left}
\end{alignat*}

\noindent
This attribute specification works directly with chained calls. There will be one equation for each
\emph{Dot}
but it applies only to the \emph{receiver} attributes of the \emph{Right} child.
For the \emph{Left} child, an equation of an enclosing \emph{Dot} applies.\footnote{%
This example assumes that a \emph{Call} in this language is never allowed to be used
outside a \emph{Dot} expression. Otherwise, we would need additional equations
for the \emph{receiver} attribute to be complete.}
\figref{fig:inh1} illustrates this situation.

Broadcasting is used in the \emph{receiver} attribute to get the right reference for
left-hand children of a \emph{Dot} without directly giving an equation for that child.
The equation given for the right child of a \emph{Dot}
does not distinguish between which part of the right child the attribute is in.
Note that with broadcasting,
an inherited attribute equation defines an attribute value for a subtree at
a particular AST edge and broadcasts the value to matching attributes below that edge.


\subsubsection{Parameterized Attributes}

A natural extension to attributes is to allow parameterization of the attribute equation.
This is useful when an attribute answers a question with one or more unknowns.
A common example is an attribute that checks if a variable declarator declares a variable with
a given name:

%syn boolean Declarator.declares(String name) = getNAME().equals(name);

\vbox{%
\begin{gather*}
\textit{Declarator} \; \Coloneqq \; \langle\textit{NAME}\rangle \\
\textbf{\textit{syn}} \; \textbf{\textit{boolean}} \; \textit{Declarator}.\textit{declares}(\textit{String} \; n) = (n = \textit{NAME})
\end{gather*}
}

\noindent
Another application for parameterized attributes are attributes that select
one of several possible values, like the individual declarations in a multiple
variable declaration statement:

%syn Declarator VarDecl.declAt(int index) = getDeclarator(index);

\vbox{%
\begin{gather*}
\textit{VarDecl} \; \Coloneqq \; \textit{Declarator}\ast \\
\textbf{\textit{syn}} \; \textit{Declarator} \; \textit{VarDecl}.\textit{declAt}(i) = \textit{Declarator}[i]
\end{gather*}
}

Both synthesized and inherited attributes can be parameterized, as well as a few other types of
attributes discussed in the following sections (circular and higher-order).

\goodbreak
Parameterized attributes have many practical uses in compilers and static analysis construction.
A typical use case is for name lookup, where the name lookup attributes take as parameter
the name being looked up.
Other interesting applications for parameterized attributes occur when
combined with higher-order attributes, for instance the parameterized type
lookup attribute in ExtendJ (see \secref{sec:exj-name-analysis}).

In the formalization of RAGs by \textcite{Buckley:2017:FPR:3136014.3136024},
inherited attributes are implemented with parameterized synthesized attributes
and by having a parent reference as an intrinsic attribute of each node in the AST.


\subsubsection{Higher-Order Attributes}
% NOTE: The desugaring part here sort of overlaps with the corresponding part
% in the Using RAGs for Extensible Analyses section.

\emph{Higher-Order Attributes} (HOAs) \cite{DBLP:conf/pldi/VogtSK89}
are attributes which compute a derived subtree of the AST. The derived subtree is
considered as part of the AST it belongs to and can itself have attributes.
Lazy evaluation is necessary for HOAs, because they can represent infinite trees.

HOAs, also known as \emph{Non-Terminal Attributes} (they are both nonterminals and attributes),
are declared by adding the \emph{\textbf{nta}} keyword
to an ordinary attribute declaration:

\begin{equation*}
\textbf{\textit{syn}} \; \textbf{\textit{nta}} \; X \; A.x = \textbf{\textit{new}} X
\end{equation*}

To illustrate HOAs, they are shown as part of the AST but connected to the attribute
with a dashed edge, like this:

\vspace{1em}
\begin{center}
\begin{forest}
[A, name=A, for children={l=8ex} [B] [X, name=X, no edge]]
\node[attribute, below right = -2mm of A] (x) {x};
\draw[dashed] (x) to (X);
\end{forest}
\end{center}
\vspace{1em}

A HOA must compute a fresh object. That is, a new part of
the AST must be built which is not allowed to link to an existing subtree of the
AST -- otherwise it would destroy the tree structure with
a subtree that has multiple parents.
Existing parts of the AST can be safely reused in a HOA by copying the relevant parts instead
of linking them directly. This is a simple rule to follow, but easy to forget or accidentally break.

Higher-order attributes are useful for computing AST structures that were not built directly
by the parser. This has many applications, for example:
normalizing syntactic sugar and reifying implicit
program elements. These applications are explained in more detail below.

\newpage
\paragraph{Normalizing Syntactic Sugar}

Syntactic sugar is a term used to describe syntax elements in a programming language
which map directly to other, more elementary, language constructs.\footnote{The term syntactic sugar was coined by
Peter J. Landin in 1964.}
Examples include the \verb'+=' operator in the C language,
which is equivalent to addition and assignment.

For program analysis it is useful to
\emph{desugar} syntactic sugar into a corresponding elementary form, so that fewer special cases
must be handled by static analysis. This normalization consists of transforming the AST where
the specialized syntax occurs.\footnote{Normalization is additionally important during code
generation, where the program is transformed into a very simple form to enable general optimizations
that work on many different surface syntaxes.}

With HOAs, we can compute the normalized AST as an attribute. The normalized AST must be explicitly
accessed to use the transformed version. This is a small overhead compared to using imperative tree
transformations.  \emph{Forwarding} is an extension of HOAs in which the attribute implicitly replaces the
sugared AST \cite{DBLP:conf/cc/WykMBK02}.


\paragraph{Reifying Implicit Constructs}

Java is an example of an object-oriented language that has a default supertype of all classes, named
\texttt{Object}. The \texttt{Object} class is never declared but behaves as any other declared class (except that it
does not have a superclass). The \texttt{Object} class must be somehow represented internally in a Java
compiler, and since it behaves just like any other Java class it should be placed in the class
table among the other library classes (which are parsed from library class files).

A more complex application of reifying constructs is for polymorphic type instantiation. This use case is
described in more detail in \secref{sec:exj-polytypes}.

\paragraph{Turing Machines}
HOAs can be used to evaluate any Turing machine. Although this is quite an esoteric application,
it illustrates how HOAs can evaluate infinite ASTs.

For the Turing machine encoding we need an attribute that
computes the next configuration of a Turing machine:

%syn nta Configuration Configuration.next() = ...

\begin{equation*}
\textit{\textbf{syn}} \; \textit{\textbf{nta}} \; \textit{Configuration} \; \textit{Configuration}.\textit{next}
\end{equation*}

\noindent
Here, \emph{Configuration} is a nonterminal containing the configuration of a Turing machine
(tape content, head position, and current state).
A Turing machine starts out in an initial configuration, and the successor configuration is computed
by the higher-order \emph{next} attribute, which itself has a successor configuration.
To run a Turing machine until it halts, the successor attribute is evaluated until we reach a
halting state. The \emph{next} attribute is
expanded indefinitely if the machine setup corresponds to a non-halting Turing machine.
The following diagram illustrates the AST for a Turing machine:

\vspace{1em}

\begin{center}
\begin{forest}
[TuringMachine
  [{Instruction$\ast$}]
  [Configuration, name=c1
    [Configuration, name=c2, l=10ex, no edge
      [$\cdots$, name=c3, l=10ex, no edge]
    ]
  ]
]
\node[attribute, below = -2mm of c1] (n1) {next};
\node[attribute, below = -2mm of c2] (n2) {next};
\draw[dashed] (n1) to (c2);
\draw[dashed] (n2) to (c3);
\end{forest}
\end{center}

\vspace{1em}

\noindent
The instruction list stored at the root of the AST, in \emph{TuringMachine}, represents the action
table of the Turing machine. When evaluating the next configuration, the matching instruction for
the current state and symbol under the head is found in the instruction list.  An inherited
attribute is used to access the instruction list from any configuration.

An implementation of this encoding of Turing machines in RAGs is available
at the following public repository:
\url{https://bitbucket.org/joqvist/turing}.


\subsubsection{Circular Attributes}

Circular attributes are attributes which depend (indirectly) on themselves.
The semantics of circularly attributes were first proposed
for AGs by \textcite{DBLP:conf/sigplan/Farrow86} and improved by \textcite{DBLP:journals/toplas/Jones90}.
Circular attributes were later adapted for RAGs by \textcite{DBLP:journals/scp/MagnussonH07}.
A circular attribute can be used to express fixpoint functions, which occur
in control flow analysis, dataflow analysis, and type inference.
An example of such a fixpoint function is the reachable procedures from a given procedure,
which is conveniently defined as follows:

\begin{equation*}
\begin{split}
%\textit{reachable}(p) & = \bigcup _{c \in \textit{calls}(p)}{\{c\} \cup \textit{reachable}(c)} \\
  \textit{calls}(p) & = \{ \; \text{procedures called by \textit{p}} \; \} \\
  \textit{reachable}(p) & = \{ p \} \cup \left( \bigcup _{c \in \textit{calls}(p)}{\textit{reachable}(c)} \right) \\
\end{split}
\end{equation*}

%A fixpoint function is which is recursively defined, and evaluation of 
Evaluation of circular attributes is done by fixpoint iteration: the equation is computed until
its value reaches a fixed point. For circular attributes we require that
the attribute is well-defined according to the following criteria:
the attribute equation is monotone with values arrangeable in a lattice of finite height.
This ensures that there is a single least fixed point.

To declare a circular attribute, the attribute is provided
with an initial value for the fixpoint iteration.
Given a well-defined circular attribute, the fixpoint iteration will terminate
and give a single well-defined value.

\vbox{
The reachable procedures function above can be implemented as a circular attribute in
the following way:

%  syn Set<Procedure> Procedure.reachable() circular [ emptySet() ] =
%     concat(
%        of(this),
%        calls().stream().flatMap(c -> c.reachable().stream())
%      ).collect(toSet());

\begin{align*}
& \textit{\textbf{syn}} \; \textit{Set}\langle\textit{Procedure}\rangle \; \textit{Procedure}.\textit{reachable} \; \textit{\textbf{circular}}(\emptyset) = \\
& \qquad \{ \textit{\textbf{this}} \} \cup \left( \bigcup _{c \in \textit{\textbf{this}}.\textit{calls}}{c.\textit{reachable}} \right)
\end{align*}
}

\noindent
The $\textit{\textbf{circular}}(\emptyset)$ part gives the initial value for the fixpoint iteration.
\figref{fig:reachable} shows the reachable procedures in a small imperative program.

\begin{figure}
  \centering
\begin{lstlisting}
// External declarations:
bool isleaf(Tree t);
int min(int a, int b);
void print(int a);

// Finding minimum leaf value:
int tmin(Tree t) {
  if (isleaf(t)) {
    return t.value;
  } else {
    return min(tmin(t.left), tmin(t.right));
  }
}

void findmin(Tree t) {
  print(tmin(t));
}
\end{lstlisting}
\begin{tabular}{lll}
\toprule
\emph{Procedure} & \emph{Calls} & \emph{Reachable} \\
\midrule
\texttt{findmin} & $\{ \texttt{print}, \texttt{tmin} \}$ & $\{ \texttt{print}, \texttt{findmin}, \texttt{tmin}, \texttt{min}, \texttt{isleaf} \}$\\
\texttt{tmin} & $\{ \texttt{tmin}, \texttt{min}, \texttt{isleaf} \}$ & $\{ \texttt{tmin}, \texttt{min}, \texttt{isleaf} \}$\\
\texttt{min} & $\{\}$ & $\{ \texttt{min} \}$\\
\texttt{isleaf} & $\{\}$ & $\{ \texttt{isleaf} \}$\\
\texttt{print} & $\{\}$ & $\{ \texttt{print} \}$\\
\bottomrule
\end{tabular}
\caption{Reachable procedures for a small program for finding the minimum leaf value in a tree.
Above: the source code of the program.
Below: reachable procedure sets.
}
\label{fig:reachable}
\end{figure}



\subsubsection{Collection Attributes}

In static analysis it is often necessary to 
collect a multitude of values of some kind from different
nodes in an AST. Examples include error messages, local procedure calls, and static type
dependencies.  \emph{Collection attributes} are an extension for RAGs by
\textcite{DBLP:conf/scam/MagnussonEH07} which
supports these kinds of value collections.
The attribute evaluator traverses the AST to collect contributions for
the attribute value from disparate nodes in the tree.

To declare a collection attribute, we must declare what is to be collected and on which
class:

%coll Collection<String> Program.errors();

\begin{equation*}
\textit{\textbf{coll}} \; \textit{Collection}\langle\textit{String}\rangle \; \textit{Program}.\textit{errors}
\end{equation*}

\noindent
This attribute collects error messages on the \emph{Program} class (the
root of the AST). The meaning of the attribute is only implicit from the attribute name
(\emph{errors}); the actual meaning of the attribute is defined by
\emph{contribution statements} declared for all classes which may provide a
value for this collection attribute. In this case, we need a contribution statement for
each class that may report an error message:

%VarUse contributes "variable not initialized before use"
%    when !definitelyAssigned()
%    to Program.errors();

\begin{align*}
\textit{VarUse} \; & \textit{\textbf{contributes}} \; \texttt{"variable not initialized before use"} \\
  & \textit{\textbf{when}} \; \neg \, \textit{definitelyAssigned} \\
  & \textit{\textbf{to}} \; \textit{Program}.\textit{errors}
\end{align*}

\noindent
This contribution is \emph{conditional}, where the \textit{\textbf{when}} clause contains an expression
that controls if the error message will be reported for the current node or not.
It is also possible to declare unconditional contributions, for example when collecting
static type dependencies.

\subsection{Attribute-Controlled Rewrites}

In \emph{Rewritable Reference Attribute Grammars} \cite{DBLP:conf/ecoop/EkmanH04},
parts of the AST can be automatically transformed
by using attribute-controlled rewrite rules.
While higher-order attributes can also be used to transform part of the AST, they need to be
explicitly referenced in order
to access the transformed version of the AST. In contrast, rewrite rules are invisible to
other attributes: they are automatically activated whenever the rewritable part of the AST is
first accessed.

Rewrite rules can have an optional condition, which decides if the rewrite will be applied
to the target node.

The following rewrite rule replaces any multiplication by zero with a constant zero literal:

%  rewrite MinusExpr {
%    when (getOperand() instanceof IntegerLiteral
%        && ((IntegerLiteral) getOperand()).isDecimal()
%        && getOperand().isPositive())
%    to IntegerLiteral {
%      IntegerLiteral original = (IntegerLiteral) getOperand();
%      return new IntegerLiteral("-" + original.getLITERAL());
%    }
%  }

\begin{align*}
& \textit{\textbf{rewrite}} \; \textit{MulExpr} \\
& \qquad \textit{\textbf{when}} \; \textit{Left}.\textit{isZero} \lor \textit{Right}.\textit{isZero} \\
& \qquad \textit{\textbf{to}} \; \textit{\textbf{new}} \; \textit{IntegerLiteral}(\texttt{"0"})
\end{align*}

Rewrite rules are a simple way of transforming parts of the AST.
Unlike higher-order attributes, a rewrite rule replaces the original part of the AST that is
rewritten. With a higher-order attribute, on the other hand, it is always possible to access the original AST,
making it easier to pretty-print the original source form of the program.

\textcite{DBLP:journals/cl/SoderbergH15} showed that rewrites are equivalent to circular higher-order
attributes. This mapping makes it simple to implement rewrites
in a RAG system which already has circular and higher-order attributes.


\subsection{The JastAdd Metacompiler}
\label{sec:jastadd}

JastAdd is a metacompiler\footnote{A compiler that compiles compilers; a compiler for a metalanguage.}
for \emph{Reference Attribute Grammars} (RAGs) which has found success in
modular programming language composition and for real-world programming language implementation
\cite{DBLP:journals/scp/HedinM03,Hedin2011,DBLP:journals/scp/EkmanH07}.

JastAdd generates Java AST classes to represent the nonterminals of an abstract grammar.
Attributes are generated as Java methods in these AST classes,
based on a set of attribute specifications.
JastAdd attributes are specified in a domain-specific language with embedded Java
code for attribute equations. This language uses \emph{Inter-Type Declarations} (ITDs):
a concept from \emph{Aspect-Oriented Programming} (AOP) in which the attribute is declared
separately from the class it belongs to. It is also possible to declare new utility methods in
AST classes as ITDs.\footnote{The flavor of AOP used in JastAdd is similar to subject-oriented
programming \cite{DBLP:conf/oopsla/HarrisonO93}.}

%Since attribute equations can involve arbitrary Java code it is imperative that
%the programmer does not use side effects, which would break attribute evaluation.

In previous attribute grammar examples, we have already used a simplified version of
the JastAdd notation for attributes.
Here is the literal JastAdd attribute syntax for a synthesized attribute:

\begin{lstlisting}
syn int A.x() = 3;
\end{lstlisting}

\noindent
This declares a synthesized attribute (hence, \verb'syn') belonging to AST class \verb'A'.
We will continue using a simplified version of the JastAdd syntax that removes semicolons and
the parentheses for non-parameterized attributes.

Thanks to the use of inter-type declarations, attribute declarations can be organized into
aspect files by whatever categorization is most appropriate. An aspect file has the following
layout:

\vbox{
\begin{lstlisting}
import java.util.*;
aspect MyAnalysis {
  <attributes and other ITDs>
}
\end{lstlisting}
}

The JastAdd abstract grammar syntax is similar to the previous grammar examples.
However, there may only be one production rule per
nonterminal, since each production rule corresponds to an AST class declaration.
For alternatives, JastAdd grammars have object-oriented inheritance between
grammar productions. For each alternative, we use separate productions with a common superclass.
Binary expressions are a common example where alternatives are useful in the abstract grammar.
In JastAdd, we can represent all binary expressions with
a common supertype \emph{Binary}, say, and we then have subtypes for each concrete kind of
binary expression, like addition, subtraction, multiplication, etc.

%abstract Binary ::= Left:Expr Right:Expr;
\begin{equation*}
\texttt{abstract} \; \textit{Binary} : \textit{Expr} \; \Coloneqq \; \textit{Left:Expr} \; \textit{Right:Expr}
\end{equation*}

\noindent
The common supertype \emph{Binary} does not represent any specific
kind of binary expression, in fact it will never exist in any concrete AST so we declare it
\verb'abstract'. Concrete subclasses of \emph{Binary} are the actual binary expressions
of the language. Following are some common examples of binary expressions:

\vbox{
\begin{align*}
\textit{Add} & : \textit{Binary} \\
\textit{Sub} & : \textit{Binary} \\
\textit{Mul} & : \textit{Binary} \\
\ldots &
\end{align*}
}

\noindent
By inheriting from \emph{Binary}, these classes automatically receive the child components
of \emph{Binary}. It is possible to add additional child components in a subtype.
In the above example, \emph{Binary} could alternatively have been declared as empty,
and each subtype could have specified the \emph{Left} and \emph{Right} children.
This would, however, remove the possibility for attributes common to all
binary expressions to directly access the children, which would result in less reuse.

JastAdd abstract grammars may use optional, list, and token components:

\begin{equation*}
\textit{A} \; \Coloneqq \; \textit{B} \quad [ \, \textit{C} \, ] \quad \textit{D}\ast \quad \langle\textit{E}\rangle  \\
\end{equation*}

\noindent
Lists and optional children are wrapped by implicitly generated nonterminals \emph{List} and \emph{Opt}.

JastAdd supports all previously discussed attribute kinds: synthesized, inherited, parameterized,
higher-order, circular, and collection attributes. JastAdd also allows automatic
attribute-controlled rewriting of AST nodes, and JastAdd has an aspect-oriented mechanism for
replacing existing attribute equations.

JastAdd generates AST classes and weaves attributes into them.
Attributes are generated as methods of the AST classes.
This forms a foundation for building compilers and static analyses: only a parser needs to
be added in order to build a complete static analysis framework with JastAdd.
In fact, JastAdd has been used to build compilers for several languages like Java, Modelica, and
Bloqqi \cite{jastaddj,DBLP:journals/cce/AkessonAGBT10,DBLP:conf/oopsla/ForsH16}.
In this dissertation we will look mainly at the ExtendJ Java compiler, but the techniques covered
can be applied to any other JastAdd-based compiler project, or indeed, to other RAG-based
compiler specifications.

JastAdd makes it eminently easy to build extensible compilers thanks to RAGs and aspect-oriented
programming. The following section describes how JastAdd features are used for building
extensible static analyses.

%\begin{figure}
%  \centering
%\begin{tikzpicture}
%  { [start chain, ever node/.style={draw}]
%    \node[on chain] (A) {Module A};
%    \node[on chain, join=by {<-}, right=of A] (B) {Module B};
%  }
%\end{tikzpicture}
%\caption{Module~B extends Module~A with new attributes and equations.}
%\end{figure}


\section{Using RAGs for Extensible Analyses}
\label{sec:extension-mechanisms}

In this section, I show how some features of RAGs
can be used to build extensible static analyses.
This is based on my experiences from working with the JastAdd metacompiler.
Although the following discussion focuses on JastAdd RAGs, many of
the results apply to other RAGs as well.

The main features of JastAdd that are of interest from the perspective of extensibility are:

\begin{itemize}
  \item inter-type declarations,
  \item object-oriented attribute inheritance,
  \item attribute replacement,
  \item structure-shy programming with inherited attribute broadcasting,
  \item desugaring for reusable code generation,
  \item collection attributes.
\end{itemize}

\noindent
The following sub-sections examine each of these features in detail.


\subsection{Inter-Type Declarations}

As mentioned in \secref{sec:jastadd}, JastAdd attributes are specified with
inter-type declarations (ITDs).
By using ITDs, JastAdd solves the \emph{expression problem}, a common
yardstick when comparing extensibility of programming language implementations.
The expression problem was defined by \textcite{expressionproblem} as follows:

\begin{quote}
The Expression Problem is a new name for an old problem.  The goal is to define
a datatype by cases, where one can add new cases to the datatype and new
functions over the datatype, without recompiling existing code, and while
retaining static type safety (e.g., no casts).
\end{quote}

With ITDs, new attributes (functions) can be added to pre-existing AST classes.
New AST classes (datatype cases) are also easily added by new abstract grammar rules.
There is one caveat: JastAdd RAGs require recompilation when the attribute grammar
is changed or extended.
Recompilation is needed because JastAdd generates the analysis
or compiler by weaving the introduced attributes into AST class declarations
during code generation.

For an example of the expression problem, I will demonstrate how to extend a
small expression language with a new operator and new functionality by using
JastAdd.  We start with the expression language defined by the following
abstract grammar:

\begin{align*}
\texttt{abstract} \; & \textit{Expr} \; \\
\textit{ParExpr} : \; & \textit{Expr} \; \Coloneqq \; \textit{Expr} \\
\textit{IntLiteral} : \; & \textit{Expr} \; \Coloneqq \; \langle\textit{VALUE}\rangle \\
\texttt{abstract} \; \textit{Binary} : \; & \textit{Expr} \; \Coloneqq \; \textit{Left:Expr} \; \textit{Right:Expr} \\
\textit{Add} : \; & \textit{Binary} \\
\textit{Sub} : \; & \textit{Binary}
\end{align*}

This language can easily be extended with a new factorial operator by appending
the following abstract grammar rule:

\begin{equation*}
\textit{Factorial} : \textit{Expr} \; \Coloneqq \; \textit{Expr}
\end{equation*}

\vbox{
We will now extend the behaviour of the language by adding attributes for
printing an expression in \emph{Reverse Polish Notation} (RPN),
in which operands are written before operators.
To this end, we declare the \emph{rpn} attribute on \emph{Expr}:

\begin{equation*}
\textit{\textbf{syn}} \; \textit{String} \; \textit{Expr}.\textit{rpn}
\end{equation*}
}

\noindent
Equations for the new attribute are needed on each concrete subclass of \emph{Expr}:

\vbox{
\begin{alignat*}{2}
& \textit{\textbf{eq}} \; &\textit{Add}&.\textit{rpn} = \textit{Left.rpn} + \texttt{" "} + \textit{Right.rpn} + \texttt{" +"} \\
& \textit{\textbf{eq}} \; &\textit{Sub}&.\textit{rpn} = \textit{Left.rpn} + \texttt{" "} + \textit{Right.rpn} + \texttt{" -"} \\
& \textit{\textbf{eq}} \; &\textit{ParExpr}&.\textit{rpn} = \textit{Expr.rpn} \\
& \textit{\textbf{eq}} \; &\textit{IntLiteral}&.\textit{rpn} = \textit{VALUE} \\
& \textit{\textbf{eq}} \; &\textit{Factorial}&.\textit{rpn} = \textit{Expr.rpn} + \texttt{" !"}
\end{alignat*}
}%vbox

For the factorial operator, RPN coincides with the standard algebraic notation.
For the binary expressions \emph{Add} and
\emph{Sub}, the operands are printed first, then the operator.  Parenthesis
expressions (\emph{ParExpr}), which are significant in algebraic notation,
are not needed in RPN.  Integer literals (\emph{IntLiteral}) need no
special handling for RPN output. \figref{fig:rpn} shows the AST of an
expression in the extended language and the corresponding RPN
attribute values.

{\def\tt#1{{\normalfont\texttt{#1}}}
\begin{figure}
\centering
\begin{forest}
  [Sub, name=sub
    [{IntLiteral \tt{"10"}}, name=n10]
    [Factorial, name=fact
      [ParExpr, name=par
        [Add, name = add
          [{IntLiteral \tt{"40"}}, name=n40]
          [{IntLiteral \tt{"2"}}, name=n2]
        ]
      ]
    ]
  ]
  \node[attribute, below right = -2mm of add] {rpn $=$ \tt{"40 2 +"}};
  \node[attribute, below right = -2mm of par] {rpn $=$ \tt{"10 40 2 +"}};
  \node[attribute, below right = -2mm of fact] {rpn $=$ \tt{"40 2 + !"}};
  \node[attribute, below right = -2mm of sub] {rpn $=$ \tt{"10 40 2 + ! -"}};
  \node[attribute, below left = -2mm of n10] {rpn $=$ \tt{"10"}};
  \node[attribute, below left = -2mm of n40] {rpn $=$ \tt{"40"}};
  \node[attribute, below right = -2mm of n2] {rpn $=$ \tt{"2"}};
\end{forest}
\caption{Attributed AST for the algebraic expression $10 - (40 + 2)!$.
The \emph{rpn} attribute shows the reverse polish notation at each subexpression. The
RPN for the whole expression is \texttt{10 40 2 + ! -}.}
\label{fig:rpn}
\end{figure}
}

\subsection{Object-Oriented Attribute Inheritance}
\label{sec:oo-attr}

When RAGs are combined with object-oriented programming, attributes can be inherited through
the class hierarchy. This can be useful for factoring out common attributes to an
AST superclass.
For instance, consider the previous RPN example: the \emph{Binary} class
is a common superclass of both \emph{Add} and \emph{Sub} expressions.
We can factor out the separate equations of the \emph{rpn} attribute from \emph{Add} and \emph{Sub}
with a new equation on \emph{Binary}, like this:

\begin{equation*}
\textit{\textbf{eq}} \; \textit{Binary}.\textit{rpn} = \textit{Left.rpn} + \texttt{" "} + \textit{Right.rpn} + \texttt{" "} + \textit{opSuffix}
\end{equation*}

\noindent
Now we just need \emph{opSuffix} on \emph{Add} and \emph{Sub}:

\begin{alignat*}{2}
& \textit{\textbf{syn}} \; \textit{String} \; &\textit{Binary}&.\textit{opSuffix} \\
& \textit{\textbf{eq}} \; &\textit{Add}&.\textit{opSuffix} = \texttt{"+"} \\
& \textit{\textbf{eq}} \; &\textit{Sub}&.\textit{opSuffix} = \texttt{"-"}
\end{alignat*}

\noindent
Although we got rid of some duplicated code by moving the \emph{rpn} equation to
\emph{Binary}, it is of little benefit in the present example, especially since we had
to add a new \emph{opSuffix} attribute to fill in the blank at \emph{Binary}.
This type of attribute factoring pays off to a much greater extent in more realistic programming
languages which typically have many more binary operators.


\subsection{Attribute Replacement}
\label{sec:refine}

In addition to inter-type declarations, JastAdd has another AOP-inspired mechanism:
it is possible to replace an existing attribute
by using a \textbf{\emph{refine}} declaration, like this:

\begin{equation*}
\textit{\textbf{refine}} \; \textit{\textbf{eq}} \; \textit{Sub}.\textit{opSuffix} = \texttt{"?"}
\end{equation*}

\noindent
This changes the equation for an existing attribute \emph{Sub.opSuffix}.

The \textbf{\emph{refine}} mechanism is similar to point-cuts in AOP, but instead of inserting a new computation
it replaces the whole computation.\footnote{It is possible to reuse the old equation inside
the new one by using the \textbf{\emph{refined}} keyword.}

Replacing attributes like this may at first seem to provide little benefit for building
static analyses with RAGs. However, it is very useful when extending an existing
analysis.
When extending an analysis, there is often a need to change the meaning of an existing attribute.
This can either be accomplished by using a \textbf{\emph{refine}} rule, or by creating a subclass of
the corresponding AST class and overriding the attribute. The \textbf{\emph{refine}} solution is
often more lightweight: it requires less boilerplate code and specialization to handle the new AST
class.

When extending a programming language, it is often necessary to change the behaviour of an
existing attribute to
accommodate some new language feature. This can either be done by refactoring the pre-existing
attribute into smaller parts that can be overridden separately in an extension.  However, this can
reduce the readability of the original code which in may outweigh the benefit of reduced
code duplication in the extension.  Furthermore, if the base system can not be changed, use of
\textbf{\emph{refine}} can allow some changes which would otherwise not be possible.


{\raggedright
\subsection{Structure-Shy Programming with Inherited Attributes and Broadcasting}
}

\noindent
Inherited attributes with broadcasting embody a style of programming which was named
\emph{structure-shy} programming by \textcite{lieberherr1996adaptive}.  A structure-shy program
specifies certain behaviour for some substructures and leaves handling of the rest to a generic
solution, according to \textcite{DBLP:journals/scp/CunhaV11}.  A typical example of a structure-shy
program is an XPath query which matches only certain parts of an XML document tree and ignores the
rest \cite{DBLP:reference/snam/X14xcf}.  An XPath query is insensitive to irrelevant changes in the
tree which the query does not directly match.

An inherited attribute equation (with broadcasting) is propagated to all matching attributes in the subtree below
the edge where the equation is attached.  If we add a new
AST node inside some subtree which is already covered by an inherited equation,
there is often no need to add a new equation for that attribute. In several cases,
it is possible to add new language features that reuse existing inherited attributes with
little effort.

The structure-shyness of inherited attributes is particularly apparent in lookup attributes.
Name lookup is a typical example of a lookup attribute: an inherited attribute is used
for finding a reference to the declaration of a variable use. Suppose we have the following
abstract grammar:

%\textit{VarUse} : \textit{Expr} \Coloneqq \; \langle\textit{NAME}\rangle \\
%\textit{VarDecl} \Coloneqq \; \langle\textit{NAME}\rangle \\

\vbox{
\begin{alignat*}{2}
\texttt{abstract} \; & \textit{Expr} \; &&\\
    \textit{Let} : \; &\textit{Expr} \; &&\Coloneqq \; \langle\textit{NAME}\rangle \;
    \langle\textit{VALUE}\rangle \; \textit{Expr} \\
 \textit{VarUse} : \; &\textit{Expr} \; &&\Coloneqq \; \langle\textit{NAME}\rangle \\
\textit{Add} : \; & \textit{Expr} \; &&\Coloneqq \; \textit{Left:Expr} \; \textit{Right:Expr}
\end{alignat*}
}

\noindent
The \emph{Let} expressions of this language are only allowed to define variables with constant
values, and the expression inside the \emph{Let} may only use variable names defined by some
enclosing \emph{Let}.
An example of a valid expression in the language looks like this:

\begin{lstlisting}
let x = 40 in
  let y = 2 in
    x + y
\end{lstlisting}

\noindent
This expression computes the value $42$.

The above language is almost entirely pointless. Nevertheless, name analysis for this language
works much like name analysis in any other programming language. Here is a name lookup
attribute for the \verb'let'-expression language:

\begin{alignat*}{3}
& \textbf{\textit{inh}} \; \textit{Let} & \; \textit{Expr}.&\textit{lookup}(n) && \\
& \textbf{\textit{eq}} & \; \textit{Let.Expr}.&\textit{lookup}(n) &&=
  \; \textit{\textbf{this}} \;\; \textit{\textbf{if}} \;\; \textit{n} = \textit{NAME}; \;
  \textit{\textbf{else}} \;\; \textit{lookup}(n)
\end{alignat*}

The lookup attribute at a declaration node (\emph{Let} expression) will give a reference to the
declaration if its name matches the sought-after variable. If the name $n$ does not match the
declaration, the attribute equation uses $\emph{lookup}(n)$ of the \emph{Let} expression itself
to delegate name lookup to the enclosing scope.

The lookup attribute above is structure-shy in the sense that we could introduce new name
declarations outside the expression, or irrelevant name declarations inside it, without
affecting the lookup attribute value for previous nodes.
For example, the following expression has the same name bindings as in the previous one:

\begin{lstlisting}
let unused = 1 in
  let x = 40 in
    let abc = 0 in
      let y = 2 in
        x + y
\end{lstlisting}

We can add many language extensions to this \verb'let'-language without affecting
name lookup. For instance, if we wish to add a division operator to the language, we could
add the following abstract grammar rule:

\begin{equation*}
\textit{Div} : \; \textit{Expr} \; \Coloneqq \; \textit{Left:Expr} \; \textit{Right:Expr}
\end{equation*}

\noindent
Now, we can write expressions like

\begin{lstlisting}
let x = 355 in
  let y = 113 in
    x / y
\end{lstlisting}

\noindent
Importantly, this extended language works as intended without having to add new attribute
equations for the lookup attribute. This works despite the fact that the new
\emph{Div} node occurs between the pre-existing \emph{Let} and \emph{VarUse}
constructs in the AST.
Because we added a kind of expression which does not declare
new names or alter name scoping rules, the old equations just work.

% TODO: fix section titles spacing
%\vfill
\vspace{0.5em}
\subsection{Desugaring with Higher-Order Attributes}

Higher-order attributes are useful for developing code generation for programming
language extensions, among other things.
Code generation for a new language construct can often be conveniently implemented by mapping the
new language mechanism to an equivalent form using pre-existing language features.

For an example of desugaring, we will look at a small extension to the ExtendJ compiler
which overloads the multiplication operator for string repetition.
Multiplying a string is not allowed in plain Java (as of the current latest version,
Java~11). Our goal is to allow a string to be multiplied with an integer, resulting
in the string repeated a number of times equal to the integer operand.
For instance, the following
expression should store the string \verb'"gogogo"' in variable \verb'msg':

\begin{lstlisting}
String msg = "go" * 3;
\end{lstlisting}

\noindent
The above is equivalent to the following \verb'for'-loop:

\begin{lstlisting}
StringBuilder buf = new StringBuilder();
for (int i = 0; i < 3; ++i) {
  buf.append("go");
}
String msg = buf.toString();
\end{lstlisting}

\noindent
If we wish to implement the code generation for this in ExtendJ,
we could create a higher-order attribute to compute the desugared version:

%syn nta Stmt Mul.desugared() =
%    new ForStmt(...);

\begin{equation*}
\textbf{\textit{syn}} \; \textbf{\textit{nta}} \; \textit{Stmt} \; \textit{Mul.desugared} = \textbf{\textit{new}} \; \textit{ForStmt}(\ldots)
\end{equation*}

\noindent
Code generation can be accomplished by reusing existing code generation on the
desugared form, like this:

\begin{equation*}
\textbf{\textit{eq}} \; \textit{Mul.code} = \textit{desugared.code}
\end{equation*}

\noindent
The details of a complete implementation are just a little bit more involved.
A functional implementation of this example, as a small extension to ExtendJ,
is available as open source from the following public repository:

\begin{center}
\url{https://bitbucket.org/extendj/string-repeat}
\end{center}

The ExtendJ compiler uses desugaring for a few important features. For example, lambda expressions
in the Java~8 module are implemented by desugaring to anonymous classes.
Another example is the try-with-resources statement, introduced in Java~7 \cite[\S 14.20.3]{jls7},
for which partial desugaring is used to simplify code generation.


\subsection{Collection Attributes}
\label{sec:coll-prop}

Collection attributes receive their values from contribution statements, which are declared separately
from the target collection attribute, using ITDs. This makes it easy to introduce new
contributions to an existing collection attribute in an extension.
In language extensions we often need to add new error messages to the compiler, which can be easily
done by adding new contributions if the error messages are collected with a collection attribute.

For an example of adding a new error message to a compiler, suppose we want to
report when strings are compared using the equals operator in Java:

\begin{lstlisting}
if (name == "Not Sure") {
  ...
}
\end{lstlisting}

\noindent
Normally, Java allows this code without warning, though it usually does not work the way
the programmer intended. Strings should instead, in most cases, be compared with the \verb'equals'
method:

\begin{lstlisting}
if (name.equals("Not Sure")) {
  ...
}
\end{lstlisting}

\noindent
An error message can be added to the ExtendJ compiler by just adding a new contribution
for a collection attribute, like this:

%  EQExpr contributes error("String equality testing should be done with .equals()!")
%      when left().type().isString() && right().type().isString()
%      to CompilationUnit.problems();

\begin{align*}
\textit{EQExpr} \; & \textit{\textbf{contributes}} \; \textit{error}(\texttt{"Incorrect string equality test!"}) \\
  & \textit{\textbf{when}} \; \textit{Left.type.isString} \land \textit{Right.type.isString} \\
  & \textit{\textbf{to}} \; \textit{CompilationUnit.problems}
\end{align*}

\subsubsection{Collections Over Higher-Order Attributes}

Collection attributes do not normally search for contributions in higher-order
attributes.  This is due to the fact that higher-order attributes can expand
indefinitely.

In some cases, however, it is useful to have collections which range over
certain finite higher-order attributes. To this end, we can control the search
during collection attribute evaluation by using a special JastAdd mechanism
that I developed for JastAdd version~2.2.1. This new mechanism is a variation of
the contributes statement that allows specifying which nodes to search for contributions.
Here is an example from the ExtendJ compiler:

\begin{lstlisting}
LambdaExpr contributes {
  toClass().collectContributions();
} to TypeDecl.nestedTypes();
\end{lstlisting}


\noindent
I added this particular code fragment to the compiler to solve a problem in finding
all anonymous classes induced by lambda expressions.
In the ExtendJ Java compiler, extensions may need to generate new anonymous classes.
In particular, the Java~8 module in ExtendJ uses anonymous classes to implement lambda
expressions.  Since these anonymous classes are built with higher-order attributes
at various places in the AST (inside
expressions), they need to be located during code generation in order for the
anonymous class to be written to a class file.
There was already such a collection attribute to gather ordinary anonymous class
declarations. However, it did not locate the new lambda classes because they were
nested inside higher-order attributes. This could be solved with a new contribution statement,
except for lambda expressions nested in other lambda expressions.
For nested lambdas, we have the problem
of needing to look inside the anonymous class, which is a higher-order attribute,
to find all anonymous classes. The above code fragment solved this issue by adding the anonymous
class for a lambda expression, \emph{LambdaExpr.toClass},
to be searched for contributions to the \emph{nestedTypes} collection attribute
(the attribute for finding anonymous classes).


\subsection{Generic AST Traversal}
\label{sec:ast-traversal}

In imperative programming, traversing a tree of nodes in a particular order is straightforward,
for example using standard preorder traversal (as in depth-first search).
However, this task is trickier with declarative attributes. Attributes typically hide their
evaluation order, and have no direct mechanism for ordering attribute evaluations. So the question
is: How can we use attributes to implement ordered traversal?

The solution is to use a reference attribute to point out the predecessor in the
required traversal order. We can then implement our traversal using this predecessor
attribute. Suppose that we need to number the nodes of an AST by their preorder number.
Let \emph{pred} be a reference attribute pointing to the predecessor in the
traversal order. We can achieve the required preorder numbering with the following equations:

\begin{align*}
\textit{\textbf{syn}} \; \textit{\textbf{int}} \; \textit{ASTNode} \; \textit{ASTNode}.&\textit{dfnum} =
  \textit{pred.dfnum} + 1 \\
\textit{\textbf{eq}} \; \textit{Root}.&\textit{dfnum} = 0
\end{align*}

\begin{figure}[htb]
\centering
\begin{forest}
  [{$1$}, name=A
    [{$2$}, name=B]
    [{$3$}, name=C
      [{$4$}, name=D]
      [{$5$}, name=E
        [{$6$}, name=F]
        [{$7$}, name=G]
        [{$8$}, name=H]
      ]
    ]
    [{$9$}, name=I,
      [{$10$}, name=J]
      [{$11$}, name=K]
    ]
  ]
  \path[every edge/.style={color=red, draw, ->, thick}]
    (K) edge[bend right] (J)
    (J) edge[bend left] (I)
    (I) edge[bend right=52] (H)
    (H) edge[bend right] (G)
    (G) edge[bend right] (F)
    (F) edge[bend left] (E)
    (E) edge[bend right] (D)
    (D) edge[bend left] (C)
    (C) edge[bend right] (B)
    (B) edge[bend left] (A)
  ;
\end{forest}
\caption{An AST with nodes labeled by their preorder number. The red arrows show the
	\emph{pred} attribute references for all nodes except the root.}
\label{fig:dfnum}
\end{figure}

\noindent
Here, \emph{ASTNode} is the generic superclass of all AST classes, and \emph{Root} is the
root class of the AST (from the abstract grammar). The equation on \emph{Root} is needed
to give the base case for \emph{dfnum}: a \emph{Root} node is the first node in the traversal.
\figref{fig:dfnum} shows the preorder numbering
of a generic AST.

\goodbreak
The implementation of the \emph{pred} attribute is a little bit more involved.
The \emph{pred} attribute is an inherited attribute which uses \emph{prevNode} to
find the predecessor of the current node in its parent.
We also need an attribute, \emph{last}, to point to the last node inside
a subtree. Here is the full implementation of \emph{pred}
for a preorder traversal:

\begin{alignat*}{3}
& \textit{\textbf{inh}} \; \textit{ASTNode} & \; \textit{ASTNode}&.\textit{pred} && \\
& \textit{\textbf{eq}} \; & \; \textit{ASTNode}&.\textit{child}[i].\textit{pred} \; &&= \; \textit{prevNode}(i) \\
& \textit{\textbf{syn}} \; \textit{ASTNode} & \; \textit{ASTNode}&.\textit{prevNode}(i)
  \; &&= \; \textit{child}[i-1].\textit{last} \;\; \textit{\textbf{if}} \;\; i > 0; \; \textit{\textbf{else}} \;\; \textit{\textbf{this}} \\
%  \; &&= \; \textit{\textbf{if}} \;\; i > 0 \;\; \textit{\textbf{then}} \;\; \textit{child}[i-1].\textit{last} \;\;\textit{\textbf{else}}\;\; \textit{\textbf{this}} \\
& \textit{\textbf{syn}} \; \textit{ASTNode} & \; \textit{ASTNode}&.\textit{last} \; &&= \; \textit{prevNode}(|\textit{child}|)
\end{alignat*}

\noindent
Here, \emph{child} is
an array of the children in an \emph{ASTNode}, and $|\textit{child}|$ means the length
of the child vector. This code uses a JastAdd mechanism which we have not previously discussed:
the \emph{pred} equation uses the index $i$ of the child edge which the equation is evaluated on
(this works similarly for list children).


We can apply this preorder traversal pattern to a number of useful tasks, like
numbering local variable declarations (needed for code generation).
Here is an example of a local variable numbering attribute, \emph{varNum}, for
a fictional procedural language:

\begin{align*}
\textit{\textbf{syn}} \; \textit{\textbf{int}} \; \textit{ASTNode} \; \textit{ASTNode}.&\textit{varNum} =
  \textit{pred.varNum} \\
\textit{\textbf{eq}} \; \textit{VarDecl}.&\textit{varNum} = \textit{pred.varNum} + 1 \\
\textit{\textbf{eq}} \; \textit{Function}.&\textit{varNum} = 0
\end{align*}

\noindent
The equation on \emph{Function} resets the numbering for each function, making
variable numbering local to each function.
The \emph{varNum} attribute is structure-shy, as it is only affected by the presence of
\emph{VarDecl} and \emph{Function} nodes in the AST, and insensitive to
the structure of the rest of the tree. New language constructs can easily be added to the
language without affecting the \emph{varNum} attribute.


\subsection{Discussion}

%In traditional compiler architecture, tree traversal is the main method of
%computing information from the program, typically using the Visitor Pattern.
%In contrast, RAGs directly promote extensibility Tree traversal does not directly promote extensibility

% Cross-cutting concerns

While ITDs are useful for extensibility, it can been argued that AOP counters modularity.
Parnas famously promotes information hiding as the main
criteria for decomposing a system into modules \cite{DBLP:journals/cacm/Parnas72a}.
However, few things can be effectively hidden in the presence of AOP \cite{DBLP:conf/oopsla/Steimann06}.

With ITDs we gain the ability to easily decompose a compiler
into separate modules according to any cross-cutting concern. Additionally,
AOP in JastAdd provides high composability: we can build components of attributes which
are combined without having to pre-design extension points or callback mechanisms.
The benefits of the limited form of AOP combined with attributes in JastAdd were investigated
in a paper by \textcite{DBLP:conf/aosd/AvgustinovET08}.

In practice, extensibility need not be deliberately designed into a JastAdd-based compiler.
Instead, extensibility occurs as a happy
coincidence of using RAGs and AOP.

%Purity also benefits extensibility because it
%means that an attribute equation can be reasoned about in isolation from other
%attributes.\footnote{%
%Of course, any attributes used inside the equation must also be understood, but there
%are no side effects due to execution order that need to be considered.}
%This is in contrast to a
%conventional imperative compiler where the state of data structures used in an analysis could
%be modified in multiple different locations in the compiler.

With attributes, there is no need for data structures like symbol tables, which are external to the
AST. Instead, all information that is needed for compilation can be computed directly by attributes.
Symbol tables can be replaced with reference attributes. Lookup tables indexed by AST nodes
correspond directly to attributes where the attribute equation computes the value in the lookup
table. This lack of specialized data structures is also a benefit from the perspective of
extensibility as there are no existing data structures that need to be expanded to store more
information than they were originally designed for.


\section{The ExtendJ Java Compiler}
\label{sec:extendj}

ExtendJ (formerly, JastAddJ\footnote{Originally known as
the JastAdd Extensible Java Compiler, later shortened to JastAddJ. The compiler
has occasionally been referred to as the JastAdd frontend for Java.
The compiler was finally renamed to ExtendJ to avoid confusion with the JastAdd metacompiler.}
\cite{jastaddj})
is an extensible Java compiler, implemented using the JastAdd metacompiler, supporting full
Java source-to-bytecode compilation \cite{extendjorg}.

ExtendJ is free and open source, provided under The Modified BSD License \cite{bsd3clause}. The source
code is available from the following repository:

\begin{center}
\url{https://bitbucket.org/extendj/extendj}
\end{center}

With ExtendJ, it is possible to develop new language extensions,
static analyses, source transformation tools, and other tools based on the Java language.

The rest of this section is organized as follows.
The next sub-section describes the development
history of ExtendJ, then interesting examples of extensions are presented, then continuing with
a high-level overview of the design of the compiler, and some of the most important attributes.

\subsection{Development History}

ExtendJ was originally designed as a case study in using JastAdd
to construct a practical compiler with RAGs.
The result was a highly extensible compiler that proved suitable for building static analyses
and language extensions for Java.

Torbj{\"o}rn Ekman developed the first versions of ExtendJ, including the Java~1.4 and Java~5
versions \cite{DBLP:conf/ecoop/EkmanH04,jastaddj}.
I later took over the project, implementing new versions for Java~6 (a minimal change to
Java~5), and Java~7.  Ekman had left the project before I started working on ExtendJ. I started by
learning about RAGs and how the compiler worked. I received help from G{\"o}rel Hedin and Anders Nilsson
who had some knowledge of ExtendJ.
The next major Java version, Java~8, was implemented by Erik Hogeman for his Masters Thesis project
under my supervision \cite{Hogeman2014}.  Hogeman did an excellent job, but there were some
final enhancements
which I completed for the Java~8 extension (improved type inference and code generation issues).

The following table summarizes the authorship of ExtendJ, as of version \extendjversion:

\begin{center}
\begin{threeparttable}
\begin{tabular}{lrrr}
  \toprule
  \emph{Author} & \emph{Commits} & \emph{Inserted} & \emph{Removed} \\
  \midrule
  Jesper Öqvist & 890 & 136\,043 & 116\,391 \\
  Torbjörn Ekman & 401 & 69\,606 & 38\,231 \\
  Erik Hogeman & 17 & 12\,273 & 2\,477 \\
  Max Schäfer & 14 & 8\,695 & 172 \\
  Pavel Avgustinov & 9 & 1\,730 & 865 \\
  Emma Söderberg & 20 & 563 & 107 \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item \emph{Inserted/Removed}: number of lines inserted/removed across all commits.\footnotemark{}
\item Sorted by decreasing number of inserted plus removed lines.
Authors with fewer than 300 lines inserted plus removed are not listed (10 in total).
\end{tablenotes}
\end{threeparttable}
\end{center}
\footnotetext{The number of lines were counted by using the \texttt{show} command for
\texttt{git} with the\;\;\texttt{--numstat} option. Line counts include changes to non-source files like
README and ChangeLog.}

During my time working on ExtendJ, I have fixed many bugs in the compiler.
This includes code generation errors
for which the compiler
created faulty bytecode, compile failures where the compiler failed to compile well-formed
Java code, or incorrectly accepting code which should have caused a compile error.
About 250 of the bugs I fixed are tracked on the current issue tracker for ExtendJ,\footnote{%
The issue tracker is at the main code repository: \url{https://bitbucket.org/extendj/extendj}.}
many bugs that I fixed were not tracked.
I also developed the test framework and test suite for ExtendJ, which currently
contains about 1700 tests.  Erik Hogeman wrote 888 tests for the Java~8
implementation \cite{Hogeman2014}.

In order to enable parallel compilation in ExtendJ, side effects had to
be removed. Thus, an important part of my
contributions in this dissertation is a fully declarative, side effect free, version
of ExtendJ.  This makes it possible to parallelize
compilation, and to run concurrent analyses in interactive tools based on ExtendJ.
Previously, ExtendJ had some known uses of side effects and imperative tree
transformations which prevented concurrent evaluation.

I have also made several other refactorings to the compiler in order to
improve correctness or simplify compilation.
Some of this work is presented in \secref{sec:contributions}.

%Emma S{\"o}derberg did some maintenance and refactoring in ExtendJ related to
%library class loading.


\subsection{Extensions Overview}

ExtendJ has been useful to researchers over several years, thanks to continued development
of the compiler and support for newer Java versions.
The ability to implement new Java versions efficiently,
by only a few developers, was to a large extent afforded by the inherent extensibility
in using JastAdd to build the compiler.

While other extensible Java compilers do exist, language extensions and tools based on ExtendJ are
often smaller in terms of code size, and more maintainable \cite{DBLP:conf/aosd/AvgustinovET08}.
The smaller implementation size and declarative coding paradigm in ExtendJ
seems to outweigh some of the drawback of using a more unconventional RAG-based
compiler architecture. Researchers who are unfamiliar with RAGs will find
learning
the JastAdd code in ExtendJ to be an obstacle to overcome before they can start implementing
a project in the compiler. Still, ExtendJ has been used by several researchers to
implement their language extensions, static analyses, and other tools.

To illustrate the variety of research that has been done with ExtendJ,
here are some of the interesting results that have been published,
including work from our own research group as well as from others who worked independently:

\goodbreak
{\raggedright
\vspace{1em}
\noindent
\textbf{Language extensions:}

\begin{itemize}
  \item
    Pavel Avgustinov, Torbj{\"{o}}rn Ekman, and Julian Tibble,
    ``Modularity first: a case for mixing {AOP} and attribute grammars''.
    In \emph{AOSD}, 2008.
    \cite{DBLP:conf/aosd/AvgustinovET08}.
  \item
    (Paper~II) \paperIIref
  \item
    Sukyoung Ryu,
    ``ThisType for Object-Oriented Languages: From Theory to Practice''.
    In \emph{TOPLAS}, 2016 \cite{DBLP:journals/toplas/Ryu16}.
  \item
    YungYu Zhuang and Shigeru Chiba,
    ``Expanding Event Systems to Support Signals by Enabling the Automation of Handler Bindings''.
    In \emph{Journal of Information Processing}, 2016 \cite{DBLP:journals/jip/ZhuangC16}.
  \item
    Tetsuo Kamina and Tomoyuki Aotani,
    ``Harmonizing Signals and Events with a Lightweight Extension to Java''.
    In \emph{Programming Journal, Vol. 2, no. 3}, 2018 \cite{DBLP:journals/programming/KaminaA18}.
  \item
    Jan C. Dagef{\"{o}}rde and Herbert Kuchen,
    ``A constraint-logic object-oriented language''.
    In \emph{SAC}, 2018 \cite{DBLP:conf/sac/DagefordeK18}.
\end{itemize}

\goodbreak
\noindent
\textbf{Tools and analyses:}

\begin{itemize}
  \item
    Torbj{\"{o}}rn Ekman and G{\"{o}}rel Hedin,
    ``Pluggable checking and inferencing of nonnull types for Java''.
    In \emph{Journal of Object Technology, Vol. 6, no. 9, 2007}
    \cite{DBLP:journals/jot/EkmanH07}.
  \item
    Emma S{\"{o}}derberg, Torbj{\"{o}}rn Ekman, G{\"{o}}rel Hedin, and Eva Magnusson,
    ``Extensible intraprocedural flow analysis at the abstract syntax tree level''.
    In \emph{Sci. Comput. Program.}, 2013 \cite{DBLP:journals/scp/SoderbergEHM13}.
  \item
    (Paper~III) \paperIIIref
  \item
    Friedrich Steimann, J{\"{o}}rg Hagemann, and Bastian Ulke,
    ``Computing repair alternatives for malformed programs using constraint attribute grammars''.
    In \emph{OOPSLA}, 2016 \cite{DBLP:conf/oopsla/SteimannHU16}.
  \item
    Mohammad R. Azadmanesh and Matthias Hauswirth,
    ``Concept-Driven Generation of Intuitive Explanations of Program Execution for a Visual Tutor''.
    In \emph{VISSOFT}, 2017 \cite{DBLP:conf/vissoft/AzadmaneshH17}.
\end{itemize}
}%raggedright

At the computer science department at Lund University, we have for the past four
years offered a course where students work in groups of two to implement
small compiler-related projects. Some of the compiler projects are
implemented as extensions to ExtendJ. Here are a few of the interesting
projects that have been done by students so far in the course:

\begin{itemize}\raggedright
\item 
  Olle Tervalampi-Olsson and Marcus Lacerda,
  ``Object-oriented metrics for Java programs'',
  2014.
\item
  Joel Lindholm and Johan Thorsberg,
  ``Package metrics on Java projects'',
  2014.
\item
  Ella Eriksson and Zimon Kuhs,
  ``Bug detection through static analysis'',
  2015.
\item
  Hans Bjerndell and Linus Lexfors,
  ``Extending Java with new operators'',
  2016.
\item
  Sebastian Hjelm and Markus Olsson,
  ``Extending the ExtendJ Java compiler to Java~9 support'',
  2017.
\item
  Wawrzyn Chonewicz and Filip Stenström,
  ``Extending Java with new operators using ExtendJ'',
  2017.
\end{itemize}

Although most of the student projects listed above were either very small
extensions or partially implemented, they still demonstrate that it is
reasonably easy for developers who are previously unfamiliar with ExtendJ
to start implementing useful extensions within a couple of weeks of work. The students spend about
six to eight man-weeks of implementation work on the project (the rest of the time is spent on
reading related work and writing a report). At the beginning of the project,
they have a very basic understanding of JastAdd and RAGs from the prerequisite
compilers course.

Most of the contributions in this dissertation were implemented and/or evaluated in ExtendJ.
The Java~7 extension (Paper~I) is a core extension for ExtendJ, in which I developed
design patterns which were later used in the Java~8 extension.
The Multiplicities case study (Paper~II) was developed as an ExtendJ extension.
The test selection project (Paper~III) was developed as an ExtendJ extension.
Finally, the concurrent evaluation algorithms for RAGs presented in Paper~IV were
evaluated in ExtendJ.

%AOP has been described as combining obliviousness and quantification -
%a program is oblivious to the aspects that are added to it, and an aspect declaration quantifies
%where it should apply. Obliviousness is useful for extensibility because it allows someone
%to make changes where they were perhaps not pre-designed to be able to be made.
%\cite{filman2000aspect}

%A base system is oblivious to the attributes that it is extended with.
%JastAdd supports a limited form of static Aspect Oriented Programming (AOP). The available
%aspect features only make it possible to declare new methods or attributes for AST classes
%in separate aspects. This is useful for separation of concerns: we can declare all attributes
%needed for some particular feature in a self-contained aspect. The other aspects are oblivious
%to the new feature: they have not been written specifically to provide the possibility to
%add the feature.


%Side effects are any modification of data outside an attribute. An attribute is not
%pure if there are side effects. Side effects tend to
%occur in practice even though they can affect attribute correctness.
%There are several reasons why side effects might be added in attributes: to implement an AST
%transformation, and unintentional reference aliasing (e.g. collections and higher-order attributes).
%These issues can be fixed in several different ways. Often higher-order attributes can be
%used to replace imperative transformations. Unintentional aliasing is often easy to fix,
%but the hard part is to find the cause of the aliasing issue. Often, the effects are only
%noticed long after the actual aliasing problem occurred.


\subsection{Compilation Passes}

%While some analyses are, in principle, independent from each other, they are seldom run in parallel.

Compilers are typically organized into multiple passes consisting of various
static analyses and AST transformations.
Passes normally need to run in a specific order because some passes depend on transformations
or analyses done by a previous pass. For example, name analysis is usually one of the
first passes because most other analyses require name analysis information.
Passes communicate information both through the AST and through other shared data structures
like symbol tables and control flow graphs.

ExtendJ differs from conventional compilers by having relatively few passes,
and by using practically no data structures apart from the AST.
In fact, ExtendJ has only three passes:
\emph{parsing}, \emph{error checking}, and \emph{code generation}, as illustrated in
\figref{fig:exj-passes}.
Error checking is done by evaluating
the collection attribute \emph{problems} on each source file in the program. The
\emph{problems} attribute contains error messages and warnings for the current program.
If there were error messages found, then ExtendJ proceeds by generating the necessary Java bytecode
in the code generation pass.

\begin{figure}
  \centering
\begin{tikzpicture}[
    >={Triangle[angle=45:6pt,length=6pt]},
    every join/.style={->},
    base/.style={draw, join, minimum height=0.8cm, align=center},
    proc/.style={base, text width=8em}]
  { [start chain=trunk going below]
    \node[proc, on chain] {\textbf{Parse Sources} \\ {\small Produces AST for each source file.}};
    \node[proc, on chain] {\textbf{Error Checking} \\ {\small (With library loading.)}};
    \node[base, on chain, diamond, aspect=2] (test) {Found errors?};
    { [start branch=fail going below]
      \node[proc, text width=4em, on chain, right=of test] (fail) {Fail};
    }
    \node[proc, on chain] (codegen) {\textbf{Code Generation} \\ {\small Produces bytecode (class files).}};
  }
  \path (test.south) to node [near start, xshift=1em] {\textit{no}} (codegen);
  \path (test.east) to node [near start, yshift=1em] {\textit{yes}} (fail);
\end{tikzpicture}
\caption{Flowchart for compilation in ExtendJ. Compilation is split into three
explicit passes: \emph{parsing}, \emph{error checking}, and \emph{code
generation}. ExtendJ uses dynamic loading of library classes, interleaving some
parsing (of bytecode and Java source code) with error checking.}
\label{fig:exj-passes}
\end{figure}

The reason there are so few passes in ExtendJ is that attributes do most of the work:
when the \emph{problems} attribute is evaluated, it automatically causes the evaluation of
all attributes it depends on. Code generation similarly relies on additional attributes, which are
evaluated as needed.
There are cyclic dependencies between a small number of
attributes, which are solved by using fixpoint iteration (with circular attributes).
Because attributes are declarative, we do not have to consider the ordering of attributes during
evaluation. Any valid attribute evaluation order\footnote{%
Meaning a reverse dependency order, of which there are many.}
always gives the same result.

An alternative viewpoint is to regard each attribute as a kind of mini-pass. In this sense,
ExtendJ contains very many interleaved passes.

\cleartoleftpage
\subsection{Modular Architecture}

ExtendJ is composed of a set of modules supporting different versions of Java.
At the base is a Java~1.4 module, upon which Java~5 through 8 modules are added as extensions
(each new extension version depending on the previous).
The modules consist of JastAdd aspect files, abstract grammar, and separate scanning
specification and parsing grammar. For each supported Java version, there are two modules:
a frontend module for parsing and semantic analysis, and a backend module for generating
Java bytecode.

The following table summarizes the contents of the current modules in ExtendJ, version
\extendjversion:

\begin{center}
\begin{threeparttable}
  \begin{tabular}{c}
    \adjustbox{center=.8\textwidth}{\extendjstats}
  \end{tabular}
  \begin{tablenotes}
    \small
    \item The \emph{f}/\emph{b} suffix indicates frontend/backend module.
    \emph{LOC} is the number of lines of aspect code (excluding lines with only comments, spaces, and braces).
    \emph{AST} is the number of grammar classes added in each module.
    \emph{Classes} are non-grammar classes and interfaces.
    \emph{Attrs} is the number of attribute declarations, and
    \emph{Refines} is the number of replaced attribute equations and methods.\footnotemark{}
  \end{tablenotes}
\end{threeparttable}
\end{center}
\footnotetext{The number of lines of code were counted with a tool based on
the lexer from ExtendJ.}


\subsection{Abstract Grammar}

This section gives a high-level overview of the abstract grammar of ExtendJ, omitting
many details which do not impact the rest of the discussion.
This section can be skipped if you are not
interested in the technical details of the ExtendJ implementation.

% TODO refer to JLS8 instead?
The abstract grammar in ExtendJ is not too different from any other Java compiler.
Most of the AST class names are derived from the Java specification \cite{jls7}. A~somewhat typical
ExtendJ AST is shown in \figref{fig:exj-grammar1}.

\begin{figure}
\centering
\resizebox{\textwidth}{!}{%
\begin{forest}
[Program
  [CompilationUnit]
  [CompilationUnit
    [InterfaceDecl
      [MethodDecl]
    ]
    [ClassDecl
      [ConstructorDecl]
      [MethodDecl
        [Block
          [ExprStmt [MethodAccess]]
          [ReturnStmt [VarAccess]]
        ]
      ]
    ]
  ]
]
\end{forest}
}%resizebox
\caption{A minimal ExtendJ AST, exhibiting most of the typical high-level structure.}
\label{fig:exj-grammar1}
\end{figure}

At the top level, we have the main AST root node, \emph{Program}, which contains
multiple compilation units:

\begin{equation*}
\textit{Program} \; \Coloneqq \; \textit{CompilationUnit}\,\ast
\end{equation*}

\noindent
Each compilation unit represents one Java source file, containing a list of
type declarations:

\begin{equation*}
\textit{CompilationUnit} \; \Coloneqq \; [\,\textit{PackageDecl}\,] \; \textit{ImportDecl}\ast \; \textit{TypeDecl}\,\ast
\end{equation*}

\noindent
Type declarations include, among others, class declarations and interface declarations:

\begin{alignat*}{2}
\texttt{abstract} & \; \textit{TypeDecl}    & & \; \Coloneqq \; \textit{Modifiers} \; \langle\textit{ID}\rangle \; \textit{BodyDecl}\,\ast \\
\texttt{abstract} \; \textit{ReferenceType} : & \; \textit{TypeDecl}  & & \\
\textit{ClassDecl} : & \; \textit{ReferenceType}  & & \; \Coloneqq \; \ldots \\
\textit{InterfaceDecl} : & \; \textit{ReferenceType}  & & \; \Coloneqq \; \ldots \\
\textit{EnumDecl} : & \; \textit{ClassDecl}  & & \\
\textit{PrimitiveType} : & \; \textit{TypeDecl}  & &
\end{alignat*}

\noindent
Each type declaration contains a list of member declarations like methods, fields, constructors,
and so on:

\begin{alignat*}{2}
\texttt{abstract} & \; \textit{BodyDecl} & & \\
\texttt{abstract} \; \textit{MemberDecl} : & \; \textit{BodyDecl} & & \\
\textit{ConstructorDecl} : & \; \textit{BodyDecl}  & & \; \Coloneqq \; \ldots \\
\textit{MethodDecl} : & \; \textit{MemberDecl}  & & \; \Coloneqq \; \ldots \\
\textit{FieldDecl} : & \; \textit{MemberDecl}  & & \; \Coloneqq \; \ldots
\end{alignat*}

\noindent
The difference between \emph{BodyDecl} and \emph{MemberDecl} is minor.
There are many similar cases where we use multiple levels of abstract superclasses in the
full ExtendJ grammar. These are used both for sharing attribute code between similar statements, and
for preventing some kinds of constructs occurring in certain places of the tree.

Inside body declarations are statements and expressions. As in most programming languages, methods
and constructors have a list of statements, and statements contain expressions.
Statements include typical imperative programming language statements such as for-loops,
if-statements, switch statements, etc.

\begin{alignat*}{3}
\texttt{abstract} & \; \textit{Stmt} & &  \\
\textit{IfStmt} : & \; \textit{Stmt} & \; \Coloneqq & \; \textit{Condition:Expr}\;\textit{Then:Stmt}\;[\textit{Else:Stmt}] \\
\textit{WhileStmt} : & \; \textit{BranchTargetStmt} & \; \Coloneqq & \; \textit{Condition:Expr}\;\textit{Stmt} \\
\textit{ForStmt} : & \; \textit{BranchTargetStmt} & \; \Coloneqq & \; \textit{InitStmt:Stmt}\ast\;[\textit{Condition:Expr}] \\
& & & \;\textit{UpdateStmt:Stmt}\ast\;\textit{Stmt} \\
\textit{BreakStmt} : & \; \textit{Stmt} & \; \Coloneqq & \; \langle{}\textit{Label}\rangle \\
\textit{ThrowStmt} : & \; \textit{Stmt} & \; \Coloneqq & \; \textit{Expr} \\
\textit{TryStmt} :  & \; \textit{Stmt} & \; \Coloneqq & \; \textit{Block}\;\textit{CatchClause}\ast \\
& & & \;\textit{ExceptionHandler:Block}
\end{alignat*}

The expression grammar is much larger than the statement grammar. Seen from a high level, there are
two kinds of expressions:

\begin{description}
  \item[Access] A reference to some entity: a type, variable (or parameter, field), method (a call),
    \verb'this' pointer, qualified expression (dot).
  \item[Non-Access] A non-reference expression, e.g., literal values, arithmetic expressions,
    type casts, etc.
\end{description}

Both access and non-access expressions are subclasses of the \emph{Expr} class,
and all access expressions inherit from \emph{Access}.

\begin{alignat*}{1}
\texttt{abstract} & \; \textit{Expr} \\
\texttt{abstract} & \; \textit{Access} : \textit{Expr}
\end{alignat*}

\noindent
There are many subtypes of \emph{Access} for the different kinds of access expressions.
The following list includes the most important kinds of access expressions:

\begin{alignat*}{3}
\textit{Dot} : & \; \textit{Access} & \; \Coloneqq & \; \textit{Left:Expr}\;\textit{Right:Access} \\
\textit{VarAccess} : & \; \textit{Access} & \; \Coloneqq & \; \langle\textit{\textit{ID}}\rangle \\
\textit{MethodAccess} : & \; \textit{Access} & \; \Coloneqq & \; \langle\textit{\textit{ID}}\rangle\;\textit{Arg:Expr}\ast \\
\textit{ConstructorAccess} : & \; \textit{Access} & \; \Coloneqq & \; \langle\textit{\textit{ID}}\rangle\;\textit{Arg:Expr}\ast \\
\textit{TypeAccess} : & \; \textit{Access} & \; \Coloneqq & \; \langle\textit{\textit{Package}}\rangle\;\langle\textit{\textit{ID}}\rangle \\
\textit{ThisAccess} : & \; \textit{Access} & & \\
\textit{SuperAccess} : & \; \textit{Access} & & \\
\textit{PackageAccess} : & \; \textit{Access} & \; \Coloneqq & \; \langle\textit{\textit{Package}}\rangle \\
\textit{ArrayAccess} : & \; \textit{Access} & \; \Coloneqq & \; \textit{Expr} \\
\textit{ParseName} : & \; \textit{Access} & &
\end{alignat*}

\noindent
There are many different subtypes of \emph{TypeAccess}, for different kinds of references to types
including polymorphic type accesses.
The \emph{ParseName} access is a special kind of ambiguous name that is reclassified automatically
by \emph{syntactic classification}, as described in \secref{sec:exj-name-analysis}.

Continuing with the expression grammar, we have non-access expressions.
These types of expressions follow a typical imperative programming language expression grammar:

\begin{alignat*}{1}
\texttt{abstract} \; \textit{AssignExpr} : & \; \textit{Expr} \; \Coloneqq \; \textit{Dest:Expr}\;\textit{Source:Expr} \\
\texttt{abstract} \; \textit{PrimaryExpr} : & \; \textit{Expr} \\
\textit{ParExpr} : & \; \textit{PrimaryExpr} \; \Coloneqq \; \textit{Expr} \\
\texttt{abstract} \; \textit{Binary} : & \; \textit{Expr} \; \Coloneqq \; \textit{LeftOperand:Expr} \; \textit{RightOperand:Expr} \\
\texttt{abstract} \; \textit{ArithmeticExpr} : & \; \textit{Binary} \\
\texttt{abstract} \; \textit{AdditiveExpr} : & \; \textit{ArithmeticExpr} \\
\textit{AddExpr} : & \; \textit{AdditiveExpr} \\
\textit{SubExpr} : & \; \textit{AdditiveExpr}
\end{alignat*}

The \emph{AdditiveExpr} class is an abstract class used solely for the convenience of being able
to declare a shared attribute on a single superclass: there are several attributes that are common
between additive expressions and thus the \emph{AdditiveExpr} class lets us specify only one
attribute equation for all of them.

\vbox{
A special kind of statement, named \emph{ExprStmt},  links expressions and statements. Its purpose is
to allow single expressions to be treated as statements. This is necessary, e.g., for method calls:

\begin{alignat*}{3}
\textit{ExprStmt} : & \; \textit{Stmt} & \; \Coloneqq & \; \textit{Expr}
\end{alignat*}
}

\subsection{Attributes}

The attributes in ExtendJ can roughly be grouped into these categories:

\begin{description}
  \item[Semantic Attributes] Attributes that implement a distinct part of the Java
    specification \cite{jls7}.
    Some of these attributes follow the specification closely, and can almost be read out loud
    as if part of the specification. Others diverge a bit more from the specification or
    are simply less readable than the corresponding natural language specification.
    An example of semantic attributes are the attributes that implement definite assignment
    analysis \cite[\S 16]{jls7}:
    they closely follow the specification and the equations are often very
    readable.
  \item[Utility Attributes] Attributes that are mainly concerned with collecting or organizing
    auxiliary information for the semantic attributes.
    A typical example is the \emph{TypeDecl.supertypes} attribute which
    collects all supertypes of the receiver type. Utility attributes are often useful in extensions.
  \item[Transformation Attributes] Higher-order attributes that transform
    part of the AST, either to implement a transformation required by the Java specification,
    or to simplify the work for other attributes.
    Examples include enum constructor transformation, implicit diamond access methods,
    etc.
\end{description}

The attributes in ExtendJ are divided into aspect files based on what Java version and which
kind of analysis they implement.
The next sections present some of the most important attributes in ExtendJ, and give
an overview of the major static analyses in ExtendJ.

\subsection{Name Analysis}
\label{sec:exj-name-analysis}

The primary name analysis tasks in Java compilers are binding uses of named entities
to corresponding declarations \cite[\S 6]{jls7}, and syntactic classification
\cite[\S 6.5.1]{jls7}.

The purpose of syntactic classification is to determine the meaning of all names in
the program, classifying them as variable names, type names, method names, or package names.
Consider the following import declaration:

\begin{lstlisting}
import treemap.Map.Entry;
\end{lstlisting}

\noindent
The meaning of \texttt{Map} is ambiguous here: it could refer to a
package \texttt{treemap.Map}, or a class inside the \texttt{treemap} package.

The parser produces a \emph{ParseName} node when a name is used in a context where the
parser is not able to unambiguously decide which kind of name it is. For example,
the ExtendJ parser builds the following AST subtree from the above import statement:

\vspace{1em}
{\def\tt#1{{\normalfont\texttt{#1}}}
\begin{center}
\begin{forest}
[SingleTypeImportDecl
  [{ParseName \tt{"treemap.Map.Entry"}}]
]
\end{forest}
\end{center}
}
\vspace{1em}


Syntactic classification is performed in
ExtendJ by using the JastAdd rewrite mechanism: a rewrite rule for the \emph{ParseName} class
automatically transforms each \emph{ParseName} node into an appropriate \emph{Access} when
it is first referenced from an attribute \cite{DBLP:conf/ecoop/EkmanH04}.
The \emph{ParseName} node is present in the parsed AST
but all attributes are oblivious to it because they can only observe the syntactically classified
result.\footnote{If syntactic classification fails, the \emph{ParseName} is replaced by an
\emph{AmbiguousAccess}.}

Name analysis is needed to find matching declarations for variable
and type names. Name analysis is done with the idiomatic lookup pattern for JastAdd, in which
inherited attributes are used for looking up names from enclosing scopes \cite{Hedin2011}.
The main attributes used for finding declarations via name analysis are:

\begin{alignat*}{2}
& \textbf{\textit{syn}} \; \textit{Variable} \; &\textit{VarAccess}.&\textit{decl} \\
& \textbf{\textit{syn}} \; \textit{MethodDecl} \; &\textit{MethodAccess}.&\textit{decl} \\
& \textbf{\textit{syn}} \; \textit{ConstructorDecl} \quad &\textit{ConstructorAccess}.&\textit{decl} \\
& \textbf{\textit{syn}} \; \textit{ConstructorDecl} \quad &\textit{ClassInstanceExpr}.&\textit{decl} \\
& \textbf{\textit{syn}} \; \textit{TypeDecl} \; &\textit{TypeAccess}.&\textit{decl}
\end{alignat*}

\noindent
These attributes find matching declarations for different kinds of named entities.
The \emph{Variable} type is an interface used for local variables, fields, and
parameter declarations. Method and constructor lookup can involve overload resolution,
shadowing, and type inference. The \emph{decl} attributes point to a single declaration
node, but if the declaration is undefined or ambiguous it will point to a singleton
representing an unknown declaration, using the null object pattern.

The \emph{decl} attributes are implemented by lower-level attributes which are parameterized
by the name being looked up (except for constructor lookup where the name is not needed).
These lower-level lookup attributes are:

\begin{align*}
& \textbf{\textit{inh}} \; \textit{Set}\langle\textit{Variable}\rangle \; \textit{Expr.lookupVariable}(\textit{String name}) \\
& \textbf{\textit{inh}} \; \textit{Set}\langle\textit{MethodDecl}\rangle \; \textit{Expr.lookupMethod}(\textit{String name}) \\
& \textbf{\textit{inh}} \; \textit{Set}\langle\textit{ConstructorDecl}\rangle \; \textit{ConstructorAccess.lookupConstructor} \\
& \textbf{\textit{inh}} \; \textit{TypeDecl} \; \textit{Expr.lookupType}(\textit{String packageName, String typeName})
\end{align*}

As a part of type analysis, typenames are resolved by name lookup via the \emph{lookupType} attribute.
The type name may
match an existing type declaration somewhere in the program AST, or it can match
a library class (either in a user library or system library included with the Java runtime),
or else it is an unknown type. For unknown types, the \emph{UnknownType} singleton is returned.

It would be wasteful to load all available libraries before starting compilation,
so instead ExtendJ uses demand-loading of libraries. Only the library types needed for the current
compilation task will actually be loaded. This dynamic library loading is implemented by
parameterized higher-order attributes: the attribute

\begin{equation*}
\textbf{\textit{syn}} \; \textit{CompilationUnit} \; \textit{Program}.\textit{getLibCompilationUnit}(\textit{String name})
\end{equation*}

\noindent
is responsible for loading a library type by its fully qualified type name.
This higher-order attribute builds an implicit part of the AST, so that the
loaded type becomes part of the full program AST once loaded and subsequent
accesses to the same library type reuse the already-loaded type.

%syn nta CompilationUnit Program.getLibCompilationUnit(String name);

\subsection{Type Analysis}
\label{sec:exj-type-anal}

ExtendJ represents each Java type by a subclass of the AST class \emph{TypeDecl}.
For example, Java classes are represented by the AST class \emph{ClassDecl},
interfaces by \emph{InterfaceDecl}, and enum types by \emph{EnumDecl}.
Each type in a Java program has a corresponding \emph{TypeDecl} node somewhere in the AST.
Even primitive types like \verb'int' and \verb'boolean'
have a corresponding type declaration node (reified as higher-order
attributes of type \emph{PrimitiveType}) and behave for the most part
like any other kind of type declaration.
In this system, user types are first-class citizens and, for the most part, indistinguishable
from library classes.

In most places where a type is used in a Java program, it is referred to
by name, as a \emph{TypeAccess}.
When analyzing Java code, we often need to look up the type declaration for a given name
in order to compare types or query any property of some named type.
To this end, name lookups are used to find a matching type declaration for typenames. In particular,
the attribute \emph{TypeAccess.decl} and \emph{Expr.lookupType} are used for finding the
type declaration matching a typename.
For parameterized types, like \verb'List<Integer>', the particular
parameterization used must be instantiated. This process is described in more detail
in \secref{sec:exj-polytypes}.

The attribute for computing the type of a general expression is

\begin{equation*}
\textbf{\textit{syn}} \; \textit{TypeDecl} \; \textit{Expr.type}
\end{equation*}

\noindent
For variables (and similarly for fields and parameter uses), the type is
computed by looking up the variable declaration. The declared type in the variable declaration
is then used to compute a reference to the relevant \emph{TypeDecl} node.
For method calls, the type is the declared return type of the matching method declaration.
For class instance
expressions the type is the same as the constructed class. The type attribute is in some cases
straight-forwardly implemented, but there are also challenges that occur due to type inference.

Another important task of type analysis is \emph{type checking}:
ensuring that all expressions are correctly typed and match the expected type from the expression
context.
Type checking relies heavily on the attribute

\begin{equation*}
\textbf{\textit{syn}} \; \textbf{\textit{boolean}} \; \textit{TypeDecl.subtype}(\textit{TypeDecl type})
\end{equation*}

\noindent
which determines if the receiver type is
a subtype of the argument type.
The subtype attribute is implemented with double dispatch to handle all combinations of different
kinds of types \cite{jastaddj}. For example, the subtype attribute for classes and interfaces
looks like this:

% TODO: explain reason for circular

%syn boolean TypeDecl.subtype(TypeDecl type)
%    circular [true]
%    = type == this;
%eq ClassDecl.subtype(TypeDecl type)
%    = type.supertypeClassDecl(this);
%eq InterfaceDecl.subtype(TypeDecl type)
%    = type.supertypeInterfaceDecl(this);

\begin{alignat*}{2}
& \textbf{\textit{syn}} \; \textbf{\textit{boolean}} & \; \textit{TypeDecl}.&\textit{subtype(TypeDecl type)} \; \textit{\textbf{circular(true)}} \\
& \textbf{\textit{eq}} & \; \textit{ClassDecl}.&\textit{subtype(TypeDecl type)} =\\
&&& \textit{type.supertypeClassDecl(\textbf{this})} \\
& \textbf{\textit{eq}} & \; \textit{InterfaceDecl}.&\textit{subtype(TypeDecl type)} =\\
&&& \textit{type.supertypeInterfaceDecl(\textbf{this})}
\end{alignat*}

\noindent
For each combination of two kinds of types $X$ and $Y,$ a corresponding set of attributes
\emph{TypeDecl.supertypeX(X)} and \emph{TypeDecl.supertypeY(Y)} are needed.
As an example, the equation for testing if a class declaration is a supertype of an interface
declaration looks like this:

\begin{equation*}
\textbf{\textit{eq}} \; \textit{ClassDecl}.\textit{supertypeInterfaceDecl(InterfaceDecl type)} = \textit{isObject}
\end{equation*}

\noindent
This attribute equation follows directly from the Java specification: the
\verb'Object' class (from package \verb'java.lang') is the only class which is a
supertype to any interface, and it is a supertype to all interfaces.

While double dispatch enables us to extend the type system with
new kinds of types without
editing all \emph{subtype} equations, it still can lead to a lot of work
due to the need for many additional equations.
In principle, the double dispatch pattern requires a pair of equations
for each pair in the Cartesian product of all kinds of types in the language.

%TODO: refer to multiple dispatch?


\subsection{Method Call Resolution}

One of the most demanding parts of the Java specification, in terms of implementation effort required,
is method call resolution \cite[\S 15.12.2]{jls7}.
Method call resolution is used for finding which method
declaration a method call refers to. If we gloss over the details, we can illustrate the
method resolution process as the following algorithm:

\begin{enumerate}
  \item Determine the receiver type for instance method calls.
    For qualified method calls, the receiver type is
    the type of the qualifying expression. For unqualified method calls, the receiver
    type is the enclosing class at the call site.
  \item Find matching method declarations based on the called method name.
    For instance-method calls, matching methods are searched for in the receiver type,
    otherwise the members of the enclosing class and imported static
    methods are searched for matching declarations.
  \item Filter candidate method declarations based on declaration visibility rules.
  \item Filter out overridden method declarations from candidate methods.
  \item Select the most specific method declaration (if one exists) based on the actual argument
    types used in the call.  Take into account variable arity, type parameters, and type inference.
\end{enumerate}

\noindent
A key attribute in the method resolution algorithm is

\begin{equation*}
\textbf{\textit{syn}} \; \textit{Set}\langle\textit{MethodDecl}\rangle \; \textit{MethodAccess.maxSpecific}(\textit{candidates})
\end{equation*}

\noindent
which implements the algorithm for determining the most specific method, a very precisely
defined concept from the Java specification \cite[\S 15.12.2.5]{jls7}.
The equation for this attribute was not too complicated in the Java~1.4 version of ExtendJ,
but in the Java~5 extension it indirectly uses the complicated type inference system
via the attribute

\begin{equation*}
\textbf{\textit{syn}} \; \textit{Set}\langle\textit{MethodDecl}\rangle \; \textit{MethodAccess.potentiallyApplicable}(\textit{candidates})
\end{equation*}

\noindent
For generic candidate methods, \emph{potentiallyApplicable} uses a utility attribute to infer
the type arguments for the current method call based on its context.

%An interesting problem in this algorithm
%arises when type inference interacts with method resolution.
%During method resolution, we need to know the argument types for the method call.
%Yet, in many important type inference use-cases, the argument types depend on which method is selected.
%
%I developed a new solution for this problem, which solved several previously\-failing
%type inference cases for Java~8. The new solution uses higher-order attributes to create
%temporary method calls during type inference.
%The idea is that, because the inferred type arguments depend on which target method is selected,
%we can try selecting all available methods and seeing which types are inferred for each
%candidate method. Then, we simply pick the most specific candidate method while using the
%inferred types to determine the most specific one (``most specific'' in this context is very
%precisely defined by the Java specification, see \cite[\S 15.12.2.5]{jls7}).


\subsection{Control Flow and Dataflow Analysis}

The Java specification requires several control flow and dataflow analyses,
including the following:

\begin{itemize}
  \item exception handling checks,
  \item unreachable statements,
  \item missing returns,
  \item definite assignment
  \item finally handlers (code generation).
\end{itemize}

\noindent
The following text gives some examples of how these analyses work in ExtendJ.

\subsubsection{Exception Handling Checks}

All \emph{checked} exceptions must be caught by an enclosing \verb'try'-statement, or else declared
to be thrown \cite[\S 11.2]{jls7}.
This requirement is handled in ExtendJ by \emph{handlesException},
an inherited parameterized attribute that determines if the argument exception type
is handled by the surrounding context (an enclosing \verb'try'-statement, for example).

A separate exception handling check ensures that \verb'try'-statements with a \verb'catch' clause
enclose a statement that can actually throw the caught exception type.
For this analysis, ExtendJ uses the synthesized parameterized attribute \emph{reachedException}.
The attribute determines if the argument exception type can be
thrown from the receiver statement (e.g., a block or method call).

\subsubsection{Unreachable Statements and Missing Returns}

The aforementioned exception handling check for \verb'catch' clauses implements
a small part of the more general requirements for unreachable statements
analysis in the Java specification \cite[\S 14.21]{jls7}.
The purpose of the specification is to disallow many, but not all, kinds of unreachable code.
In ExtendJ, most of the unreachable statement analysis is done with an inherited attribute
named \emph{reachable}. As the name implies, the attribute determines if the receiver node
is reachable in its context (this is necessarily an imprecise analysis).

The unreachable statement analysis is used in another kind of analysis: checking that
all paths through a non-\verb'void' method end with a \verb'return' statement (or
throw an exception). This analysis uses an attribute on statements named
\emph{canCompleteNormally}. The equations for this attribute, for the most part, rely on the
\emph{reachable} attribute. For example, here is the equation for \emph{canCompleteNormally}
on \verb'if'-statements:

%  eq IfStmt.canCompleteNormally() =
%    (reachable() && !hasElse())
%    || (getThen().canCompleteNormally() || (hasElse() && getElse().canCompleteNormally()));

\begin{align*}
& \textbf{\textit{eq}} \; \textit{IfStmt.canCompleteNormally} = \\
& \qquad (\textit{reachable} \land \textit{Else} = \textit{\textbf{nil}}) \\
& \qquad \lor \textit{Then.canCompleteNormally} \\
& \qquad \lor (\textit{Else} \ne \textit{\textbf{nil}} \land \textit{Else.canCompleteNormally})
\end{align*}

\noindent
This attribute equation is derived directly from the Java specification, which intentionally
treats \verb'if'-statements with constant \verb'true' conditions as if they can be
\verb'false'.\footnote{See the examples at the end of \S 14.21 in the Java 7 specification.}

\subsubsection{Definite Assignment}

Definite assignment ensures that all local variables are initialized before use \cite[\S 16]{jls7}.
The central attributes for definite assignment analysis in ExtendJ are the parameterized
attributes
\emph{assignedAfter}, \emph{assignedBefore}, \emph{unassignedAfter}, and \emph{unassignedBefore}.
These attributes determine if the argument variable
is definitely (un)assigned before/after the receiver statement or expression.

% NOTE: The core idea here is that a variable is definitely assigned before a statement if
% is definitely assigned after the preceding statement.

The Java specification defines the meaning of definitely assigned and definitely unassigned
by many rules for all kinds of expressions and statements. For example, one
rule in the Java~7 specification states that a variable $v$ is definitely assigned
after a variable declaration statement that contains no initializer if $v$
is definitely assigned before the declaration (paraphrased). This rule,
combined with a few other rules, is implemented in ExtendJ by the following
attribute equation:

%  syn boolean Declarator.assignedAfter(Variable v) circular [true] {
%    if (v == this) {
%      return hasInit();
%    } else {
%      return hasInit() ? getInit().assignedAfter(v) : assignedBefore(v);
%    }
%  }

\begin{align*}
& \textbf{\textit{eq}} \; \textit{Declarator.assignedAfter}(\textit{Variable}\;v) = \\
& \quad \begin{cases}
  \textit{Init} \ne \textit{\textbf{nil}}, & \text{if $v = \textit{\textbf{this}}$} \\
  \begin{cases}
    \textit{assignedBefore}(v), & \text{if $\textit{Init} = \textit{\textbf{nil}}$} \\
    \textit{Init.assignedAfter}(v), & \text{if $\textit{Init} \ne \textit{\textbf{nil}}$}
  \end{cases}, & \text{otherwise}
\end{cases}
\end{align*}


\noindent
In the first case, the \emph{Declarator} declares a variable with the same name as
$v$, and $v$ is only definitely assigned after the declaration if
there is an initializer ($\textit{Init} \ne \textit{\textbf{nil}}$).

% New paragraph to avoid bad page break after 'The'.
\noindent
The $\emph{assignedBefore}(v)$ case implements the rule described above. The remaining
case comes from another rule in the specification.

The equation above is one of the smaller definite assignment equations; some are much larger,
although they follow the Java specification closely. The definite assignment attributes
can circularly depend on themselves when loop statements are involved. This is discussed
briefly in the Java specification \cite[\S 16]{jls7}.
Interestingly, the definite assignment attributes have remained mostly untouched since the Java~1.4
version of ExtendJ, with few additions for later Java versions.

\subsubsection{Finally Handlers}

Finally handlers are needed in the generated bytecode for all control-flow paths out of a
\verb'try'-statement. Even though there is at most one \verb'finally' block for each \verb'try'-statement, the statement
may require multiple copies of the \verb'finally' block to handle if the exception is re-thrown, or if the
\verb'try'-block executed a return statement.
To illustrate, the following two pieces of code are equivalent:

{
\newcommand{\g}[1]{\adjustbox{bgcolor=white!80!gray}{\strut{}#1}}
\begin{center}
\begin{tabular}{l|l}
\begin{minipage}[t]{.42\textwidth}%
\begin{lstlisting}[mathescape=true]
try {
  if (m()) {

    return;
  }
} catch(Exception e) {
  print("y");

  throw e;
} finally {
  $\g{doLast();}$
}
\end{lstlisting}
\end{minipage}%
\hspace{.05\textwidth}&\hspace{.05\textwidth}%
\begin{minipage}[t]{.40\textwidth}%
\begin{lstlisting}[mathescape=true]
try {
  if (m()) {
    $\g{doLast();}$
    return;
  }
} catch(Exception e) {
  print("y");
  $\g{doLast();}$
  throw e;
}
$\g{doLast();}$
\end{lstlisting}
\end{minipage}%
\end{tabular}
\end{center}
}

\noindent
Notice that the body of the \verb'finally' block (highlighted gray) is
implicitly duplicated to three places in the code on the right.
The implicit \verb'finally' blocks are reified with a higher-order attribute
named \emph{ntaFinallyBlock}, which duplicates the code from the \verb'finally' block.

\subsection{Representation of Polymorphic Types}
\label{sec:exj-polytypes}

Java has parametric polymorphism in the form of generic classes and methods with
\emph{type parameters}.
Type arguments can be specified at the use-site of a parametric type or method.
Alternatively, type arguments can be inferred in certain contexts.

In ExtendJ, each generic type is represented by a \emph{GenericTypeDecl}.
Because Java is nominally typed, each instantiation of a generic type (a class or interface) needs
to be reified in the compiler. In ExtendJ, this reification is done by constructing a
type declaration node in the AST by using a higher-order attribute.
This higher-order attribute is a parameterized attribute where the parameter is the type argument
list for the specific parameterized type to be reified.

The higher-order attribute for reifying a parameterized type has the following declaration:\footnote{%
The type of the parameter has been left out to make it fit in one line here.
The \textbf{\textit{nta}} keyword comes from Non-Terminal Attribute, another name
for higher-order attributes.}

%syn nta TypeDecl GenericTypeDecl.lookupParTypeDecl(Collection<TypeDecl> typeArgs);

\begin{equation*}
\textbf{\textit{syn}} \; \textbf{\textit{nta}} \; \textit{TypeDecl} \; \textit{GenericTypeDecl}.\textit{lookupParTypeDecl}(\textit{typeArgs})
\end{equation*}

\noindent
The declaration constructed by the attribute is a shallow copy of the generic class, containing
only the externally visible API in the form of member signatures.
Member signatures are
needed for type checking any use of the class, but the specific parameterization of the class
is not used in code generation and
so the code in the methods (and field initializers, instance initializers, etc.) can be discarded.

Previously, type variables were substituted for their corresponding type argument when building
a parameterized type in the higher-order attribute \cite{jastaddj}. %However, this is both slow (in parallel eval) and premature.
However, this can lead to problems.
For generic members, this process prevents recursive type substitutions.
During the implementation for Paper~IV, I refactored this so that the original type variables
are kept unmodified and substitution is instead
handled by adding new equations for the type lookup attributes on the parameterized types.
This solved the problem of recursive type substitutions in generic methods, and also seems
to have sped up parallel compilation (not specifically evaluated).


\subsection{Type Inference and Generic Types}

Type inference was added to the Java language in Java~5, together with generic types and methods.
Type inference can be used to compute the type parameters for generic method invocations
if they are omitted. For example, the static method \texttt{Collections.emptyList()} is generic and
we can call it with explicit type parameters like this:

\begin{lstlisting}
List<String> list = Collections.<String>emptyList();
\end{lstlisting}

\noindent
In this case it is also possible to omit the type parameter \texttt{String}
and instead rely on type inference to compute the right type:

\begin{lstlisting}
List<String> list = Collections.emptyList();
\end{lstlisting}

Type inference was further extended in Java~7, with the diamond expression,
and in Java~8 to make anonymous functions easier to use.
In Java~8, we can write a lambda expression (anonymous function) without specifying the
types of the formal parameters, e.g.

\begin{lstlisting}
x -> 3*x
\end{lstlisting}

Type inference can be implemented in several ways. For example, by using the well-known
unification algorithm \cites[p. 326]{DBLP:books/daglib/0005958}[p.
19]{moller2018static}[p. 102]{DBLP:series/utcs/Sestoft17}.  In ExtendJ, type
inference works a bit differently: we gather a set of subtype and supertype
constraints from the context.  The constraints are solved by finding the
greatest lower bound or least upper bound of constraint types in the type
hierarchy.  Constraints are solved one at a time without backtracking, and at
the end there is either a most general solution or no solution.
With improved type inference, introduced in Java~8 \cite[\S 18]{jls8},
the constraint systems become more
complicated, by introducing new circular dependencies in type inference that
were not previously possible.  Additionally, type inference can occur
simultaneously at different parts of a single expression, whereas previously sub-expression
types were inferred one at a time and bottom-up.


\subsection{Code Generation}

The final pass in ExtendJ generates bytecode for the Java Virtual Machine (JVM) to run.
The generated bytecode is unoptimized. Like OpenJDK, ExtendJ relies on the JVM to optimize the
bytecode during runtime.
The JVM usually does a good job of runtime optimization, resulting in high performance.

The Java bytecode consists of instructions for a stack-based virtual machine. Most of the bytecode
generation is straightforward, except for one part: since Java~7, the JVM requires so-called \emph{stack
map frames} in the bytecode. Stack map frames describe the possible types of stack and local
variables at each point where control flow merges in the bytecode instructions. The stack map frames
are used for type checking the bytecode during runtime. Previous to Java~7, the JVM automatically
inferred all stack map frames. However, inferring these stack map frames has a cost.
To avoid that cost the Java language designers decided that stack map frames
should instead be computed by the compiler and output alongside the Java bytecode.
When the stack map frames are included with bytecode, the JVM just has to perform the
computationally simpler task of type checking the bytecode against the provided stack map frames.

During my work for this thesis I implemented the stack map frames generation in ExtendJ, so that it
can output Java~7+ bytecode.


\section{Contributions}
\label{sec:contributions}

In this section I describe my key contributions in this dissertation.
To give a quick overview, the main contributions are:

\begin{itemize}
  \item An extension of the ExtendJ compiler to Java~7 (Paper~I),
    with two new methods for extending a programming language with higher-order
    attributes.

    The resulting extended compiler remains comparatively
    fast and the implementation was much smaller than the reference
    compiler for Java.
  \item The multiplicities Java language extension implemented as an ExtendJ extension (Paper~II).

    The main contribution in this paper is the case study of a new language mechanism,
    multiplicities.  The implementation is an extension of the Java type system
    with new code generation for handling multiplicities.
  \item An automated algorithm for incremental regression testing
    based on program extractions (Paper~III).
      The implementation is an efficient dependency graph extraction method
      based on the ExtendJ compiler.
  \item Algorithms supporting concurrent RAGs. In particular, a new algorithm for concurrent
    fixpoint attribute evaluation (Paper~IV).

    These algorithms enable automatic parallelization of static analyses built with RAGs.
    By parallelizing the ExtendJ compiler, Java error checking was sped up by about a factor of two.
    Additionally, our evaluation showed reduced attribute response time in an incremental
    evaluation benchmark, from seconds to below a millisecond.
  \item Correctness proofs for the concurrent RAG algorithms (Paper~IV).

    Correctness is of paramount importance in concurrent settings, not least due
    to the well-known difficulties of debugging and reproducibly testing flaws in concurrent code.
    The correctness proofs are thus an essential contribution for the new concurrent RAG algorithms.
    The implementation of the concurrent RAG algorithms in JastAdd could of course still contain
    errors, irrespective of the correctness of the algorithms themselves.
    However, the implementation follows the algorithms closely so that the implementation is
    easier to manually verify.
  \item Simplification of circular attributes in RAGs (Paper~IV).

    This is an important relaxation of the requirements for specifying circular attributes
    which I discovered while working on the concurrent attribute algorithms.
  %\item Improvements to the JastAdd metacompiler.
  \item ExtendJ improvements.

    One of the most important improvements I have made to the ExtendJ compiler was
    to remove side effects in the frontend of ExtendJ, to enable parallel error
    checking for the evaluation of
    Paper~IV.
    I have implemented several additional redesigns in the compiler in order to improve
    correctness and/or to simplify the design and make the compiler more usable by others.

  %\item New software tools for developing extensible compilers with JastAdd. (JastAdd Gradle plugin)
  %\item New tools for generating API documentation for JastAdd compilers.
\end{itemize}

The following sections describe each contribution in more detail.

\newpage
\subsection{Extension of ExtendJ to Java~7}

In Paper~I, we describe the design of the Java~7 extension to ExtendJ.
The Java~7 extension includes the following main additions to the compiler:
try-with-resources, diamond access (type inference), and strings in switch.

\subsubsection{Try-With Resources}

\emph{Try-With Resources} (TWR) was the largest language change in Java~7, adding resource declarations in try-statements.
Each resource declaration contains an initializing expression which opens a resource.
At the end of the TWR statement, the resource is closed. For example:

\begin{lstlisting}
try (OutputStream fout = new FileOutputStream("x");
     PrintStream out = new PrintStream(fout)) {
  out.println("Solving old problems in new ways.");
  ...
}
\end{lstlisting}

\noindent
This TWR statement uses two resources: a \texttt{FileInputStream} and a \texttt{PrintStream}.
When control leaves the try-statement, both resources are automatically closed.

The main challenge in implementing TWR statements in ExtendJ was code generation. There are several
special cases that must be handled depending on how many resources are used inside the resource
declaration part of the statement. Each resource declaration should be initialized in order, and
each initialization may be interrupted by an exception. If an initialization is interrupted, then
all previously initialized resources must be closed.

The implementation described in Paper~I elegantly solves code generation for TWR resources:
we use higher-order attributes to unfold a TWR statement
into simpler statements that each have only a single resource declaration. This greatly reduces the
number of different cases that need to be handled with regard to handling exceptions during
resource initialization. The unfolding of TWR statements into simpler statements is a kind
of desugaring, but instead of desugaring to elementary language features we desugar to
a new language feature which is just a simplified form of the full construct.

\subsubsection{Diamond Access}

The \emph{Diamond Access} is a new way of using type inference to create
instances of a generic class.  For example, the following statement

\begin{lstlisting}
List<String> list = new ArrayList<String>();
\end{lstlisting}

\noindent
can be replaced by


\begin{lstlisting}
List<String> list = new ArrayList<>();
\end{lstlisting}

\noindent
The ExtendJ implementation of diamond access reuses generic method type inference which existed in the
compiler for Java~5. Using higher-order attributes, a synthetic method invocation is created
which corresponds to the class instance expression. Then, for each accessible constructor
for the current class, a synthetic method is created with type parameters matching the
class type parameters. The synthetic method call is then used to infer type arguments
for the method call, which gives directly the type arguments for the diamond access.

\subsubsection{Strings in Switch}

The implementation of strings in switch was straightforward. I extended the
bytecode generation for the case when the \verb'switch' argument is a string by
using the \verb'refine' mechanism described in \secref{sec:refine}.
Type analysis for strings in switch was similarly extended by refining the attribute
\emph{SwitchStmt.type}.

\subsubsection{Evaluation}

The empirical evaluation of the Java~7 extension showed that the Java~7 version of ExtendJ
was only 52\% the size of the corresponding OpenJDK compiler, the reference Java compiler.
ExtendJ has always been slower than OpenJDK, but the compile time remained
reasonably close in comparison.

The compile time evaluation was done by measuring total compilation time
across seven Java applications of varying sizes, between 5 and 87 kilo lines of
code.  Compile time was measured both for cold-start and steady-state
compilation. In the cold-start case, ExtendJ compile
time was within a factor 1.6 from that of OpenJDK.
In the steady-state case, ExtendJ was at most 3.3 times slower than OpenJDK.


\subsection{Multiplicities Implementation}

Paper~II is a case study in programming with \emph{Multiplicities} as a Java language extension.
The concept of multiplicities was invented by Friedrich Steimann, the main author. I implemented
these concepts as an extension to the ExtendJ compiler.

Multiplicities allow the programmer to easily change how many objects a reference can relate to,
either to-one, or to-many. For example, a regular Java program may contain the following
code fragment to keep track of a single account:

\begin{lstlisting}
Account acc = new Account(name);
acc.export();
\end{lstlisting}

\noindent
With multiplicities, the programmer may easily track multiple account objects by
changing adding a new modifier to change the multiplicity type of \verb'acc' to \textbf{any}:

\begin{lstlisting}[morekeywords={any}]
any Account acc = new Account(name);
acc += otherAccount();
acc.export();
\end{lstlisting}

\noindent
This code will export all accounts added to the \texttt{acc} reference, which may be many.

The most novel part of the multiplicities implementation is the extension of
the type system in ExtendJ to support the new multiplicity types.
I developed the new typing rules in collaboration with Steimann, based
on the original multiplicities concept.

The implementation of multiplicities as an ExtendJ extension is straightforward.
The type system was extended by adding new kinds of types for the new multiplicities,
with new equations extending the double dispatch framework discussed in \secref{sec:exj-type-anal}.
Code generation was extended by adding new specialized bytecode generation for some statements
and expressions involving non-bare multiplicities.

We evaluated the performance of code compiled with the multiplicities extension, to see
if code that used multiplicities was slower than the corresponding code without multiplicities.
To this end, we modified a version of JUnit, a popular Java unit testing framework,
to use multiplicities
in many places. The modified version of JUnit was compiled and compared against the unmodified
version of JUnit. The performance evaluation showed a very small run-time overhead for the
multiplicities-modified version of JUnit compared to the unmodified version.
We also evaluated the
correctness of the extended version of ExtendJ by verifying that the modified version of
JUnit passed all of its own unit tests.


\subsection{Safe Regression Test Selection}

Regression testing is the practice of running tests after each change to an application in order to
ensure that previously implemented, and tested, features do not break (which would cause a
regression).
Paper~III presents a new algorithm for \emph{safe regression test selection}.
The goal of safe regression test selection is to reduce the number of
tests that are run after each modification to the code while still guaranteeing that
no regression will go undetected.

In Paper~III we also describe the
implementation of our algorithm in a tool based on ExtendJ.
The tool is a small and efficient extension for extracting a program
dependency graph from Java programs. Our test selection algorithm
is then run on the dependency graph to quickly select tests to run.

We evaluated the implementation by using real-world Java programs. The commit history of the
programs was replayed and the test selection tool was run for each change. We ran all tests
selected by our tool and verified that the tool selected all tests that changed result
from the previous commit.

The implementation of the test selection tool itself was quite straightforward, but it showed
that it was very easy to develop a tool using class dependency graphs based on ExtendJ.


\subsection[Concurrent Evaluation of Reference Attribute Grammars]{\texorpdfstring{%
Concurrent Evaluation of Reference Attribute Grammars}{%
Concurrent Evaluation of Reference Attribute Grammars}}
\label{sec:contrib-parallel}

Paper~IV describes new algorithms that I developed for concurrent evaluation of RAGs,
with correctness proofs (in the extended version of the paper), and implementation in JastAdd.
Importantly, the algorithms support circular (fixpoint) attributes, which are
needed for many types of static analyses like dataflow analyses and type
inference.

The following diagram illustrates two threads, $T_1$ and $T_2$, concurrently
evaluating mutually dependent attributes:

\begin{center}
\begin{tikzpicture}[
    >={Triangle[angle=45:6pt,length=6pt]},
    every node/.style={draw, ellipse},
    every join/.style={->},
    state/.style={circle, font=\itshape}]
  \node[state] (R) {v};
  \node[state, below=of R] (C) {x};
  \node[state, below left=of C] (D) {y};
  \node[state, below right=of C] (E) {z};
  \node[state, left=of D] (R1) {u};
  \node[above=of R1, color=red] (T1) {$T_1$};
  \node[right =of R, color=blue] (T2) {$T_2$};
  \draw[->,color=red]
      (T1) edge (R1)
      (R1) edge (D)
      (D) edge [bend left] (C)
      (C) edge [bend left] (E)
      (E) edge [bend left] (D);
  \path[every edge/.style={draw, ->, color=blue}]
      (T2) edge  (R)
      (R) edge  (C)
      (C) edge  (E)
      (E) edge  (D)
      (D) edge  (C);
\end{tikzpicture}
\end{center}

\noindent
Arrows illustrate the evaluation control flow, which follows the attribute dependency graph.
Thread $T_1$ starts evaluating attribute $u$, and $T_2$ starts at attribute $v$. The
start attributes are independent, but the other attributes are in a dependency cycle.
A locking implementation that tries to lock individual attributes would cause a \emph{deadlock} in
the current scenario, getting stuck forever due to circular hold-and-wait.
The algorithms in Paper~IV are lock-free, which ensures that they never deadlock.
Furthermore, different evaluation threads can cooperate by sharing partial results. The algorithms
enable threads to share results even in fixpoint iterations like the one that
arises in the illustrated scenario.

The evaluation in Paper~IV is based on ExtendJ. I measured attribute evaluation latency
for short-running attributes while long-running attributes were computed in parallel.
Latency was reduced from seconds to less than a millisecond.
I also measured total error checking time for Java programs and found an approximate twofold
speedup with parallel error checking.

The parallelized version of ExtendJ is publicly available on the main ExtendJ source code
repository.\footnote{\url{https://bitbucket.org/extendj/extendj}}
Parallelization is done by evenly distributing source files between threads. Each thread
then performs error checking for its allocated source file, in parallel with other threads.
This division of work between threads was implemented by hand.

An alternative way of dividing work between threads in JastAdd compilers
is to use automatic parallelization through concurrent collection attributes.
Concurrent collection attributes were only briefly mentioned in Paper~IV, so I will describe them in
more detail here.

JastAdd collection attributes work in two phases \cite{DBLP:conf/scam/MagnussonEH07}.
First, in the \emph{survey phase},
the attribute evaluator traverses the AST and looks for contribution statements matching the
collection attribute. Second, the \emph{collection phase} collects values from contribution
statements.
An ordinary collection attribute can be annotated with two annotations to activate parallel
evaluation of the attribute: the \verb'@Parallel' annotation makes the value collection phase
run in parallel, and the \verb'@ParallelSurvey' annotation makes the survey phase parallelized.
The workload is split among worker threads by using fixed-size thread
pools.\footnote{The number of worker threads used is controlled by the \texttt{numThreads} option to
JastAdd. The number of worker threads can also be changed at runtime via the
\emph{ASTState.numThreads} field.}

%This is an example of a parallelized collection attribute declaration:
%
%\vbox{\hfuzz=1000pt
%\begin{adjustbox}{max width=0.9\linewidth}
%\begin{lstlisting}
%@Parallel coll List<Problem> CompilationUnit.problems();
%\end{lstlisting}
%\end{adjustbox}
%}

%\noindent
%Parallelized collection attributes should not circularly depend on each other. This causes an
%explosion in the number of worker threads since each nested evaluation needs to spawn its own
%thread pool.

%\subsection{Parallelization}
%
%For Paper~IV, I parallelized the error checking in ExtendJ. This work is available in the
%parallel branch of the public ExtendJ source repository.
%
%The implementation is straightforward: compilation is split up per each Java source file
%(CompilationUnit). A thread pool is used to divide work evenly among multiple threads.
%
%Below is the main processing loop of the parallel version of ExtendJ:
%
%\begin{lstlisting}
%ExecutorService threadPool =
%    ASTState.newThreadPool(6);
%...
%Iterator<CompilationUnit> iter =
%    program.compilationUnitIterator();
%List<Future<Integer>> futures =
%    new LinkedList<>();
%while (iter.hasNext()) {
%  final CompilationUnit unit = iter.next();
%  futures.add(threadPool.submit(
%      new Callable<Integer>() {
%        public Integer call() {
%          return process(unit);
%        }
%      }));
%}
%
%for (Future<Integer> future : futures) {
%  int result = future.get();
%  if (result != EXIT_SUCCESS) {
%    handle(result);
%  }
%}
%\end{lstlisting}
%
%The current version of the code uses a thread pool of fixed size. However, the
%thread pool allocation line may easily be replaced to create a custom thread
%pool of any desired size.


\subsubsection{Parallel Performance}

Improved run-time performance is possible, but not guaranteed, when parallelizing JastAdd code.
The performance results in ExtendJ are especially pleasing given that the Java language
has many interdependencies between classes. These dependencies mean that when evaluating an attribute
in one source file,
it is likely to have several dependencies on attributes in other source files.
This leads to redundant computations in parallel attribute evaluation when two or more threads
evaluate the same attribute
at the same time: at least one thread is then wasting time computing an attribute when it would
ideally be left to a single thread.

While it would be more ideal to split attribute evaluation evenly among threads, so that each
thread separately works only on attributes from its own source file, this is not practically
possible as Java code tends to be highly interconnected.

The degree of parallelization that can be achieved probably varies largely based on language, but
we have not specifically evaluated other languages to compare how parallel performance differs
with JastAdd for different languages.


\subsection{Simplification of Circular Attributes}

Previous work on circular attributes defined them with the assumption that
all attributes in a dependency cycle are treated
equally, with fixpoint iteration \cite{DBLP:conf/sigplan/Farrow86,DBLP:journals/toplas/Jones90}.
This led to the requirement
for circular attributes in JastAdd to be annotated with the \emph{circular} keyword,
and that all attributes which could be in a dependency cycle must be annotated as such
\cite[p. 27]{DBLP:journals/scp/MagnussonH07}.
While working on the concurrent circular attribute algorithm, I realized that this condition
is needlessly strict. It is possible to loosen the requirement to the following: \emph{at
least one} of the attributes in \emph{each possible} dependency cycle is annotated as 
circular (and thus evaluated with fixpoint iteration).\footnote{The relaxed requirements
for circular attributes do require that memoization is delayed for attributes that are
part of a circular attribute evaluation, but which are not annotated as circular.}

This relaxed requirement for circular attributes makes the task of writing attribute
grammars much simpler.
Static analysis extensions and language extensions could often introduce circularity by
linking previously non-circular attributes. With the relaxed circular evaluation, such
extensions only need to annotate their circularity-causing attributes for fixpoint iteration.
Additionally, this saves some extra memory overhead needed in the fixpoint iteration mechanism
for circular attributes.


\subsection{ExtendJ Improvements}

This section describes some of the larger improvements to ExtendJ that I implemented
during my thesis work. These improvements are not described in the included papers.
The purpose of the changes was either to fix correctness problems, or to simplify
the compiler design.

The most important change to ExtendJ was to remove all extant side effects from the frontend
so that error checking could be parallelized for the evaluation in Paper~IV.
The side effects that I removed were of three different kinds:

\begin{description}
  \item[Imperative tree transformations]
    Modifying the AST is not safe after any attribute has been evaluated. This is because
    attribute values are derived from the AST, and any change in the AST can affect previously
    memoized attribute values.  Modifications to the AST are safe if they are done before any attribute
    is evaluated, for example during parsing.  In ExtendJ, some transformations were
    done after parsing was finished, between attribute evaluations, and this caused errors in
    concurrent compilation. These side effects were replaced by using higher-order attributes
    to safely compute the transformation.
  \item[Non-pure attributes]
    As previously discussed, attributes must be observationally pure. JastAdd does not yet
    have a mechanism for checking that attributes are pure, however, and side effects can be
    introduced easily by accident. In ExtendJ, there were several attributes which had
    unintentional side effects.

    A common example of an unintentional side effects in ExtendJ were attributes that modified a mutable
    data structure returned by another attribute.  Mutable data structures should be used carefully:
    it is not safe to modify a data structure returned by some attribute because that same data
    structure may have been memoized. Modifying the data structure is then equivalent to modifying
    the memoized value of an attribute, which is not safe.
  \item[Non-fresh higher-order attributes]
    Higher-order attributes must build fresh subtrees. However, there is currently no check for this
    requirement and it is surprisingly easy to accidentally break the requirement.
    Non-fresh higher-order attributes were used in a few places in ExtendJ, unintentionally.
\end{description}

The following sub-sections describe some of the other redesigns that I implemented in ExtendJ.

\subsubsection{Desugaring Multiple Declarations}

%The multiple declaration refactoring was done to avoid partially unsafe tree transformations
%in ExtendJ. These tree transformations were akin to the kind of imperative tree transformations
%that are commonly done in non-RAG based compilers.

Java allows variable (and field) declarations that declare multiple names at once (individual name
declarations are referred to as \emph{declarators}).
For example, this statement:

\begin{lstlisting}
int a, b[2] = { 1, 2 };
\end{lstlisting}

\noindent
is equivalent to

\begin{lstlisting}
int a;
int b[2] = { 1, 2 };
\end{lstlisting}

\noindent
Previously, ExtendJ transformed programs using the former pattern into the equivalent desugared form
with single-variable declarations.
This was done by using a deprecated JastAdd feature called \emph{list rewrite}.
List rewrites were problematic for a few reasons. There were doubts about the correctness
of the list rewrite implementation in JastAdd, and they caused large runtime overhead when
accessing list items in certain ways.

The reason a multi-declaration transformation was used in ExtendJ
is because the different variables in a multi-declaration can have different types.
To simplify the analysis of multi-declarations in the compiler, we need an easy way
to access the type of a single variable inside a multi-declaration.
Without transforming the AST, this can be accomplished in a clean way by using higher-order
attributes (HOAs).  Here is the relevant part of the abstract grammar for variable declarations:

\begin{alignat*}{2}
\textit{VarDeclStmt} : & \; \textit{Stmt} & \; \Coloneqq & \; \textit{Modifiers}\;\textit{TypeAccess:Access} \\
& & & \textit{Declarator:VariableDeclarator}\,\ast \\
\texttt{abstract} \; \textit{Declarator} : & \; \textit{ASTNode} & \; \Coloneqq & \; \langle\textit{\textit{ID}}\rangle \; \textit{Dims}\ast\;[\textit{Init:Expr}] \\
\textit{VariableDeclarator} : & \; \textit{Declarator} & & \\
\textit{FieldDeclarator} : & \; \textit{Declarator} & & \\
\end{alignat*}

\noindent
The two subclasses of \emph{Declarator} are used to distinguish local variables
from fields. The context of the declaration also makes the difference clear, but having a subclass
for each makes it slightly easier to specialize some attributes for each case.

I added the following HOA to compute the type of each variable declarator:

%syn access declarator.gettypeaccess() =
%    ((Access) declarationType().treeCopyNoTransform())
%        .addArrayDims(getDimsList());

\begin{align*}
& \textbf{\textit{syn}} \; \textbf{\textit{nta}} \; \textit{Access} \; \textit{Declarator.TypeAccess} =\\
& \qquad \textit{treeCopy}(\textit{declarationType}).\textit{addArrayDims}(\textit{Dims})
\end{align*}

\noindent
The \emph{declarationType} attribute is a helper attribute which gives a reference to the enclosing
variable declaration type (\emph{TypeAccess:Access} in \emph{VarDeclStmt}).
The \emph{treeCopy} function is used to clone the original type access, which is necessary
in order to build a fresh tree for the HOA.
The \emph{addArrayDims} attribute is a helper attribute to include any necessary array dimensions.

An advantage of using HOAs to compute single variable types instead of
using a rewrite to transform the AST is that we retain the source AST, making it easier to
pretty-print the original code or to report errors with precise source locations.


\subsubsection{Reifying Implicit Constructs}

Java compilers use many implicit constructs which the programmer never sees, but which are necessary
for generating correct Java bytecode. Examples of implicit constructs include, among others,

\begin{description}
  \item[Enum switch maps] Implicit classes are generated with a field \verb'$SwitchMap$'
    that is used in bytecode for switch statements with enum type arguments.
  \item[Accessor methods] Implicit accessor methods are needed for all cases where
    a class accesses a non-public field of an inner class. These are needed to
    bridge a gap between the Java source language and the Java bytecode language which does
    not have inner classes.
  \item[Bridge methods] When generics are erased in bytecode generation, it may lead to
    overriding methods that no longer override the method of a superclass. A bridge method
    is implicitly generated to recover the intended overriding behaviour.
  \item[Enclosing and super references]
    Constructors are automatically augmented with an implicit parameter for the superclass
    reference. Inner classes receive an additional implicit parameter for the enclosing
    class reference.
\end{description}

Previously, ExtendJ reified the above implicit constructs by using imperative AST modifications
during a transformation pass.
The transformation pass was not cleanly separated from static analyses
(due in part to uses of attributes in the transformations),
which caused problems when evaluating attributes which depended on
transformed AST structures. I replaced these imperative transformations by higher-order attributes.
All implicit constructs in ExtendJ are now reified by higher-order attributes.
% TODO: describe what kind of problems were caused

%This caused a new problem, which is
%to discover the constructs created by these higher-order attributes during code generation.
%Finding the implicit constructs
%is solved by using collection attributes. An example such a solution is shown in \secref{sec:coll-prop}.

\subsubsection{Type Variable Substitution}

Generic classes and methods are parameterized by one or more \emph{type variables}.  Type variables
are replaced by the corresponding \emph{type arguments} in each parameterization of a generic class
or method.  For instance, consider the following generic class:

\begin{lstlisting}
public class Container<T> {
  private T value;
  public void set(T v) {
    value = v;
  }
  public T get() {
    return value;
  }
}
\end{lstlisting}

\noindent
This class has one type parameter: the type variable \verb'T'. We can instantiate the class by
writing, e.g., \verb'Container<String>', which means the type of \verb'Container' where
all occurrences of \verb'T' are substituted by \verb'String'.
Here, \verb'Container<String>' is called a \emph{parameterization} of \verb'Container'.
In ExtendJ, each parameterization of a generic class must be reified as a type declaration,
that is,
an AST subtree that represents the full type with all its public member declarations.
For example, ExtendJ represents the type of \verb'Container<String>' by the following
structure:

\begin{lstlisting}
public class Container<String> {
  public void set(String v);
  public String get();
}
\end{lstlisting}

\noindent
Notice that the private field and the method bodies were removed. Only the public
interface of \verb'Container<String>' is needed to reify the parameterization.
As described in \secref{sec:exj-polytypes}, the parameterization is implicitly created by
a higher-order attribute. However, we have not yet discussed how the type variable
is replaced by \verb'String'. The replacement process is called \emph{type variable substitution}.

Previously, ExtendJ performed type variable substitution by replacing all occurrences of \verb'T'
by \verb'String' when creating the parameterization \verb'Container<String>'. ExtendJ
created an AST matching the reified parameterization code shown above. For various reasons,
this process did not work perfectly: it led to
some low-level problems in type inference with generic methods,
and it made parallel evaluation of parameterized types run slowly.

I redesigned type variable substitution by delaying the substitution process until type lookup.
Type lookup is the latest possible time we can substitute type variables, and
it turns out to work very well for enabling correct type inference in some cases
that previously failed because of
the too-eager type variable substitution. Additionally, this change improved
parallel evaluation performance because it made the higher-order attribute for
parameterized types much faster to evaluate and led to fewer threads trying to
compute the same attribute instance at the same time.

The central attribute equation which performs type variable substitution in the redesigned
implementation looks like this:

%eq ParTypeDecl.getBodyDecl().lookupType(String name) {
%  TypeDecl paramType = getParameterization().substitute(name);
%  if (paramType != null) {
%    return paramType;
%  }
%  return localLookupType(name);

%\begin{align*}
%& \textbf{\textit{eq}} \; \textit{ParTypeDecl.BodyDecl.lookupType(String name)} = \\
%& \qquad  \textit{\textbf{let}} \; t \coloneqq \textit{Parameterization.substitute(name) \textbf{in}} \\
%& \qquad\qquad t \;\; \textit{\textbf{if}} \;\; t \ne \textit{\textbf{nil}}; \; \textit{\textbf{else}  localLookupType(name)}
%\end{align*}

\begin{align*}
& \textbf{\textit{eq}} \; \textit{ParTypeDecl.BodyDecl.lookupType(String name)} = \\
& \qquad  \textit{\textbf{let}} \; t \coloneqq \textit{Parameterization.substitute(name) \textbf{in}} \\
& \qquad\qquad \begin{cases}
  t, & \text{if $t \ne \textit{\textbf{nil}}$} \\
  \textit{localLookupType}(\textit{name}), & \text{otherwise}
\end{cases}
\end{align*}

\noindent
The equation above is slightly abbreviated. The \emph{localLookupType}
attribute is used for searching the local scope of the type declaration.

%Here is an example of a type inference case that failed before the type variable substitution change:
%
%\begin{lstlisting}
%interface I<T> { }
%class C<T> implements I<T> { }
%class Impl<R> {
%  public <S extends I<R>> void run(S l) {}
%}
%
%public class Test {
%  void test() {
%    new Impl<String>().run(new C<String>());
%  }
%}
%\end{lstlisting}
%
%\noindent
%This case failed because \verb'Impl<String>' was reified as
%
%\begin{lstlisting}
%class Impl<String> {
%  public <S extends I<String>> void run(S l) {}
%}
%

\subsection{Related Work}

% TODO: discuss relation to Datalog, another language used for declarative static analysis.

% TODO: Boyland (APS) Descriptional Composition of Compiler Components

AGs have been an active research topic in the programming language community
for many years. Classical AGs, with only synthesized and inherited attributes,
were never really used for implementing real-world
programming languages.\footnote{That is, languages other than
the kind of ``toy'' languages that are often used in teaching and research
articles to demonstrate a handful of programming language concepts.}
On the other hand, various extended AGs have been used to develop practical implementations of
real-world programming languages, for example:
Pascal \cite{DBLP:books/sp/KastensHZ82},
Ada \cite{DBLP:books/sp/UhlDP82},
VHDL \cite{DBLP:conf/pldi/FarrowS89}, and Oberon2 \cite{boyland1996descriptional}.
More recently, RAGs, another AG extension, have been successful for implementing several
languages like
Java \cite{jastaddj,DBLP:conf/ecoop/WykKBS07},
Modelica \cite{DBLP:journals/cce/AkessonAGBT10},
PROMELA \cite{DBLP:conf/spin/MaliW11},
Grafchart \cite{theorin2012rewriting},
Bloqqi \cite{DBLP:conf/oopsla/ForsH16},
and C \cite{DBLP:journals/pacmpl/KaminskiKCW17}.

ExtendJ is a Java compiler built with the JastAdd metacompiler \cite{DBLP:journals/scp/HedinM03},
and as such has quite different compiler
architecture compared to most other conventional compilers, which use tree visitors for
most analyses.
The following table gives a quick
overview of related open source Java compilers and source code analysis frameworks:

\vspace{1em}

\begin{center}
\newcommand{\rbhead}[1]{\rotatebox{70}{\emph{#1}}}
\newcommand*{\cfull}{\tikz[baseline=-3pt]{\fill[black] circle(1ex);}}
\newcommand*{\cempt}{\tikz[baseline=-3pt]{\draw circle(1ex);}}
\newcommand*{\cpart}{\tikz[baseline=-3pt]{\fill[black] (0,0) -- (45:1ex) arc (45:-135:1ex) -- cycle;%
\draw (0,0) -- (45:1ex) arc (45:225:1ex);}}
\begin{threeparttable}
\begin{tabular}{lllllll}
  \emph{Name} & \rbhead{Bytecode} & \rbhead{Analysis} & \rbhead{Transformation} & \rbhead{Java Version}
  & \rbhead{Active Devel.} & \rbhead{Implementation} \\
  \toprule
  ableJ             & \cempt & \cfull & \cfull & 1.4     & \cempt & RAG (Silver) \\
  \hline
  Eclipse JDT       & \cfull & \cfull & \cfull & 11 & \cfull & Java \\
  \hline
  Error Prone       & \cempt & \cfull & \cpart & 11 & \cfull & OpenJDK \\
  \hline
  ExtendJ           & \cfull & \cfull & \cfull & 8  & \cfull & RAG (JastAdd) \\
  \hline
  OpenJDK           & \cfull & \cfull & \cfull & 11 & \cfull & Java \\
  \hline
  JavaParser        & \cempt & \cpart & \cempt & 11 & \cfull & Java \\
  \hline
  Polyglot          & \cempt & \cfull & \cfull & 7  & \cfull & Java \\
  \hline
  \textsc{Spoon}    & \cempt & \cfull & \cfull & 11 & \cfull & Eclipse JDT \\
  \hline
  SugarJ            & \cempt & \cempt & \cfull & 5       & \cempt & SDF/Stratego \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item \emph{Bytecode}: bytecode generation is implemented.
\item \emph{Analysis}: full static analysis for Java (following the specification).
\item \emph{Transformation}: allows modifying the program AST to affect other analyses.
\item \emph{Java Version}: latest supported Java version.
\item \emph{Active Devel.}: public implementation with functional changes in the past two years.
\end{tablenotes}
\end{threeparttable}
\end{center}

\vspace{1em}

OpenJDK is the reference implementation of Java, including a compiler, standard library,
and virtual machine.
The OpenJDK compiler, \emph{javac}, can either be extended by forking and editing the compiler
code, or by developing plugins for the compiler (including annotation processors).
Plugins are used for compile-time program transformation.
It is possible to do many useful things with plugins like annotation processors,
but in order to extend the language with new syntax it is
necessary to modify javac itself.
An advantage of extending javac is that
it has near-perfect conformance to the Java specification.

Error Prone uses the Java reflection API for javac, adding static analysis for common bug patterns
in Java code.  Error Prone supports the addition of custom checks via plugins
\cite{DBLP:conf/scam/AftandilianSPK12}. AST transformation with analysis is not supported,
instead transformed code is printed as code patches.

The Eclipse Java Development Tools (JDT) contain an incremental Java compiler and
a collection of static analysis tools used in the Eclipse editor \cite{eclipsejdt}.
Eclipse JDT uses a conventional visitor-based compiler architecture.

\textsc{Spoon} is a static analysis framework for Java that uses the Eclipse
JDT internally \cite{DBLP:journals/spe/PawlakMPNS16}.\footnote{%
The connection to Eclipse JDT is neither
mentioned in the documentation or in the paper about \textsc{Spoon}, but can be seen its implementation.}
\textsc{Spoon} provides a transformation framework which can be used to easily construct
type-safe syntax transformations.

Polyglot is an extensible compiler framework for Java based on an extensible visitor pattern
\cite{DBLP:conf/cc/NystromCM03}. Polyglot provides similar functionality as ExtendJ, and
a detailed comparison of the two was given by \cite{DBLP:conf/aosd/AvgustinovET08}.
In Polyglot, the visitor pattern results in large amounts of boilerplate code and monolithic
data structures for some problems which are solved more succinctly in ExtendJ.

SugarJ is a library-based syntax language extension framework for Java \cite{DBLP:conf/oopsla/ErdwegRKO11}.
SugarJ is implemented
in Java,
SDF \cite{DBLP:journals/sigplan/HeeringHKR89},
and Stratego \cite{DBLP:conf/rta/Visser01}.
SugarJ enables language extensions to be imported as libraries in user code
\cite{DBLP:conf/oopsla/ErdwegRKO11}.

JavaParser is a Java library for parsing and building ASTs of Java programs, with name analysis
provided through an additional library \cite{javaparser}.

ableJ is an extensible compiler supporting Java~1.4 and built with RAGs in the Silver metacompiler
\cite{DBLP:conf/ecoop/WykKBS07}.  There seems to be no active
work on the project as the latest changes in the past few years appear to be non-functional.

%ExtendJ does not support bytecode analysis.
%Soot is a popular framework for analyzing and optimization Java bytecode \cite{DBLP:conf/cascon/Vallee-RaiCGHLS99}.

%While it does perform analysis, it does not fit in our table since it does not support parsing Java source
%code.

%The regression testing tool developed for the regression testing contribution is based on the
%ExtendJ compiler, additionally

%Non-declarative extensible compilers have been used to develop static analyses, for example \cite {DBLP:conf/scam/AftandilianSPK12}.

%TODO: mention jacks? Not relevant enough.


\section{Conclusions}
\label{sec:conclusions}

This thesis presents my contributions to declarative specification of static program analysis with
RAGs. My contributions include new language extensions, static analyses, and tools for
the Java language and based on the Java compiler ExtendJ.
In developing these extensions, I found new design principles for developing declarative language
and analysis extensions with RAGs. For example, I developed a partial desugaring technique
with higher-order attributes which I used in the implementation of try-with-resources for Java~7.

My more fundamental contributions to RAGs themselves include
concurrent attribute evaluation algorithms with support for circular (fixpoint) attributes.
The concurrent evaluation algorithms are presented in Paper~IV, with correctness proofs
included in the extended technical report version of the paper.
Another important contribution to RAGs is a relaxation of the requirements for circular attribute
specifications to be well-defined. Previous work on circular attributes required all
attributes to be evaluated with fixpoint iteration. In Paper~IV, I show how this requirement
can be relaxed with a modification of the evaluation algorithm.

Static program analysis with RAGs has many benefits: declarative specification of analyses
improves their composability and readability, enabling code reuse and extensibility
for evolving programming languages.
Some of the benefits of using RAGs, like structure-shy programming,
are demonstrated in \secref{sec:extension-mechanisms}. In  \secref{sec:ast-traversal}, I present a
pattern for specifying generic tree traversals in a RAG.

With static analyses specified in RAGs, there is an opportunity for optimizing the
static analysis to improve run-time performance, by memoizing attributes and
parallelizing evaluation.  Compilers specified with RAGs can be
automatically parallelized using parallel collection
attributes which I implemented for the JastAdd metacompiler (see
\secref{sec:contrib-parallel}, and briefly mentioned in Paper~IV).

The ExtendJ compiler is a main focus in this thesis. ExtendJ was used for developing implementations
for the included papers, as well as empirically evaluating the results of the included papers.
A contribution in this dissertation is the development of a fully declarative and side effect
free version of ExtendJ which was possible to parallelize. This work benefits other programming
language research which use ExtendJ to develop and evaluate language extensions for Java.

%There is room for improving static analysis with RAGs: traditional imperative evaluation and
%tree traversal is still the common choice of compiler architecture. However, as languages become
%more advanced, and more formalized, there is an opportunity for more declarative and generative
%development techniques to benefit compiler developers.
%There is also room to improve the runtime efficiency of RAGs. There already exist methods for
%automatically tuning memoization to improve performance \cite{DBLP:conf/sle/SoderbergH10},
%but more can be done in the field
%of automatic tuning. For instance, the previous work in memoization tuning did not take into account the
%total running time of attributes, only the number of times they were evaluated.


{\raggedright% TODO remove me
\printbibliography[segment=\therefsegment,heading=subbibliography]
}%raggedright

% Remove the page number(the LaTeX chapter) from all numberings.
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thefigure}{\arabic{figure}}
\renewcommand{\thetable}{\arabic{table}}
\renewcommand{\thechapter}{\Roman{chapter}}
\renewcommand{\theequation}{\arabic{equation}}

% Include a reference at the footer of each page.
\newlength{\apa}
\setlength{\apa}{0cm}
\setlength{\spiff}{0cm}

% Paper mark in margin.
\renewcommand{\markmargin}{%
\begin{tikzpicture}[remember picture, overlay]%
\node at (current page text area.north -| current page.east) [
  anchor = north east,
  yshift = -\apa,
  rectangle,
  fill = black,
  minimum width = 1.6cm,
  minimum height = 2cm,
  inner sep = 0pt,
  outer sep = 0]
  (rec) {};
\node at (rec.south west) [
  anchor = north west,
  text width = 2cm,
  text height = 0.4cm,
  inner sep = 0,
  outer sep = 0,
  text centered,
  font=\sffamily\bfseries\small,
  color=white,
  rotate=90]
  {\textsc{Paper~\thechapter}};
\end{tikzpicture}%
}


\newfloat{paperfoot}{b}{paper}
\newcommand{\paperRemark}[1]{
  \begin{paperfoot}%
  \hrulefill \flushleft \footnotesize #1 \end{paperfoot}
}
%-------------------------------------------------------

\part{Included Papers}

\setcounter{chapter}{0}
\renewcommand{\chaptername}{Paper}

\ifpaperI
\fancyhead[RE,LO]{Paper~I: Extending the JastAdd Extensible Java Compiler to Java 7}
\include{papers/java7}
\fi

\ifpaperII
\addtolength{\apa}{2cm}
\fancyhead[RE,LO]{Paper~II: Multitudes of Objects}
\include{papers/multiplicities}
\fi

\ifpaperIII
\addtolength{\apa}{2cm}
\fancyhead[RE,LO]{Paper~III: Extraction-Based Regression Test Selection}
\include{papers/testsel}
\fi

\ifpaperIV
\addtolength{\apa}{2cm}
\fancyhead[RE,LO]{Paper~IV: Concurrent Circular Reference Attribute Grammars}
\include{papers/corags}
\fi

\part[Popular Science Summary in Swedish]{\texorpdfstring{%
Popular Science Summary\\in Swedish}{%
Popular Science Summary in Swedish}}

\fi

\makeatletter
\def\@makeschapterhead#1{%
  \vspace*{10\p@}%
  {\parindent \z@ \raggedleft \reset@font
            \sffamily \bfseries \scshape \vphantom{\@chapapp{} \thechapter}
        \par\nobreak
        \interlinepenalty\@M
    \Huge  #1\par\nobreak
    \hrulefill
    \par\nobreak
    \vskip 16\p@
  }}
\makeatother

%\renewcommand{\figurename}{Figur}
\chapter*{Utveckling av statisk analys}

%Den här avhandlingen handlar om utveckling av \emph{statisk programanalys}.
\emph{Statisk programanalys}, eller kort och gott statisk analys,
är av stor betydelse inom mjukvaruutveckling.
Statisk analys är en samlingsterm för olika automatiserade analyser av datorprogram.
Framför allt behövs statisk analys för att kompilera program till exekverbar maskinkod,
så att de kan köras på en dator (eller mobiltelefon, till exempel).
Statisk analys används också för att hitta och förhindra fel i program,
samt för att optimera prestandan hos program.
Med statisk analys kan man bland annat hitta säkerhetshål och förhindra att känslig data läcker ut ur
ett program.
Dessutom används statisk analys i \emph{programmeringsverktyg}, alltså de verktyg en programmerare
använder för att konstruera sina program. Till exempel kan statisk analys användas för att
föreslå ändringar i koden (kodkomplettering och korrigering av enkla fel),
eller för att låta programmeraren snabbt hoppa mellan definitioner och användningar av
namn i koden.

\begin{center}
\scalebox{0.8}{
% http://www.texample.net/tikz/examples/pdca-cycle/
\definecolor{centerfill}{RGB}{108,80,190}
\definecolor{arrowfill}{RGB}{246,140,180}

\newcommand{\aarrow}[3]{%
   \pgfmathsetmacro{\astart}{#1}
   \pgfmathsetmacro{\aend}{#2}
   \pgfmathsetmacro{\atip}{5}
   \fill[arrowfill, very thick] (\astart+\atip:\rin)
                         arc (\astart+\atip:\aend:\rin)
      -- (\aend-\atip:\rmid)
      -- (\aend:\rout)   arc (\aend:\astart+\atip:\rout)
      -- (\astart:\rmid) -- cycle;
   \path[
      decorate,
      decoration={
        text along path,
        raise=-0.5ex,
        text={|\sffamily\footnotesize\bfseries|#3},
        text align={center}
      },
   ](\astart:\rmid) arc (\astart:\aend:\rmid);
}

\pgfmathsetmacro{\rin}{3.5}
\pgfmathsetmacro{\rmid}{4}
\pgfmathsetmacro{\rout}{4.5}

\begin{tikzpicture}[
	font={\sffamily},
  proc/.style={circle, align=center}
  ]
  \node[proc] at (180:\rmid) {
    \includegraphics[width=2cm]{figures/doggo} \\
    programmering
  };
  \aarrow{150}{93}{källkod}
  \node[proc] at (60:\rmid) {
    \includegraphics[width=1.5cm]{figures/compiler} \\
    kompilering \\
    \& analys
  };
  \aarrow{25}{-27}{exekverbart program}
  \node[proc] at (300:\rmid) {
    \includegraphics[width=1.5cm]{figures/testing} \\
    testning
  };
  \aarrow{270}{210}{testresultat}
	\node[draw, circle, color=white, fill=centerfill,
		minimum width=4cm,
    align=center] at (0,0) {\sffamily\bfseries{}Utvecklingscykel\\för programvara};
\end{tikzpicture}
}
\end{center}
\noindent
\textbf{Figur:} Den vanliga utvecklingsprocessen för datorprogram.
Under programmering och kompilering används statisk analys av programmeraren och kompilatorn.

\vspace{1.5em}

Statisk analys är en form av mjukvara som ofta är komplicerad och tidskrävande att utveckla.
Dessutom finns det ofta ett behov av att kunna bygga vidare på en befintlig statisk analys med
nya egenskaper, till exempel om programmeringsspråket som analyseras uppdateras.
Således är det önskvärt att utveckla statisk analys med en flexibel mjukvaruarkitektur, det vill
säga på ett sätt som gör det möjligt att bygga på analysen utan allt för stor ansträngning.

Givet att det finns några grundläggande analyser som är utvecklade
med flexibel arkitektur så kan vi utveckla många nya viktiga analyser som använder de grundläggande
analyserna som byggstenar. Detta sparar mycket tid och pengar.

\subsection*{Deklarativ programmering}

För att uppnå en flexibel mjukvaruarkitektur kan vi använda oss av \emph{deklarativ programmering}.
Det innebär att programmeraren beskriver \emph{vad} som skall beräknas, snarare än
att exakt beskriva stegen som datorn tar för att beräkna det som behövs.
Den främsta fördelen med
deklarativ programmering, när det gäller att bygga flexibla program, är att det blir enkelt
att dela upp ett deklarativt program i moduler.
Programmoduler kan utvecklas separat och sedan kombineras på olika sätt, vilket sparar tid
för programmerarna i det långa loppet.
En deklarativ statisk analys kan ganska enkelt användas som en modul inuti en större, mer komplicerad,
analys.

En statisk analys kan delas upp i små deklarativa delar som kallas
\emph{attribut}. Attributen kan enkelt kombineras till moduler som sedan
används för att utveckla nya analyser.

Ett sätt att tänka på modulerna i en statisk analys är som pusselbitar. En pusselbit (modul)
kan läggas till i ett pussel (en statisk analys) så länge den har rätt form (programgränssnitt):

\begin{center}
\includegraphics[width=4cm]{figures/puzzle}
\end{center}

\vspace{1em}

Ett viktigt forskningsresultat i den här avhandlingen är en ny algoritm för att kunna beräkna
flera attribut samtidigt. Detta kan förkorta beräkningstiden: i ett experiment visade mina
mätningar att kompileringstiden kunde kortas med hälften.
En annan fördel är att svarstiden kan minskas i interaktiva
programmeringsverktyg: med samtidiga beräkningar måste man inte vänta på att föregående beräkningar
är klara innan man börjar på den nästa. Mina experimentella resultat visar att svarstiden kan minskas
från sekunder till under en millisekund.

\newpage
För att kunna utföra beräkningar samtidigt har de flesta datorer idag flera
\emph{kärnor}, där varje kärna exekverar varsin \emph{beräkningstråd}
jämnlöpande med andra kärnor i datorn.
Med min nya algoritm kan den sammanlagda beräkningstiden för attribut minskas genom att dela upp
beräkningsarbetet på flera (beräknings)trådar.
När en tråd beräknat ett attribut sparas värdet och kan direkt användas av en efterföljande tråd
som då slipper göra om beräkningsarbetet.
Uppdelning av arbetet för fem attribut med två jämnlöpande trådar illustreras i figuren nedan.

\begin{center}
\begin{tikzpicture}[
    >={Triangle[angle=45:6pt,length=6pt]},
    every node/.style={draw, ellipse},
    every join/.style={->},
    state/.style={pattern=north east lines}]
  \node[state] (R) {};
  \node[state, below=of R] (C) {};
  \node[state, below left=of C] (D) {};
  \node[state, below right=of C] (E) {};
  \node[state, left=of D] (R1) {};
  \node[above=of R1, color=red] (T1) {$T_1$};
  \node[right =of R, color=blue] (T2) {$T_2$};
  \draw[->,color=red]
      (T1) edge (R1)
      (R1) edge (D)
      (D) edge [bend left] (C)
      (C) edge [bend left] (E)
      (E) edge [bend left] (D);
  \path[every edge/.style={
      draw, ->, color=blue,
      decorate,
      decoration={
        snake,
        amplitude=0.8mm,
        segment length=4.1mm,
        pre length=0.8mm,
        post length=2mm}
      }]
      (T2) edge  (R)
      (R) edge  (C)
      (C) edge  (E)
      (E) edge  (D)
      (D) edge  (C);
\end{tikzpicture}
\end{center}

\noindent
\textbf{Figur:} Illustration av två trådar, $T_1$ och $T_2$, som samtidigt beräknar fem
attribut (randiga cirklar).
Beräkningsflödet för $T_1$ visas med röda pilar,
och för $T_2$ med vågiga blåa pilar.
Beräkningen är i det här fallet \emph{iterativ}, och går runt i en cirkel tills det rätta värdet har
beräknats.
Med min algoritm kan varje tråd utföra sin beräkning i varje attribut utan att
behöva vänta på den andra tråden. Dessutom hjälper trådarna varandra genom att en tråd använder
resultatet från den andra tråden om den andra hann före.

\subsection*{Kompilatorer}

Statiska analyser är centrala i kompilatorer, det vill säga program som översätter programmerarens
kod till maskinkod som en dator kan köra.
I mitt arbete har jag arbetat främst med en kompilator som heter ExtendJ, som
kompilerar program skrivna i programmeringsspråket Java. ExtendJ är fullt deklarativt
programmerad och använder attribut, vilket gör det förhållandevis enkelt att använda ExtendJ för att
bygga nya statiska analyser för Java.

För min avhandling har jag
dels utvecklat nya analyser som bygger på ExtendJ,
dels förbättrat ExtendJ självt.

Bland de statiska analyser som jag utvecklat i ExtendJ finns en analys som kan användas
för att minska testningstiden för Java-program. Idén är att endast köra om de test som kan påverkas av
den senaste ändringen i programmet som testas.
Med denna teknik lyckades jag halvera den totala tiden för testning av
flera olika program.

En av förbättringarna jag gjort i ExtendJ var att uppdatera kompilatorn till att
stödja en ny version av Java.  Sammanlagt har förbättringarna som jag gjort i
ExtendJ gjort det enklare för forskargrupper runt hela världen att bygga nya
statiska analyser och språktillägg till Java.

\end{document}

